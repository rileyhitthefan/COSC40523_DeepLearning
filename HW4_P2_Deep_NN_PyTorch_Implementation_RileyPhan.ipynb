{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rileyhitthefan/COSC40523_DeepLearning/blob/main/HW4_P2_Deep_NN_PyTorch_Implementation_RileyPhan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08f47c6a-3318-4e3f-8bb3-c520e00e63dd",
      "metadata": {
        "id": "08f47c6a-3318-4e3f-8bb3-c520e00e63dd"
      },
      "source": [
        "# Implementing Deep Neural Network Using PyTorch\n",
        "This is the second program of homework 4. There is no need to write any code in this part, but please read and execute each cell.\n",
        "https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch Introduction\n",
        "[PyTorch](https://pytorch.org/) is an open source machine learning and deep learning framework. PyTorch allows you to manipulate and process data and write machine learning algorithms using Python code. Many of the worlds largest technology companies such as [Meta (Facebook)](https://ai.facebook.com/blog/pytorch-builds-the-future-of-ai-and-machine-learning-at-facebook/), Tesla and Microsoft as well as artificial intelligence research companies such as [OpenAI use PyTorch](https://openai.com/blog/openai-pytorch/) to power research and bring machine learning to their products.\n",
        "\n",
        "![pytorch being used across industry and research](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00-pytorch-being-used-across-research-and-industry.png)\n",
        "\n",
        "For example, Andrej Karpathy (head of AI at Tesla) has given several talks ([PyTorch DevCon 2019](https://youtu.be/oBklltKXtDE), [Tesla AI Day 2021](https://youtu.be/j0z4FweCy4M?t=2904)) about how Tesla use PyTorch to power their self-driving computer vision models.\n",
        "\n",
        "PyTorch is also used in other industries such as argiculture to [power computer vision on tractors](https://medium.com/pytorch/ai-for-ag-production-machine-learning-for-agriculture-e8cfdb9849a1).\n",
        "\n",
        "Machine learning researchers love using PyTorch. And as of June 2022, PyTorch is the [most used deep learning framework on Papers With Code](https://paperswithcode.com/trends), a website for tracking machine learning research papers and the code repositories attached with them.\n",
        "\n",
        "PyTorch also helps take care of many things such as GPU acceleration (making your code run faster) behind the scenes. So you can focus on manipulating data and writing algorithms and PyTorch will make sure it runs fast.\n",
        "\n",
        "And if companies such as Tesla and Meta (Facebook) use it to build models they deploy to power hundreds of applications, drive thousands of cars and deliver content to billions of people, it's clearly capable on the development front too."
      ],
      "metadata": {
        "id": "Ke3loF-T8tdJ"
      },
      "id": "Ke3loF-T8tdJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Goals\n",
        "This part of the homework 4 will demonstrate a multiclassification task (Fashion MINIST) and a regression task (Predicting California Housing Price) using PyTorch.\n",
        "\n",
        "By the end of this homework, you'll be able to do the following in PyTorch:\n",
        "\n",
        "* Use PyTorch to quickly set up a deep neural network\n",
        "* Train a PyTorch model\n",
        "* Save the trained parameters and load pretrained parameters\n",
        "\n",
        "Programming frameworks like PyTorch and Tensorflow not only cut down on time spent coding, but can also perform optimizations that speed up the code itself."
      ],
      "metadata": {
        "id": "BAzoqKJA8n-k"
      },
      "id": "BAzoqKJA8n-k"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "4rWgYOWtAVF1"
      },
      "id": "4rWgYOWtAVF1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures."
      ],
      "metadata": {
        "id": "Y0236sWAAazv"
      },
      "id": "Y0236sWAAazv"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c263a60d-d788-482f-b9e7-9cab4f6b1f72",
      "metadata": {
        "id": "c263a60d-d788-482f-b9e7-9cab4f6b1f72",
        "outputId": "040bfcec-f08b-4ce0-8cef-b6077a3357a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version: sys.version_info(major=3, minor=10, micro=12, releaselevel='final', serial=0)\n",
            "PyTorch version: 2.4.1+cu121\n",
            "torchvision version: 0.19.1+cu121\n",
            "sklearn version: 1.5.2\n"
          ]
        }
      ],
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "\n",
        "# Import PyTorch\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Import torchvision\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Import matplotlib for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import Scikit-learn\n",
        "import sklearn\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Check versions\n",
        "# Note: your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11\n",
        "print(f\"Python version: {sys.version_info}\\nPyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\\nsklearn version: {sklearn.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building an Image Classifier Using PyTorch"
      ],
      "metadata": {
        "id": "bvqgXI53-zCr"
      },
      "id": "bvqgXI53-zCr"
    },
    {
      "cell_type": "markdown",
      "id": "a0bedcfc-e12a-4a81-9913-84c6a888742a",
      "metadata": {
        "id": "a0bedcfc-e12a-4a81-9913-84c6a888742a"
      },
      "source": [
        "### Computer vision libraries in PyTorch\n",
        "\n",
        "Before we get started writing code, let's talk about some PyTorch computer vision libraries you should be aware of.\n",
        "\n",
        "| PyTorch module | What does it do? |\n",
        "| ----- | ----- |\n",
        "| [`torchvision`](https://pytorch.org/vision/stable/index.html) | Contains datasets, model architectures and image transformations often used for computer vision problems. |\n",
        "| [`torchvision.datasets`](https://pytorch.org/vision/stable/datasets.html) | Here you'll find many example computer vision datasets for a range of problems from image classification, object detection, image captioning, video classification and more. It also contains [a series of base classes for making custom datasets](https://pytorch.org/vision/stable/datasets.html#base-classes-for-custom-datasets). |\n",
        "| [`torchvision.models`](https://pytorch.org/vision/stable/models.html) | This module contains well-performing and commonly used computer vision model architectures implemented in PyTorch, you can use these with your own problems. |\n",
        "| [`torchvision.transforms`](https://pytorch.org/vision/stable/transforms.html) | Often images need to be transformed (turned into numbers/processed/augmented) before being used with a model, common image transformations are found here. |\n",
        "| [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) | Base dataset class for PyTorch.  |\n",
        "| [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#module-torch.utils.data) | Creates a Python iteralbe over a dataset (created with `torch.utils.data.Dataset`). |\n",
        "\n",
        "> **Note:** The `torch.utils.data.Dataset` and `torch.utils.data.DataLoader` classes aren't only for computer vision in PyTorch, they are capable of dealing with many different types of data.\n",
        "\n",
        "Now we've covered some of the most important PyTorch computer vision libraries, let's import the relevant dependencies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48d6bfe7-91da-44eb-9ab6-7c41c1e9fa8e",
      "metadata": {
        "id": "48d6bfe7-91da-44eb-9ab6-7c41c1e9fa8e"
      },
      "source": [
        "### Getting a dataset\n",
        "\n",
        "To begin working on a computer vision problem, let's get a computer vision dataset.\n",
        "\n",
        "We're going to start with FashionMNIST.\n",
        "\n",
        "MNIST stands for Modified National Institute of Standards and Technology.\n",
        "\n",
        "The [original MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) contains thousands of examples of handwritten digits (from 0 to 9) and was used to build computer vision models to identify numbers for postal services.\n",
        "\n",
        "[FashionMNIST](https://github.com/zalandoresearch/fashion-mnist), made by Zalando Research, is a similar setup. Except it contains grayscale images of 10 different kinds of clothing. The images represent fashion items rather than handwritten digits, so each class is more diverse, and the problem turns out to be significantly more challenging than the traditional MNIST dataset.\n",
        "\n",
        "![example image of FashionMNIST](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-fashion-mnist-slide.png)\n",
        "*`torchvision.datasets` contains a lot of example datasets you can use to practice writing computer vision code on. FashionMNIST is one of those datasets. And since it has 10 different image classes (different types of clothing), it's a multi-class classification problem.*\n",
        "\n",
        "Later, we'll be building a computer vision neural network to identify the different styles of clothing in these images.\n",
        "\n",
        "PyTorch has a bunch of common computer vision datasets stored in `torchvision.datasets`.\n",
        "\n",
        "Including FashionMNIST in [`torchvision.datasets.FashionMNIST()`](https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html).\n",
        "\n",
        "To download it, we provide the following parameters:\n",
        "* `root: str` - which folder do you want to download the data to?\n",
        "* `train: Bool` - do you want the training or test split?\n",
        "* `download: Bool` - should the data be downloaded?\n",
        "* `transform: torchvision.transforms` - what transformations would you like to do on the data?\n",
        "* `target_transform` - you can transform the targets (labels) if you like too.\n",
        "\n",
        "Many other datasets in `torchvision` have these parameter options."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "486f8377-6810-4367-859d-69dccc7aef95",
      "metadata": {
        "id": "486f8377-6810-4367-859d-69dccc7aef95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d7e3dc4-cb3a-4aed-d958-ea1db1e00a47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:02<00:00, 10591233.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 173499.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 3250155.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 25195189.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Setup training data\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\", # where to download data to?\n",
        "    train=True, # get training data\n",
        "    download=True, # download data if it doesn't exist on disk\n",
        "    transform=ToTensor(), # images come as PIL format, we want to turn into Torch tensors\n",
        "    target_transform=None # you can transform labels as well\n",
        ")\n",
        "\n",
        "# Setup testing data\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False, # get test data\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check out the shape of the training set."
      ],
      "metadata": {
        "id": "b_Qq4vhRE3Y1"
      },
      "id": "b_Qq4vhRE3Y1"
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.data.shape # (number of examples, height, width)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jfMWPpHEwuk",
        "outputId": "ab78cea3-a9df-415c-80fe-2d4e8c93c75d"
      },
      "id": "2jfMWPpHEwuk",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([60000, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.targets.shape # (number of labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fvr5bhw-E9uq",
        "outputId": "28ad5d5e-c644-47e6-b132-ddcd117ce261"
      },
      "id": "Fvr5bhw-E9uq",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([60000])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a63246f6-3645-49de-88fe-ec18e78bfbaf",
      "metadata": {
        "id": "a63246f6-3645-49de-88fe-ec18e78bfbaf"
      },
      "source": [
        "Let's check out the first sample of the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "43bfd3d9-a132-41e8-8ccd-5ae25a7da59a",
      "metadata": {
        "id": "43bfd3d9-a132-41e8-8ccd-5ae25a7da59a",
        "outputId": "4fc9649f-ff30-4504-d26a-26fe37c814f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
              "           0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0039, 0.0039, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
              "           0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
              "           0.0157, 0.0000, 0.0000, 0.0118],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
              "           0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0471, 0.0392, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
              "           0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
              "           0.3020, 0.5098, 0.2824, 0.0588],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
              "           0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
              "           0.5529, 0.3451, 0.6745, 0.2588],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
              "           0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
              "           0.4824, 0.7686, 0.8980, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
              "           0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
              "           0.8745, 0.9608, 0.6784, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
              "           0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
              "           0.8627, 0.9529, 0.7922, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
              "           0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
              "           0.8863, 0.7725, 0.8196, 0.2039],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
              "           0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
              "           0.9608, 0.4667, 0.6549, 0.2196],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
              "           0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
              "           0.8510, 0.8196, 0.3608, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
              "           0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
              "           0.8549, 1.0000, 0.3020, 0.0000],\n",
              "          [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
              "           0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
              "           0.8784, 0.9569, 0.6235, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
              "           0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
              "           0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
              "           0.9137, 0.9333, 0.8431, 0.0000],\n",
              "          [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
              "           0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
              "           0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
              "           0.8627, 0.9098, 0.9647, 0.0000],\n",
              "          [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
              "           0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
              "           0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
              "           0.8706, 0.8941, 0.8824, 0.0000],\n",
              "          [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
              "           0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
              "           0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
              "           0.8745, 0.8784, 0.8980, 0.1137],\n",
              "          [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
              "           0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
              "           0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
              "           0.8627, 0.8667, 0.9020, 0.2627],\n",
              "          [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
              "           0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
              "           0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
              "           0.7098, 0.8039, 0.8078, 0.4510],\n",
              "          [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
              "           0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
              "           0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
              "           0.6549, 0.6941, 0.8235, 0.3608],\n",
              "          [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
              "           0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
              "           0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
              "           0.7529, 0.8471, 0.6667, 0.0000],\n",
              "          [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
              "           0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
              "           0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
              "           0.3882, 0.2275, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
              "           0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
              " 9)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# See first training sample (input and label)\n",
        "image, label = train_data[0]\n",
        "image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ad9d782-06cb-4591-ae3c-3a8b2389a1b2",
      "metadata": {
        "id": "9ad9d782-06cb-4591-ae3c-3a8b2389a1b2"
      },
      "source": [
        "### Input and output shapes of a computer vision model\n",
        "\n",
        "We've got a big tensor of values (the image) leading to a single value for the target (the label).\n",
        "\n",
        "Let's see the image shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c2997d9f-b574-4d23-aa34-1a4df1751226",
      "metadata": {
        "id": "c2997d9f-b574-4d23-aa34-1a4df1751226",
        "outputId": "6a6893e9-2abe-4232-a9fc-cc9867b5d18e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# What's the shape of the image?\n",
        "image.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5326a05-f807-448d-99a3-6d03fc8739f8",
      "metadata": {
        "id": "b5326a05-f807-448d-99a3-6d03fc8739f8"
      },
      "source": [
        "The shape of the image tensor is `[1, 28, 28]` or more specifically:\n",
        "\n",
        "```\n",
        "[color_channels=1, height=28, width=28]\n",
        "```\n",
        "\n",
        "Having `color_channels=1` means the image is grayscale.\n",
        "\n",
        "If `color_channels=3`, the image comes in pixel values for red, green and blue (this is also known a the [RGB color model](https://en.wikipedia.org/wiki/RGB_color_model)).\n",
        "\n",
        "The order of our current tensor is often referred to as `CHW` (Color Channels, Height, Width).\n",
        "\n",
        "There's debate on whether images should be represented as `CHW` (color channels first) or `HWC` (color channels last).\n",
        "\n",
        "> **Note:** You'll also see `NCHW` and `NHWC` formats where `N` stands for *number of images*. For example if you have a `batch_size=32`, your tensor shape may be `[32, 1, 28, 28]`. We'll cover batch sizes later.\n",
        "\n",
        "PyTorch generally accepts `NCHW` (channels first) as the default for many operators.\n",
        "\n",
        "However, PyTorch also explains that `NHWC` (channels last) performs better and is [considered best practice](https://pytorch.org/blog/tensor-memory-format-matters/#pytorch-best-practice).\n",
        "\n",
        "For now, since our dataset and models are relatively small, this won't make too much of a difference.\n",
        "\n",
        "But keep it in mind for when you're working on larger image datasets and using convolutional neural networks (we'll see these later).\n",
        "\n",
        "Let's check out more shapes of our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fc4f768c-c3f6-454d-a633-673ad1d6eca0",
      "metadata": {
        "id": "fc4f768c-c3f6-454d-a633-673ad1d6eca0",
        "outputId": "80fbbec6-26ff-4676-a86d-53b37c25016a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 60000, 10000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# How many samples are there?\n",
        "len(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e0267d5-946b-4c53-af69-61acd3527972",
      "metadata": {
        "id": "6e0267d5-946b-4c53-af69-61acd3527972"
      },
      "source": [
        "So we've got 60,000 training samples and 10,000 testing samples.\n",
        "\n",
        "What classes are there?\n",
        "\n",
        "We can find these via the `.classes` attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e22849c6-d93f-4b38-8403-5ebf0deaf008",
      "metadata": {
        "id": "e22849c6-d93f-4b38-8403-5ebf0deaf008",
        "outputId": "8bb46cb6-ccb8-4372-c358-4dc4699253d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['T-shirt/top',\n",
              " 'Trouser',\n",
              " 'Pullover',\n",
              " 'Dress',\n",
              " 'Coat',\n",
              " 'Sandal',\n",
              " 'Shirt',\n",
              " 'Sneaker',\n",
              " 'Bag',\n",
              " 'Ankle boot']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# See classes\n",
        "class_names = train_data.classes\n",
        "class_names"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abdd225c-5742-4d9c-8e8d-fb30a9c3cb6e",
      "metadata": {
        "id": "abdd225c-5742-4d9c-8e8d-fb30a9c3cb6e"
      },
      "source": [
        "Sweet! It looks like we're dealing with 10 different kinds of clothes.\n",
        "\n",
        "Because we're working with 10 different classes, it means our problem is **multi-class classification**.\n",
        "\n",
        "Let's get visual."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb625d80-6a98-471e-a758-4de0ce0f3a64",
      "metadata": {
        "id": "fb625d80-6a98-471e-a758-4de0ce0f3a64"
      },
      "source": [
        "### Visualizing our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b1df1f2c-28c9-43bf-aaef-cf996c9ae1c5",
      "metadata": {
        "id": "b1df1f2c-28c9-43bf-aaef-cf996c9ae1c5",
        "outputId": "8b7a620f-8a28-42fb-e9c2-0e21cf65b830",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: torch.Size([1, 28, 28])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "image, label = train_data[0]\n",
        "print(f\"Image shape: {image.shape}\")\n",
        "plt.imshow(image.squeeze()) # image shape is [1, 28, 28] (colour channels, height, width)\n",
        "plt.title(class_names[label]);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adb19c5c-2f2b-4aaf-8300-256f3594e2db",
      "metadata": {
        "id": "adb19c5c-2f2b-4aaf-8300-256f3594e2db"
      },
      "source": [
        "We can turn the image into grayscale using the `cmap` parameter of `plt.imshow()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "92f09917-88f7-4446-b65f-baae586914c9",
      "metadata": {
        "id": "92f09917-88f7-4446-b65f-baae586914c9",
        "outputId": "8c40ab88-1b93-4fe8-d689-f013606bb5e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "plt.title(class_names[label]);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a09388a-d754-485f-aa26-4e7a0f782967",
      "metadata": {
        "id": "9a09388a-d754-485f-aa26-4e7a0f782967"
      },
      "source": [
        "Beautiful, well as beautiful as a pixelated grayscale ankle boot can get.\n",
        "\n",
        "Let's view a few more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7188ed7a-5959-48c4-ac7f-19129a2adc83",
      "metadata": {
        "id": "7188ed7a-5959-48c4-ac7f-19129a2adc83",
        "outputId": "5a62fc82-12dc-44e8-d3bb-40e479e78f3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 900x900 with 16 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAswAAALfCAYAAAB1k5QvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACmVUlEQVR4nOzdd3xVVb7//08MJISEhBYICZBA6EVQQLAgRRAVRB1QYdQBbIyKZcYZv5Y7V51Rx4qoWOfnKCIOlgErqKioI+hgAwWl9xpK6E1h//7wQa5hvddmHxJIez0fj3ncy4e1zt5nn7XXWR72Z33igiAIDAAAAIB0TEmfAAAAAFCasWAGAAAAQrBgBgAAAEKwYAYAAABCsGAGAAAAQrBgBgAAAEKwYAYAAABCsGAGAAAAQrBgBgAAAEJU+AXz0KFDLSUl5ZDtunfvbt27dy+243bv3t3atGlTbK8HFFVcXJyNGDHikO2ef/55i4uLs6VLlx75kwKAco51SNlQJhfMTzzxhMXFxVnnzp1L+lTKpHvuucdef/31kj4NHEXff/+9DRw40LKzs61KlSqWlZVlvXv3tscee+yIH5vxhqPhwH/I/fp/derUsR49etjkyZNL+vRQzrAOKZqy+L1QJhfM48aNs5ycHJsxY4YtXLiwpE+nzCmLAxWHb/r06daxY0ebNWuWXXHFFTZ69Gi7/PLL7ZhjjrFHHnkk5te75JJLbNeuXZadnR2pPeMNR9Nf//pXGzt2rL3wwgt200032fr16+2ss86yt99+u6RPDeUI65CiKYvfC5VK+gRitWTJEps+fbpNmDDBhg8fbuPGjbPbb7+9pE8LKLXuvvtuS0tLsy+//NKqV69e6O/y8vJifr34+HiLj48PbRMEge3evduSkpJifn2gKM4880zr2LFjwZ8vu+wyq1u3rv3rX/+yfv36leCZobxgHVIxlblfmMeNG2c1atSwvn372sCBA23cuHFOm6VLl1pcXJw9+OCD9swzz1hubq4lJiZap06d7MsvvzzkMWbOnGnp6enWvXt32759u7fdnj177Pbbb7cmTZpYYmKiNWjQwG666Sbbs2dP5Pfz9ddf20knnWRJSUnWqFEje+qpp5w2eXl5BZN+lSpVrF27djZmzBin3Y4dO+zGG2+0Bg0aWGJiojVv3twefPBBC4KgoE1cXJzt2LHDxowZU/DPlkOHDo18vih7Fi1aZK1bt3YWy2ZmderUcWKvv/66tWnTxhITE61169b27rvvFvp79QxzTk6O9evXz9577z3r2LGjJSUl2dNPP814Q4mrXr26JSUlWaVK//f70IMPPmgnnXSS1apVy5KSkqxDhw722muvOX137dpl1113ndWuXduqVatm/fv3t1WrVllcXJzdcccdR/FdoDRhHVJB1yFBGdOiRYvgsssuC4IgCD799NPAzIIZM2YUarNkyZLAzILjjjsuaNKkSXDfffcF999/f1C7du2gfv36wd69ewvaDhkyJEhOTi7484wZM4IaNWoEvXv3Dnbu3FkQ79atW9CtW7eCP+/bty84/fTTg6pVqwY33HBD8PTTTwcjRowIKlWqFJxzzjmHfB/dunULMjMzgzp16gQjRowIHn300eCUU04JzCx49tlnC9rt3LkzaNmyZVC5cuXgD3/4Q/Doo48GXbt2DcwsGDVqVEG7/fv3Bz179gzi4uKCyy+/PBg9enRw9tlnB2YW3HDDDQXtxo4dGyQmJgZdu3YNxo4dG4wdOzaYPn36oS88yqzTTz89qFatWvD999+HtjOzoF27dkG9evWCv/3tb8GoUaOCxo0bB1WrVg02bNhQ0O65554LzCxYsmRJQSw7Ozto0qRJUKNGjeDmm28OnnrqqWDq1KmMNxw1B8blBx98EKxfvz7Iy8sLZs+eHQwfPjw45phjgvfff7+gbf369YOrr746GD16dDBy5MjghBNOCMwsePvttwu95gUXXBCYWXDJJZcEjz/+eHDBBRcE7dq1C8wsuP3224/yO0RpwTqkYq5DytSC+auvvgrMLJgyZUoQBL98OPXr1w+uv/76Qu0ODNRatWoFmzZtKoi/8cYbgZkFb731VkHs1wP1s88+C1JTU4O+ffsGu3fvLvSaBw/UsWPHBsccc0zwn//8p1C7p556KjCzYNq0aaHvpVu3boGZBQ899FBBbM+ePUH79u2DOnXqFNxMo0aNCswsePHFFwva7d27NzjxxBODlJSUYOvWrUEQBMHrr78emFlw1113FTrOwIEDg7i4uGDhwoUFseTk5GDIkCGh54fy4/333w/i4+OD+Pj44MQTTwxuuumm4L333is0YQfBLwvmhISEQmNl1qxZgZkFjz32WEHMt2A2s+Ddd991js94w9FwYFwe/L/ExMTg+eefL9T214uQIPhlTm3Tpk3Qs2fPgtjXX3/tfNEHQRAMHTqUBXMFxjrkFxVxHVKmHskYN26c1a1b13r06GFmv/ysf+GFF9r48eNt3759TvsLL7zQatSoUfDnrl27mpnZ4sWLnbZTp061Pn362GmnnWYTJkywxMTE0HN59dVXrWXLltaiRQvbsGFDwf969uxZ8HqHUqlSJRs+fHjBnxMSEmz48OGWl5dnX3/9tZmZTZo0yTIyMmzw4MEF7SpXrmzXXXedbd++3T755JOCdvHx8XbdddcVOsaNN95oQRCQJV6B9e7d2z7//HPr37+/zZo1y+6//37r06ePZWVl2Ztvvlmoba9evSw3N7fgz8cee6ylpqbKe+ZgjRo1sj59+hT7+QOxePzxx23KlCk2ZcoUe/HFF61Hjx52+eWX24QJEwra/PrZ+vz8fNuyZYt17drVvvnmm4L4gUeRrr766kKvf+211x7hd4DSjHXILyriOqTMLJj37dtn48ePtx49etiSJUts4cKFtnDhQuvcubOtW7fOPvzwQ6dPw4YNC/35wKDNz88vFN+9e7f17dvXjjvuOHvllVcsISHhkOezYMECmzNnjqWnpxf6X7NmzcwsWjJVZmamJScnF4od6H/g+dBly5ZZ06ZN7ZhjCn9ULVu2LPj7A/83MzPTqlWrFtoOFVOnTp1swoQJlp+fbzNmzLBbbrnFtm3bZgMHDrQffvihoN3B94zZL/fNwfeM0qhRo2I9Z+BwnHDCCdarVy/r1auXXXTRRfbOO+9Yq1atbMSIEbZ3714zM3v77betS5cuVqVKFatZs6alp6fbk08+aVu2bCl4nWXLltkxxxzjjOsmTZoc1feD0oN1SMVeh5SZXTI++ugjW7NmjY0fP97Gjx/v/P24cePs9NNPLxTzZfIHv3r43MwsMTHRzjrrLHvjjTfs3XffjZRJvX//fmvbtq2NHDlS/n2DBg0O+RrA0ZaQkGCdOnWyTp06WbNmzWzYsGH26quvFmR4R71nFHbEQGl0zDHHWI8ePeyRRx6xBQsW2KZNm6x///526qmn2hNPPGH16tWzypUr23PPPWcvvfRSSZ8uSjHWIRVbmVkwjxs3zurUqWOPP/6483cTJkywiRMn2lNPPXVYX9pxcXE2btw4O+ecc+z888+3yZMnH7KaTm5urs2aNctOO+00i4uLi/mYZmarV6+2HTt2FPqvu/nz55vZL7sOmJllZ2fbd999Z/v37y/0X3dz584t+PsD//eDDz6wbdu2Ffqvu4PbHXi/wIGtt9asWXNEj8N4Q0n7+eefzcxs+/bt9u9//9uqVKli7733XqF/8n7uuecK9cnOzrb9+/fbkiVLrGnTpgVx9tytuFiHVOx1SJl4JGPXrl02YcIE69evnw0cOND534gRI2zbtm3O85ixSEhIsAkTJlinTp3s7LPPthkzZoS2v+CCC2zVqlX2j3/8Q57vjh07DnnMn3/+2Z5++umCP+/du9eefvppS09Ptw4dOpiZ2VlnnWVr1661l19+uVC/xx57zFJSUqxbt24F7fbt22ejR48udIyHH37Y4uLi7MwzzyyIJScn2+bNmw95figfpk6dKn8hnjRpkpmZNW/e/Igen/GGkvTTTz/Z+++/bwkJCdayZUuLj4+3uLi4Qs+bLl261CmicOB5/CeeeKJQ/GhUx0TpwzqEdUiZ+IX5zTfftG3btln//v3l33fp0sXS09Nt3LhxduGFFx72cZKSkuztt9+2nj172plnnmmffPKJt876JZdcYq+88or9/ve/t6lTp9rJJ59s+/bts7lz59orr7xSsB9tmMzMTLvvvvts6dKl1qxZM3v55Zdt5syZ9swzz1jlypXNzOzKK6+0p59+2oYOHWpff/215eTk2GuvvWbTpk2zUaNGFfxX3Nlnn209evSw2267zZYuXWrt2rWz999/39544w274YYbCiVydejQwT744AMbOXKkZWZmWqNGjSjvWY5de+21tnPnTjvvvPOsRYsWtnfvXps+fbq9/PLLlpOTY8OGDTuix2e84WiaPHlywS9aeXl59tJLL9mCBQvs5ptvttTUVOvbt6+NHDnSzjjjDPvtb39reXl59vjjj1uTJk3su+++K3idDh062IABA2zUqFG2ceNG69Kli33yyScFv76VxV/IcPhYh7AOKRPbyp199tlBlSpVgh07dnjbDB06NKhcuXKwYcOGgu1cHnjgAaedHbQd0MH7HwZBEGzYsCFo1apVkJGRESxYsCAIAnc7lyD4ZVuV++67L2jdunWQmJgY1KhRI+jQoUNw5513Blu2bAl9T926dQtat24dfPXVV8GJJ54YVKlSJcjOzg5Gjx7ttF23bl0wbNiwoHbt2kFCQkLQtm3b4LnnnnPabdu2LfjDH/4QZGZmBpUrVw6aNm0aPPDAA8H+/fsLtZs7d25w6qmnBklJSYGZlbmtXRCbyZMnB5deemnQokWLICUlJUhISAiaNGkSXHvttcG6desK2plZcM011zj9s7OzC40R37Zyffv2lcdnvOFoUNvKValSJWjfvn3w5JNPFpoHn3322aBp06ZBYmJi0KJFi+C5554Lbr/99uDgr8QdO3YE11xzTVCzZs0gJSUlOPfcc4N58+YFZhbce++9R/stogSxDmEdEhcEEbJ5AACAzZw504477jh78cUX7aKLLirp0wFwlJSJZ5gBADjadu3a5cRGjRplxxxzjJ166qklcEYASkqZeIYZAICj7f7777evv/7aevToYZUqVbLJkyfb5MmT7corr2TLLqCC4ZEMAACEKVOm2J133mk//PCDbd++3Ro2bGiXXHKJ3XbbbVapEr83ARUJC2YAAAAgBM8wAwAAACFYMAMAAAAhWDADAAAAISJnLVDVCEdKST5GXx7GtXoP6pomJyfL/oMGDXJi27dvd2L5+fmyf0ZGhhPbtm2bbDtx4kQZL48Y1yiPGNcoj6KMa35hBgAAAEKwYAYAAABCsGAGAAAAQrBgBgAAAEJQqggohaIm8oXFD9a3b18Zr1GjhhOrXLmyE1PJfWZmbdu2dWItW7aUbY9m0l8s1xAAgDD8wgwAAACEYMEMAAAAhGDBDAAAAIRgwQwAAACEYMEMAAAAhIgLIqaNU5JSa968uRNLT0+XbXft2uXE1G4EZmZ79+6N1Hbfvn2y//79+yPFfMc65hj3v6VUzEyPjWrVqsm23377rRNTZZiPlvIwrlNTU53YgAEDnFjHjh1l/+nTpzux//f//p8TU7thmJmtXr3aif3tb3+TbVV57mXLljmxKVOmyP5btmyR8dKIEsIojxjX5Y/vupbGXYUyMzNlXO3i5FvzzJw504lRGhsAAAAoIhbMAAAAQAgWzAAAAEAIFswAAABACJL+BF9ym3qA/O6773Zi9erVk/337NnjxHwlhFWCX9WqVZ2YStgz0+WOfXbv3u3EKlVyq6avXLlS9ldDyPew/aOPPurEJk2adKhTPGJK67g+9thjndhxxx0n26rP+ueff3ZiDRs2lP3VWFPXJTc3V/Z///33ndiCBQtk2yZNmkQ6vi9pdP369U7MlyC4cOFCGT9aSjJhRs1hsZxPLPdFaUwMwpFD0l/RqPdQ1HtTxXzfwer7olGjRrLtvHnznNiOHTsOdYqhsrOzZbxu3bpOTCWJ+6SlpTkxlRBvZvbaa685sSjvi1+YAQAAgBAsmAEAAIAQLJgBAACAECyYAQAAgBAsmAEAAIAQ7lYIiCljVe18oXadMNOlsefPny/bVqlSxYnFx8c7sU2bNsn+tWrVcmK+3T8SEhIiHct3XX766ScnlpiYKNsuWrRIxiuqCy64QMZbtWrlxJYsWSLb5ufnOzE1LtTnZKazi1WG9Ycffij7qx05ateuLduqMujqvFS5bTOz6tWrO7EhQ4bItv/+97+dmCqJWtGpe33fvn2R+6uxqsaEj2+nH3UOakcV37ym3pfa/ac4dl5Qc6OK+c41luul7s2kpKTIr6nazp07V7ZV9ytKVlF3KWnevLkTS09Pl2137tzpxHxjRRkwYEDktmoXMDWG1XeAmR6ram1j5l+jHQq/MAMAAAAhWDADAAAAIVgwAwAAACFYMAMAAAAhSPorIpWw4kuYUQ+g+x5KV4koqm1WVpbsrxJDVMKLmU6OUckpvlKbqq1KzjE7/Ifty4OMjAwn5iuNPmfOHCfmS05Sn59K7PFde5U0qJJGt27dKvurhBFfwpG6N9R5+Upjq7aLFy+Wbdu3b+/EKkrSXyyJQbEk+J1++ulOrG/fvk5s6dKlsv+GDRucWNu2bWVblcSjEqd9c6hKyFb3kC8RL5ZkwKiv67vWURP5zHRpY3W91bUy0+Xpp02bJtu+/vrrMo7DdyRKi6vX9I1fNTf7kvRTUlKcWJ06dZzYZZddJvurublmzZqyrVqfqHtbJSKa6fvFd78d7mfAL8wAAABACBbMAAAAQAgWzAAAAEAIFswAAABACJL+ikglIakkLDOdyOR7AF4lzakH1X1VsmJJOFHnq17Xdyx1rpmZmbKt74H9iqBFixZObPPmzbJtLNXztmzZ4sRiGSvq81Pn5auwpMaVL8FUJTepqpa+RKxt27Y5MV+CoDpflQhzJJJwyrp77rlHxlUinUrOU1UWzfTn75sTTjzxRCemqkLGMt+qseq7L9S5+hKflajzqplOblLX2sxs3rx5Tkxdb1/i7dVXX+3EunXrJtt+9NFHMo7SzzcHq6TRHTt2yLbqHlJJfytXrpT9VRVZ37HUvaE2NfD1V/e2b244XPzCDAAAAIRgwQwAAACEYMEMAAAAhGDBDAAAAIRgwQwAAACEYJeMIkpOTnZivjKTKotTZf2b6UxWleHvK3+qsq59GaMqm1q19WVtq10WfJmsqjRuReErF67EkmFftWpVJ6bGha80thqvajcCXwlitXOAb6wo6rr4xrW6Lr5dFlSGttppZP369Yc6xTJHXVP1mZqZffDBB07s8ccfl23VjiYDBgxwYldddZXsX69ePSe2evVq2VaNoTPPPNOJqTLyZnpcq91XfDtfqPm6qKV2ValhM72jga+8eHZ2thPr2rVr5HNS78F3bzdt2lTGUXKi7vSjdpgwM2vUqJET882B6jsjPT3diW3atEn2z8jIcGK+tYGamzdu3OjEfGNV3du+7zzfDiKHwi/MAAAAQAgWzAAAAEAIFswAAABACBbMAAAAQAiS/opIlY70JZGoB/NVWVkzncikHor3JRimpqY6MV+pVBVXD8v7ks5UMpHvYfuKTCX8rFmzRrZViRnLli2TbVUinEqA8CU6RE3Q8yWG1KpVK9LxfXGVSOZLhlVtVeKtmR6vKmGqPCb9qXuye/fusm3dunWd2MSJE2Xbl19+2YmpcZWbmyv7q88qJydHth05cqQTU/PaySefLPurEtJqbvYl8qm5VSVcmelxrd5rfn6+7P/jjz86MV+SZqtWrZyYSoSKZb5XicNh54DSz5dgqpKhfXOgugd8CdmKWh/5khGjzsO+ZNZYEtXVpgpR8AszAAAAEIIFMwAAABCCBTMAAAAQggUzAAAAEIIFMwAAABCCXTKEqGVOzXSGta8Etcrw92VoK+p1fVnMKq4yVs105vnmzZudmG83ArUjhu9YFZnaTWLRokWybbt27ZyYL4tY7SihSiP7MvwVNdZUdrWvrW/nDZWNr3ZJ+M9//iP7H3vssZGPpcawKs1cUQwbNkzGr7322siv0bBhQye2du3ayP3VWK1evbpse9lllzmxv/3tb07MtyNPixYtnJiag33UHOq7h5YvX+7E1q1b58S2bt0q+6vdM3y7h6gxvGDBAifm20FJfb81a9ZMtvWVnUfJiVoa27dDhLpffHO7mkPVmqVx48ayvyqN7btfa9as6cTU+POV1lbU96DZ4e/+wi/MAAAAQAgWzAAAAEAIFswAAABACBbMAAAAQIgKn/QX9QF6H5WA4Uv627JlixPzlZtWiXifffaZE/OVO1bH8iWRqHhSUpIT27Bhg+yfmZnpxL799lvZtiJTCW++JCBVGluVzzXzl2KPSvVXYziW0tq+tirhY+bMmU7MV9Y1Ly/PifnuN5X04ktcLW9U6df+/fvLtkOGDIn8umpeUPOl7/NXn9XSpUtl2w4dOjixwYMHO7FZs2bJ/h999JETO+GEE5zY4sWLZX81t2/cuFG2Pfvss53YtGnTnJhvDlaJVOq9mpl9+OGHTkyNa1+CorrfVXKXmX5fKBt8yXUrV650YtnZ2bKtSuhV8+r27dtlf5X8r+YQM7O5c+c6MZW46tsoQcVjaRsFvzADAAAAIVgwAwAAACFYMAMAAAAhWDADAAAAIUj6K2LSn0ra8lXYUQl+vmOppKdWrVpFPq9Vq1Y5sZ9//lm2VdW3VIKaSjo0M+vXr58T++abbw51iuWaqpx0zDHuf5/6kqNUYoQvUUG9bixV/VR/dSxfEolvvCuq8lIsyRrqHkpPT5dtVSKTL+GkvPnd737nxCZPnhy5v2/8+KrHRaXmOzX+zHQVzNNOO82J+SoNqsqaqlLh119/Lfu/8MILTsyXIKmSUdVYXbZsmex/+eWXOzGVDGsWvVqhb25R1dN891ubNm0iHQtHT9T1yYoVK2RcJfj5kj5V8rlKzvMlXk+fPt2J+ZLU1fpEzde+xG1Vvc/33XS43wP8wgwAAACEYMEMAAAAhGDBDAAAAIRgwQwAAACEYMEMAAAAhCiVu2QUdeeKI0Wd165du5yYKgdppsvV+jI+VcaoKpftywKtW7du5GMpKmu2c+fOkfsvWLAgctvySJXVVWNFZfaa6c/aR2Udq/Hj2yUl6i4bvjLederUiXROvmOpbH7fuarX9e3coMq1Vq9eXbYtb1R2+5gxYyL39823KkNe7bDgy5pXfLs+RC3hfMUVV8j+U6ZMcWLz5893Yr75+tZbb3VivjlUjbU+ffo4MVWa28zs888/d2Kq5LxZ9F1pfPeg2qlG7Zxhpnf6QNFEXd/4dqqJ2t93D6oduHzHUt9Par5VaxszPdZ88vPznVjNmjWdWFZWluy/cOFCJ7Zjxw7Z1rez0qHwCzMAAAAQggUzAAAAEIIFMwAAABCCBTMAAAAQolQm/R3NBL9YSgir5BT1AL0qK20WvSywmU6OUsfylStWSRy+B+CVBg0aRDq+jy+JpKJQiRHqs/Ylt6lkC5XIaaZLCKvEIF+pXJVcpO6LzMxM2V8lPPnuAZWIpMa6L2FJJe1lZGTItt99950TU5+LLzHFl5BZFqhEzHnz5kXu70sYimW+VGJJblJjQCWhrVy5UvY/5ZRTnJgqietLZv3xxx+dmK9UtLreGzZscGIff/yx7K/uTd9nEPV+iSVpzHes0pBsX95Evaa+dlHvwcaNG8u4mu/T0tJkW5Worr6zfOcUS0KwSsRT78FXXn7jxo2RXtPM/717KPzCDAAAAIRgwQwAAACEYMEMAAAAhGDBDAAAAIQolUl/R1MsSQ1NmzZ1YuoBdl9ynXow3le1SSV2FLXqUixJBMuXL3divuQodQ1UpbuKRFUEUwkQvmu6Zs0aJ6YSlsyij2FfIp36/NT4i6XymO9Yijp/X4KiSsTzJb6qaxtLwotK2ior1PVTCTw+vgRPNVZimZdiSfpTbdXnr8aqmVleXp4TizrWzfR870sQVIlI6r7w3e+xJFOqhCX1HmKZ7333K5X+il9RKxmrz0qNq/r168v+27Ztc2K+5DhVaU9V8fUlSKtz9a0NVPL2Dz/84MR8Sb4q8dZX2XXJkiUyfij8wgwAAACEYMEMAAAAhGDBDAAAAIRgwQwAAACEYMEMAAAAhKhQu2TEUoJX6d27txNTGafVqlWT/VWGdizZySoT1VeCWJWxVmWBfa+hsux9uxGoXUHat28v27711lsyXt5E3TnC95moTHhfGXU1rmIpf6vOS8V8n7/aEcRXelTdg1FjZnqXC1+GuTovdV1UyfuyTn1WsezE4NuRRVHXvzh2aFBzkBpXvnGtxLIbgdpVJJadgtRY9e2KpOZ73+eljqXmhlh2H/FdF3bJiCaWnS+i3oex9D/hhBOcmG9XI7U28H23zJ49O9Lr+nYUat26tRPzfed9++23Mn6wJk2ayLia8zZv3izbHu645hdmAAAAIAQLZgAAACAEC2YAAAAgBAtmAAAAIMQRSfqLJbkkliSMqHxldX0Pth/svPPOk3FVZlG9pi+xQyWM+M4patKeL2FJxX1JACqRRj1Av3PnTtlfJWj5SlJWFOoeiCWRT7X1JZOqtipB1JewFDUZ1pcIpvrHktykkph8iVzqevmuoboH1LXyJaGUZSoZ2Ve+VollrMRS8r2oyWlRy537XleNCV/J8Kiluc30/RJ1DjDT18WXDBlLkqyiziGWxEu4ilraWvF9ps2aNXNiar7duHGj7J+Tk+PEfMlxah5u1KiRE/OV4Vb9Fy1aFLmtel/5+fmyv5obfHN7LInCv8YvzAAAAEAIFswAAABACBbMAAAAQAgWzAAAAEAIFswAAABAiMgpsLGUflRxX8Zn1Oxe37FUxqkvE1np0aOHEzv22GNl25UrVzoxVQK6Ro0asr/aZcK384HK5lYZ2r7sVpVJ6ruGqjS2yjj1nasaGzVr1pRtK4qon58vW1fdF75xrY4Vtdy1L66y433nqsa1LztZjRV1XXzZ+aq/b5eFzMxMJ7Z9+3YndrgZ06WZmquuvPJK2fbvf/+7E/ONtagZ/r6dS9S49n1+UXeOiGWHCPX5xzIH+6xbt86JxbJzgmrruy7qGqhr5buH1P0Wy3dmSYplHRK1v8+R2MHLzCw1NdWJqblK7WZhps9rx44dTqx58+ayv/puV7vqmOldjdRY882h6nXVPWimPxt1LLWDmJnZpk2bnNiaNWtk26jz2MH4hRkAAAAIwYIZAAAACMGCGQAAAAjBghkAAAAIcUSS/pTDfcj6cPhK+Pbr18+JNWnSxImph8fNzGrXru3E1MP2sVyXrVu3yrgqa6muYSyJl77EDvVgf4MGDZxYLMk5KomhIomaiOdLzIllDKnyoaq0ue81VdKUStpbu3at7K+SSHylsVU8lnLJaqz67iE1BlWC4tGcm46W7777zok99dRTsq1K+vNd0+zsbCc2e/ZsJ+ZLAopaGj0sfjDfuFbJbXXq1HFivtLYeXl5Tkydv5lZRkaGE1MJ2b7kqliS0aLyJV6qUuK+a+grO16axFJavaiJfL4E0+TkZCfWuHFj2bZ69epOTI0LX8Ka6q++b9TaxCy2z1/NjXXr1nVivu8GtWZS94pZ9CTbxYsXy7h6v6+88opsm5ubG+lYB+MXZgAAACAEC2YAAAAgBAtmAAAAIAQLZgAAACBE5KQ/9fB3rVq1ZNuWLVu6B4qh6pB6iF8lFpnpB9DVg+a+11APq6vEEDOdNDd37lwnphL2zMzq1asX6TXNdOKiivmSEGKp5qReQyU++j4Dlciljl+RqGut7gFVZdHMbMWKFU5MJayZRa8qGEvCi+rvq4imkot8yUK+CoBRqbHqSxaJmgjj+wzKsnfeeceJff/997LtSSed5MSmT58u26oxrOYF31hTc7vv8ytq9TnVX31nXXTRRbL/zJkzIx/rf/7nf5xYnz59nJhKJDTT19CXtOebh6NSyZQqSdjMn/xZUopa1U8lzJnpyrTp6elOzDd/RU2cNtPzeP369Z2YLxl1/fr1TiwtLc2JbdmyRfZXiavqNc3MOnXq5MTU94BvzaW+B3xrAzUG1ToklqrRvnvlcOcWfmEGAAAAQrBgBgAAAEKwYAYAAABCsGAGAAAAQrBgBgAAAEJE3iVDadq0qYyr8qm+TPSi7gaxYcMGJ+bLLt2+fbsTU5m0qp1Z9AxxVRLVTGeH+jJp1e4fsZyryoRVu3SYmaWmpjqx1atXOzHfZ6gyYYuayV3WqWxudZ1SUlJkf7WjgW9XGpUhrcagb6caNa7Urji+zzRqCWMzfQ+oezuW0ti+MtzqPahdMirKWL3ppptk/A9/+IMT8+2S8fbbbzuxY4891omp7HYzvSOGL2O9qOWio+5coHakMdP3i+9c1W4S6li+nS/UdYnlflX3oO9c1dzk+379z3/+I+OlSY0aNWRcza2+uUp9LsuXL3divtLoiu+aqnNQ362+8a/mO7Xm8e1wotYBXbp0kW3VWiwrK8uJqTWEmdmSJUucmG+3JzVfq2vlK62tvjM///xz2da3dj0UfmEGAAAAQrBgBgAAAEKwYAYAAABCsGAGAAAAQkRO+lPlort27Srbrlu3zon5HmBXSTjbtm1zYsnJybK/KnXpS6SLWi7Yl4ilEgPUw+6+Uq8qYcCXRKAejPcl+CnqHGJJjlCJBSoBwHdeKjHAzF9Cs7xR40olNfg+k08++cSJdejQQbZVyYBREyh8bdW9otr5+BKWVCKMOi/ffKHaquQcM7MmTZo4MfUeilquuzRS12/27NmyrUoQffPNN2VbNQeq6+e7z1Uimi85Sn1W6vixlJBetmyZEzv99NNl/x9//NGJ+RLp1PfT4sWLnZhvrKn36ruHFHW/+pJZVdKUSp43M/vss88in0NJadWqlYxnZmY6MV9ZZpU0p74Dfd/Xah3ja6vGihrDvu9bNS7U973vvarvC9+5qpLd6rzmzJkj+6vy3Kq0tplOXlffo7731bBhQyf2+OOPy7a+pOZD4RdmAAAAIAQLZgAAACAEC2YAAAAgBAtmAAAAIAQLZgAAACBE5DTczp07O7F+/frJtj/88IMT85VpVNmZapeNVatWyf4qi9O3m4TKrlQ7RKjMTDOdNauym9UuH2Y6O1adv5nOGI0lu1XxlRBWmd9q9wbfLgnqdX2Z62q3lfJIXROVyezbDWLjxo1OzLfLhdplwJdhraj7Qp2r7zP1ZS0rapcA1V+VETfT49KXoX3iiSc6MTWGd+/eLfuXZSqT3vc5/fe//3ViJ5xwgmy7dOlSJ6Y+K98uGerz981halyrmG9eU/eWmptHjBgh+6tsfrWbgpneJUHFfCWEfTuFKGoeUP19JYg//fRTJ3bnnXdGPn5p45sX1ZrDNy+qeSGWXanq1q3rxHzfdfn5+U5MvQfffdGxY0cnpkprr169WvZX96DaUcRMz/nz5893Yr4dXdT19r0vdW+q1/XdK+pc1c4ZvvOKgl+YAQAAgBAsmAEAAIAQLJgBAACAECyYAQAAgBCRk/7eeustJ+ZLtjj33HOdWLNmzWRblXCmknDWrl0r+6sHxX0P66tEKlU+1FeGW8VVaW6VAGCmS5L6kiHVNRg7dqwTu+CCC2R/lczoK+vqK+V9MF8ypEpYUEkIZvp6lUdqrKl7wJcAofr7kh3UGFYJEL77Vb2uek1fyfg1a9Y4MV/5U0WNH18ij0pY8ZV8VonCKpl2wYIFhzrFMsd3rypqrKmyzj5qvvV9/moM+dqqcak+f3X+ZnoMqfti2rRpsr+aL30J3b7xejA1/sx0MqEvSVPdb6qM9/fffy/7+75zFF9ScklR81JWVpZsq75r1q9fL9tmZ2c7MZVg7PucYykBrb4bo34H+6jS5jVr1pRt1Vjzra9UMmTLli2dmG9MqfsllpLfah7zfWeqser7vHwJ7IfCL8wAAABACBbMAAAAQAgWzAAAAEAIFswAAABAiMhJf8q///3vyPEWLVrItoMHD3ZijRs3dmJNmzaV/dUD9L4HvVUShXpQ3JfooOKbN292Yr7KY3/5y1+cmC/hJKqHH35YxtV5+ZIhoybH+Cr9qevqSzBs0KCBjJc3USvK+ZKIFF/Sn0rmiyWRTlFJKL6kQfX5+5LOot5vvveq4r5KfSqJRB1LzTdmumJpWeG7/5QPP/zQiX300UeyrUqOUuPCV9FOJU77EoTVeFWfXyzvdcWKFU7MlzRaXsVSVdA355eU5s2bOzFf9cXly5c7Md93u/q+VGPFNwcqvnnJt6nAwVSCo5lOsFPvy1ctVX2mvvtV3VvqO9yXyBdLJWJ1v6trGMt3SyxrwSj4hRkAAAAIwYIZAAAACMGCGQAAAAjBghkAAAAIwYIZAAAACFGkXTJ8WZwqA3Hu3Lmy7e233x7pWL7sVpU1W69ePdm2du3aTkxl0vvKRK5evdqJzZs3T7Y9Wn7/+9/L+Lp165yYKp9pprNmVYatLxtZZcL6SmXm5eU5sfHjx8u2ZZnaFUaVcPWV9VVmzpwp4+3bt3dialz7MpbV56+yi3391Y4avuxk9Roqa79WrVqyv8pm91HnkJmZWaTXrCh8mehLly49uieCYlfadr6Ihfr+GDBggGyr5kDfzhFqN4hYdopSc5ivrXpdtfuGb/cYtSOFaqvKyPv4zlXN1zt37nRivl0nYrmGO3bscGLqPRRHuWvf/HYo/MIMAAAAhGDBDAAAAIRgwQwAAACEYMEMAAAAhIgLIj797Eu6A4rqcB/ALw5HalyrJI7q1as7MV8ChC9BU+ndu7cTO+2005zYqlWrIh8rIyPDifnOdeXKlU4sJSVFtlXJMdWqVXNiKrHEzOyFF15wYrGUX1Wf95Eaf+VxXAOlbVyrpGMzneBbo0YN2VaVhlaJbL4S0CruS0JTmyWoY/lKvqv5btu2bU7MN1+ra+jbwEElTvraFpW6Xir5O5brkp+fL9vOmDHDiUUZ1/zCDAAAAIRgwQwAAACEYMEMAAAAhGDBDAAAAIRgwQwAAACEYJcMlLjSlnVdHqjs4jZt2si2NWvWdGIqa9y3G4Xa0cKXSa2ywVV5+blz58r+ZQnjGuUR4xrlEbtkAAAAAEXEghkAAAAIwYIZAAAACMGCGQAAAAgROekPAAAAqIj4hRkAAAAIwYIZAAAACMGCGQAAAAjBghkAAAAIwYIZAAAACMGCGQAAAAjBghkAAAAIwYIZQLF5/vnnLS4uzpYuXRpz36FDh1pOTk6xnxMAwMV8HZtyvWCOi4uL9L+PP/64pE8VOGzff/+9DRw40LKzs61KlSqWlZVlvXv3tscee6ykTw04KhYtWmTDhw+3xo0bW5UqVSw1NdVOPvlke+SRR2zXrl1H5JgvvfSSjRo16oi8Nsov5uuyq1JJn8CRNHbs2EJ/fuGFF2zKlClOvGXLlkfztIBiM336dOvRo4c1bNjQrrjiCsvIyLAVK1bYF198YY888ohde+21JX2KwBH1zjvv2Pnnn2+JiYn2u9/9ztq0aWN79+61zz77zP785z/bnDlz7Jlnnin247700ks2e/Zsu+GGG4r9tVE+MV+XbeV6wXzxxRcX+vMXX3xhU6ZMceIH27lzp1WtWvVIntoRsWPHDktOTi7p08BRdPfdd1taWpp9+eWXVr169UJ/l5eXVzInBRwlS5YssUGDBll2drZ99NFHVq9evYK/u+aaa2zhwoX2zjvvlOAZAv+H+bpsK9ePZETRvXt3a9OmjX399dd26qmnWtWqVe3WW281s18G8GWXXWZ169a1KlWqWLt27WzMmDGF+n/88cfysY6lS5daXFycPf/88wWxtWvX2rBhw6x+/fqWmJho9erVs3POOcd5fmjy5MnWtWtXS05OtmrVqlnfvn1tzpw5hdoMHTrUUlJSbNGiRXbWWWdZtWrV7KKLLiq264KyYdGiRda6dWtn8jUzq1OnTsH//9xzz1nPnj2tTp06lpiYaK1atbInn3zS6ZOTk2P9+vWzzz77zE444QSrUqWKNW7c2F544QWn7Zw5c6xnz56WlJRk9evXt7vuusv279/vtHvjjTesb9++lpmZaYmJiZabm2t/+9vfbN++fUV786jw7r//ftu+fbs9++yzhRbLBzRp0sSuv/56MzP7+eef7W9/+5vl5uZaYmKi5eTk2K233mp79uwp1CfKeO3evbu98847tmzZsoJH+yra85yIHfN12Vauf2GOauPGjXbmmWfaoEGD7OKLL7a6devarl27rHv37rZw4UIbMWKENWrUyF599VUbOnSobd68uWASjsWAAQNszpw5du2111pOTo7l5eXZlClTbPny5QWT7dixY23IkCHWp08fu++++2znzp325JNP2imnnGLffvttoUn5559/tj59+tgpp5xiDz74YJn8VRxFk52dbZ9//rnNnj3b2rRp42335JNPWuvWra1///5WqVIle+utt+zqq6+2/fv32zXXXFOo7cKFC23gwIF22WWX2ZAhQ+yf//ynDR061Dp06GCtW7c2s1/+469Hjx72888/280332zJycn2zDPPWFJSknPs559/3lJSUuyPf/yjpaSk2EcffWT/+7//a1u3brUHHnigeC8IKpS33nrLGjdubCeddNIh215++eU2ZswYGzhwoN1444323//+1/7+97/bjz/+aBMnTixoF2W83nbbbbZlyxZbuXKlPfzww2ZmlpKScmTeJMoN5usyLqhArrnmmuDgt9ytW7fAzIKnnnqqUHzUqFGBmQUvvvhiQWzv3r3BiSeeGKSkpARbt24NgiAIpk6dGphZMHXq1EL9lyxZEphZ8NxzzwVBEAT5+fmBmQUPPPCA9/y2bdsWVK9ePbjiiisKxdeuXRukpaUVig8ZMiQws+Dmm2+O/P5R/rz//vtBfHx8EB8fH5x44onBTTfdFLz33nvB3r17C7XbuXOn07dPnz5B48aNC8Wys7MDMws+/fTTglheXl6QmJgY3HjjjQWxG264ITCz4L///W+hdmlpaYGZBUuWLAk99vDhw4OqVasGu3fvLogNGTIkyM7OjvzeUbFt2bIlMLPgnHPOOWTbmTNnBmYWXH755YXif/rTnwIzCz766KOCWNTx2rdvX8YrYsJ8XbZV+EcyzMwSExNt2LBhhWKTJk2yjIwMGzx4cEGscuXKdt1119n27dvtk08+iekYSUlJlpCQYB9//LHl5+fLNlOmTLHNmzfb4MGDbcOGDQX/i4+Pt86dO9vUqVOdPldddVVM54HypXfv3vb5559b//79bdasWXb//fdbnz59LCsry958882Cdr/+JWHLli22YcMG69atmy1evNi2bNlS6DVbtWplXbt2Lfhzenq6NW/e3BYvXlwQmzRpknXp0sVOOOGEQu3UY0G/Pva2bdtsw4YN1rVrV9u5c6fNnTu3aBcAFdbWrVvNzKxatWqHbDtp0iQzM/vjH/9YKH7jjTeamRV6zpnxiiOF+bpsY8FsZllZWZaQkFAotmzZMmvatKkdc0zhS3RgR41ly5bFdIzExES77777bPLkyVa3bl079dRT7f7777e1a9cWtFmwYIGZmfXs2dPS09ML/e/99993kgIqVapk9evXj+k8UP506tTJJkyYYPn5+TZjxgy75ZZbbNu2bTZw4ED74YcfzMxs2rRp1qtXL0tOTrbq1atbenp6wbP6B0/ADRs2dI5Ro0aNQv+hd+D+OFjz5s2d2Jw5c+y8886ztLQ0S01NtfT09ILE24OPDUSVmppqZr98qR/KsmXL7JhjjrEmTZoUimdkZFj16tULzeeMVxxJzNdlF88wm8nneKKKi4uTcfWA/A033GBnn322vf766/bee+/ZX/7yF/v73/9uH330kR133HEFD+CPHTvWMjIynP6VKhX+uBITE50FPSquhIQE69Spk3Xq1MmaNWtmw4YNs1dffdUuvvhiO+2006xFixY2cuRIa9CggSUkJNikSZPs4YcfdhI/4uPj5esHQRDzOW3evNm6detmqamp9te//tVyc3OtSpUq9s0339j/+3//TyadAFGkpqZaZmamzZ49O3If33x9AOMVRwvzddnDgtkjOzvbvvvuO9u/f3+hRemBf5LIzs42s1/+S87sl4H2a75foHNzc+3GG2+0G2+80RYsWGDt27e3hx56yF588UXLzc01s1+yZXv16lXcbwkVSMeOHc3MbM2aNfbWW2/Znj177M033yz0a4R6xCeq7Ozsgn8R+bV58+YV+vPHH39sGzdutAkTJtipp55aEF+yZMlhHxs4oF+/fvbMM8/Y559/bieeeKK3XXZ2tu3fv98WLFhQaN/9devW2ebNmwvm81jG66EW30BUzNdlAz9Pepx11lm2du1ae/nllwtiP//8sz322GOWkpJi3bp1M7NfBmJ8fLx9+umnhfo/8cQThf68c+dO2717d6FYbm6uVatWrWBboz59+lhqaqrdc8899tNPPznntH79+mJ5byg/pk6dKn9JOPDMZvPmzQt+gfh1uy1btthzzz132Mc966yz7IsvvrAZM2YUxNavX2/jxo0r1E4de+/evc79ARyOm266yZKTk+3yyy+3devWOX+/aNEie+SRR+yss84yM3Mq840cOdLMzPr27WtmsY3X5OTkCv9P1IgN83XZxi/MHldeeaU9/fTTNnToUPv6668tJyfHXnvtNZs2bZqNGjWqINEkLS3Nzj//fHvssccsLi7OcnNz7e2333aeN54/f76ddtppdsEFF1irVq2sUqVKNnHiRFu3bp0NGjTIzH75J8Ynn3zSLrnkEjv++ONt0KBBlp6ebsuXL7d33nnHTj75ZBs9evRRvxYova699lrbuXOnnXfeedaiRQvbu3evTZ8+3V5++WXLycmxYcOG2bp16ywhIcHOPvtsGz58uG3fvt3+8Y9/WJ06dWzNmjWHddybbrrJxo4da2eccYZdf/31BdsUHfiXmQNOOukkq1Gjhg0ZMsSuu+46i4uLs7Fjxx7WPxcCB8vNzbWXXnrJLrzwQmvZsmWhSn/Tp08v2Ar0+uuvtyFDhtgzzzxT8M/OM2bMsDFjxti5555rPXr0MLPYxmuHDh3s5Zdftj/+8Y/WqVMnS0lJsbPPPvtoXwKUIczXZVzJbM5RMnzbyrVu3Vq2X7duXTBs2LCgdu3aQUJCQtC2bduCbeJ+bf369cGAAQOCqlWrBjVq1AiGDx8ezJ49u9C2chs2bAiuueaaoEWLFkFycnKQlpYWdO7cOXjllVec15s6dWrQp0+fIC0tLahSpUqQm5sbDB06NPjqq68K2gwZMiRITk4+/IuBcmHy5MnBpZdeGrRo0SJISUkJEhISgiZNmgTXXnttsG7duoJ2b775ZnDssccGVapUCXJycoL77rsv+Oc//+lsKZSdnR307dvXOU63bt2Cbt26FYp99913Qbdu3YIqVaoEWVlZwd/+9rfg2WefdV5z2rRpQZcuXYKkpKQgMzOzYCslO2g7xoq4TRGKx/z584MrrrgiyMnJCRISEoJq1aoFJ598cvDYY48VbIX1008/BXfeeWfQqFGjoHLlykGDBg2CW265pdBWWUEQfbxu3749+O1vfxtUr149MDPGLg6J+bpsiwsC/tMBAAAA8OEZZgAAACAEC2YAAAAgBAtmAAAAIAQLZgAAACAEC2YAAAAgBAtmAAAAIAQLZgAAACBE5Ep/cXFxR/I8SszGjRud2IYNG2Tb/fv3O7GUlBQnNn/+fNm/Ro0aTqxy5cqy7fbt251YzZo1ndjMmTNl/wsvvFDGS6OS3Aq8vI5rlDzG9dFx6qmnynjPnj2dWNWqVZ1YlSpVZH9V9nr58uWy7bPPPuvE1PdFecC4RnkUZVzzCzMAAAAQggUzAAAAEIIFMwAAABAiLoj4QFJpfXZInZfvLTVv3tyJzZ0714mtXLlS9o+Pj3diiYmJTsz37NqaNWsi9ffFt23b5sT27t0r+3fo0EHGSyOeiUN5xLh2xTJfK6tWrXJial420/PwMce4vxElJyfL/iq/xXes+vXrO7FTTjnFiU2bNk32L0sY1yiPeIYZAAAAKCIWzAAAAEAIFswAAABACBbMAAAAQAgWzAAAAECIyJX+SqtYMnb/+c9/OrHVq1c7sRUrVsj+KkNXVfpLSEiQ/Xfu3OnEfFnXavcL9V59xwKA4qYqk/7000+R+6v5as+ePbLt0KFDnZjaPUjtPmSmd79Qx1q2bJnsr+Z2X1XAJUuWOLGPP/7YifkquypqRw+z8ltBECjt+IUZAAAACMGCGQAAAAjBghkAAAAIwYIZAAAACFHmk/5icdJJJzmxhQsXOrGaNWtGfk1fYoaiEl58SSA///xzpJgqyQoAR4JK8Iul3LUvwU/Jzs52Ylu2bHFi1atXl/2rVavmxNLS0pyY71x37drlxNQc7It///33sm1UJPcBpQu/MAMAAAAhWDADAAAAIVgwAwAAACFYMAMAAAAhWDADAAAAIcrlLhkdOnSQ8Y0bNzoxld2ssr7NdBlrlaG9b98+2d8Xj9q2UiX34/JliKuysDt27Ih8fACIwrfLhKLmpccee0y2Pfvss53YihUrnFhmZqbsn5SU5MReeuklJ6Z23jAzO//8852YbwelxYsXOzFVxvuTTz6R/W+99VYnNm3aNNlWiWWnEgCHh1+YAQAAgBAsmAEAAIAQLJgBAACAECyYAQAAgBDlMunvhBNOkHFVhlqVaq1Ro4bsr0pbq+Q8X7ns1NRUGVfUufrKsioq4YSkPwBFoRKf1RzoS45TiWzp6emy7Zo1a5yYmsPy8vJkf/W6c+fOdWLfffed7D948GAnlp+fL9vu3r3biak5PCsrS/Z/8803ndiwYcMit1XH2rt3r+wP4PDwCzMAAAAQggUzAAAAEIIFMwAAABCCBTMAAAAQolwm/Z1zzjkyHrWq39atW2V/VTmqatWqkc9LVerbv3+/bKuqNPmSCRXfewCAwxW1Wulll10m41WqVHFi69ati3x8lcysEu7MdILfGWec4cS6d+8u+6s5eOnSpbKtSrpTCZK+RLxNmzY5sSuuuEK2VUl/JPgBRx6/MAMAAAAhWDADAAAAIVgwAwAAACFYMAMAAAAhWDADAAAAIcrlLhkNGjSQ8Z9++smJxbLzhMqEVmW0VSa3mdnGjRudmK/ctdpRQ+3o4cswj6WMNo6OWMaaytBXsaPp+OOPl3G1U8xnn30W+XXVuPZR10DdK2bR74Fq1arJ+LZt2yKfFwpTZaXNzHbt2uXEfDtvqM9PxRISEmR/Nd8nJyc7saZNm8r+am71jVX1PaDelyrt7WubkZEh20blm298OzMBCMcvzAAAAEAIFswAAABACBbMAAAAQAgWzAAAAECIcpn0l5OTI+NbtmxxYiphSSWLmOmyrqok6ahRo2T/m2++2YmtWLFCtlXJJepcv/rqK9kfpc/RTLZR48eXNKgSoS699FIn5ktCWr58uRNr27atbPvss886sVjK+qoEP19yX1ZWlhN79NFHndjmzZtl/wULFjix1157TbZduHChjFcEsYw1VS7al/TnS5A7mC/Jevv27ZHaLlu2TPZX7yE9PV22VeeqkkZ9CYpKWlqajKtS3h9//HHk1wWKmy8ZtqiJ6h9++KETGzNmjGz7wgsvFOlYUfALMwAAABCCBTMAAAAQggUzAAAAEIIFMwAAABCCBTMAAAAQoszvkhF1Nwkzs7y8vEiv6cvsrFOnjhO7+uqrndjTTz8t+6tdMmIp66syzOfMmSP7o2RF3TngSGUXx9J/586dTkztCKNKw5uZbdq0yYnVqlVLtn3kkUec2F133eXEVq1aJfur+6JFixaRj1W3bl0nNn78eNm/Zs2aTuzkk0+WbSvyLhnNmzd3YklJSbKtGpe+nSPUa6g50Lf7jNr9RR1LjXUzsz179jgx344uW7dudWLqvfrKsKsdPXz32ymnnOLE2CUDR0ssOxUpp512moxPnDjRiW3YsMGJqR2czMwmTJjgxNR9ZabnkSj4hRkAAAAIwYIZAAAACMGCGQAAAAjBghkAAAAIUeaT/jp06BC5rSp5rRJGGjVqJPurxIonn3wy8vFjETVB7Pvvvz8ix0fRRE26K2pyX3Ho2bOnE+vfv78TU0l0ZmYXXHCBE/v0009lW5UwcvfddzsxXxLTzJkzndh1110n26qS3epYzZo1k/1VaW1fgmFFpsqg+8pVq0Q6X+KzouZw3z2k5stdu3Y5MV9ikOJLFjrmGPe3JxVT52+mz9WXzNitWzcnphJnff2BolAJfr7E3f/5n/9xYpdffrlsO23aNCe2ZcsWJ9a7d2/Z/7777nNi11xzjWyr7s0o+IUZAAAACMGCGQAAAAjBghkAAAAIwYIZAAAACFHmk/46duwYua16WH3fvn1OzPcAe58+fSIdx1dpUPE9fK6SQHbv3u3EPv/888jHQjS+6nuxtFWJTKpKmKqSZmZWvXp1J+ZLRlVJTy+//LJsq6jEDHX+Q4YMkf1VsoVKmDPTSVPr1693YieccILs37lzZyc2adIk2VZVcDv33HOdmO9+VdfA1zaWMVPedOrUyYn5Es7UfOebb1WlOxXzJdKpBD/1mfqOr+4r9X1hFr0ypm++jzpfmPmTVFG8oiZy+vjugZJOxoxahdZHJXmPGTNGtp09e7YTW7ZsmWyrKnuq78Fnn31W9r/ppptkXImlMuGv8QszAAAAEIIFMwAAABCCBTMAAAAQggUzAAAAEIIFMwAAABCizO+SkZOT48R8WagqwzklJcWJ/ec//5H9fVnLB9u5c2ekdmb+7HoVr127thObO3du5GPBpa6z7zNRmcS+DHu1o4kaqyeeeKLsv23btkivaWbWunVrJ9amTRsn1rhxY9lfva97773XiV199dWy/y233OLEfDt6tGzZ0omprOdFixbJ/hkZGU7s9NNPl21V1rXa5SI/P1/2V7sv+HbJqFatmoxXBPXq1XNivix0NTfXqlVLtlVltNVY9R1L7ciidjnw7Xyh+HZJUOel7tc6derI/ps3b3Zivu8xtatMRRbLDjW+3SDUWFHj4mjucBHLWFO7rPh2j4llR4w333zTibVt29aJ+dYhO3bscGK+70xV8n3kyJFOLJbdMIobvzADAAAAIVgwAwAAACFYMAMAAAAhWDADAAAAIcp80p8qy7px40bZVj2wr0qaPvPMM0U/MUElscSSsLB9+/biPB14xJIU4UvEU1Sywpw5c2TbL7/80ompxBQznQh3/vnnOzFfEskDDzzgxNLT053Y4sWLZf++ffs6sbffflu2veGGG5yYSkZUJbB95+BLZlTvVyVTJiYmyv4qkU+VW/Ydq6LIzMx0Yr4EaXWd1qxZI9uuW7fOialkUvWZmulEKJUgqEpYm+l5wDdfq+TxvLw8J7Zq1SrZX91vvvelxmXdunWdmLp+5VEs87VP1MRPNdeZmQ0YMMCJqTFhZvbQQw85sf/+979OLJYEQ1+Cn/KHP/zBiankOjNd2lqN67S0NNlfra981+U3v/mNE5s4caJsW1SHO2Yq7iwPAAAARMCCGQAAAAjBghkAAAAIwYIZAAAACFHmk/4aNmzoxFR1GTOd3KMSpo7Ug+ZbtmyJ3FYlrKxdu7Y4Twemk3h8yRYq2caXmHPeeec5saysLCfmGxN///vfnViNGjVk248//tiJqcSS/v37y/7qPagkpD/+8Y+y/1/+8hcn1r17d9lWJdesXr3aifk+A1XVUN0rvtdo0qSJE/MlnY0ZM8aJvfHGG5GPVVGoOdiXeN20aVMn5ptvVQXG4447zomtWLFC9lf3tko6jCXx2pccppKbVEW+r776Sva//fbbndh3330n26pKaaraYnlM+osluVa19VWFbNCggRN74oknnJhK3DfTc5gvIfx///d/ndi8efOcmBoTZmbVq1d3YgMHDnRi1113neyvvnMuueQS2VYlCGZnZzsx3/3eokULJ+arbjtjxgwZL034hRkAAAAIwYIZAAAACMGCGQAAAAjBghkAAAAIwYIZAAAACFHmd8lQ5aJr1qwp26pdMlR2686dO4t+YoLKmlUZz2Y6w/fHH38s9nOq6GLZ3cC3I4aidnNQWe++0thqDPp21Pjkk08itfXtBtC2bVsnpkq13nrrrbJ/ly5dnJjvukbN3Fe7DpjpMsa+zHV1v99zzz1OzLfzheK7hhW5NLbavUXtEGGmy+Ju2rRJtlX3myoP77v2vt1TolLlc33l6aP2//TTT2VbNa58Ozqoc1DzzcyZMw9xhmWPuqa+MsexzO1qp58pU6Y4sUcffVT2P+WUU5yYKpdtZpaTk+PE1O49w4cPl/3VvbVkyRIn9swzz8j+K1eudGK+OXTq1KlOTO3IctJJJ8n+CxcudGKLFi2Sbdu0aePEqlat6sROO+002b9+/fpOTO1+YmY2bNgwGT+UijvLAwAAABGwYAYAAABCsGAGAAAAQrBgBgAAAEKU+aQ/9QC570HvXbt2OTFfYsWRoMpH+s5VJYEsX7682M+polOJBqp8r5nZhx9+6MS2bt0q286aNcuJqcSO+fPny/7jx4+XcSUtLc2JdejQwYmp8qtmOjklOTnZifmSDt98800nphLuzHQZZVVqVd2rZjrJ15f08+WXXzqxWBL8VDKZL5HIdw4VQVJSUuS26ppu2LBBts3IyHBiqjS1LxEzatl7XyJfLJ//Tz/95MRUcpRq5xNLyff27ds7sXHjxkU+Vlmh7rP09HTZVs1hS5culW0/++wzJ3b55Zc7sd69e8v+HTt2dGJr166VbV977TUnphL5VIKzmU6crVatmhNT321mZmeccUbkY3377beRYr5EPkUllJvp70z1ndW5c2fZX52DSoY0M2vevHnYKXrxCzMAAAAQggUzAAAAEIIFMwAAABCCBTMAAAAQggUzAAAAEKLM75Lx+eefO7GuXbvKtirD1pdhfSSobF7fLh2qNLDKjvVR76siZ/L79OzZ04mpjGczs0GDBjkxX4a/+vzULhN//vOfZf8777zTibVs2VK2Vdn0qtRpVlaW7L948WInFrUkqplZ//79nZjKUDfT56p2GlG7YZiZ7dixQ8aVunXrOjGVJT937lzZf9WqVU6sRYsWsu1f//rXyOdVlmVmZjoxNdZ98+ru3budmG+sqB1VNm/e7MR8u1wcifnOV4ZblfdWu3z4svPVDkqxvK9GjRrJtuVNbm6uExsxYoRsO2PGDCdWs2ZN2VZ9Lvn5+U7Mt8vJ5MmTnZhvhwa104aar9X8ZabvLbVLhm/nCvW+fLu/qF1p1Gfg2xVJ3RfqWpnptZDagec///mP7K/OtVmzZrKtb7eUQ+EXZgAAACAEC2YAAAAgBAtmAAAAIAQLZgAAACBEmU/6U+WiYymV6kusOBJUud/U1NTI/ffu3VucpwMze/TRRyO3Pf30052YKklrZnbeeec5MZXw5Ev6vPvuu52YL4mkdu3aTkwl+PlKWzdu3NiJXX/99U5MJaaYmVWtWtWJJSQkyLbfffedE1OJXL4kJl8yoKJeV5UL/uqrr2T/vLw8J+ZLGoqlNGxZVr9+fSemkm18yXGqBPDvfvc72VaNV5UgeqSS/tT7UgmOZjppSiWo+hLUVDKZ7/zVnOFL6C1v0tLSnJgak2b6mvo2BFD3r5qrfPO9+h5XpbXNzH788Ucn9vTTT0c+lkpGVslxOTk5sr9KRlWJgGb6Gqr70vfdoPjWZ1HL1qvvUTOz4447zonNmTNHtl29enXYKXrxCzMAAAAQggUzAAAAEIIFMwAAABCCBTMAAAAQoswn/X322WdOzPcAukqiUEkoR0osCX4qaUZVzUHRtGvXzompqklmZp988okTe//992Xb+++/P9LxfclR1atXd2K+qkWqAp+qvKTeayzn5buvVBKK71iqotn3338fqZ2Z2ezZs53YsmXLZNui3ttUy3SpaqXqmvgSc1QinS9pSyVtqsQgX5UxdV6+imaKugd8CUuVK1d2YipBVSWtmekKmL5jqWuoqiKWR998840Tu/LKK2VblaTdoUMH2faUU05xYirhzZf0O3/+fCf2xhtvyLYqGa9Tp05OzPfdcM4550Rq65uvVdKeL0lbJZjWqlXLiakxaabvN9/7Uuer7u2mTZvK/ioh+K677pJtDxe/MAMAAAAhWDADAAAAIVgwAwAAACFYMAMAAAAhWDADAAAAIcr8Lhlr1qxxYr6sa5VNn5ycXOzn5KPKqvqyyVV2qS8TFYdP7dDQo0cP2VaVtfXtXKLKmH/99ddObN68ebK/et0vvvhCto1q/PjxRepf0ahdFnw7F1QUqgx7LLuJqGx8X8l2NQeq1/XNiyob31dGW4ml5LfK8FfvVe06YKbfayw7F9StW1e2rQh8ZZ1ffvnlSDGfOnXqOLHMzEzZtlGjRk6sTZs2sq3avUftiqR2TjEze/PNN51Y1F1azPQ9VLVqVdlWUeeqvu/M9I4avh2Q1LVV99vrr78u+z/11FMyrhzuPM4vzAAAAEAIFswAAABACBbMAAAAQAgWzAAAAECIMp/0pyxcuFDG1UP06mF3XwLFunXrinResZTVLWpZV0SjrumHH34o26q4L4lIlYZu2bKlE7vqqqtkf5UItW3bNtlWJY6qseobfxs2bHBiWVlZkY5jZpaUlOTEfEkVKpFJtfWVEFaJs76kMXVe6j34El5U/xUrVsi2sSQTlWXqWqkSwr77Qo3BWBIEFV8ini8eVSylsdX7Vf19SX8q7iv5rRKs1PFTU1Nlf1VCGK68vLxIMTOzmTNnOrGJEycW9ymhmMSyFvs1fmEGAAAAQrBgBgAAAEKwYAYAAABCsGAGAAAAQrBgBgAAAEKUy10yfJnIKutaxXwZ+kXdJUNlMvuyNVU2ttohACXLV5b3m2++iRQjkxplSUpKihOLZfeeWOYwtauRmtt9O1dELaPt240iFuocYinDXbNmTSfm280i6i4X7du3l/FPP/008nkB+D/8wgwAAACEYMEMAAAAhGDBDAAAAIRgwQwAAACEKJdJf/Xr15fxzZs3OzGVrFG5cuXiPiUz00ksvoSZWMqqAsDRoOYwNVdVq1ZN9lfznS8RUB1L8SXXFTURT/ElaavXVWXYs7OzZf///ve/Tiw3N1e2VYnqKiG9Tp06sj+Aw8MvzAAAAEAIFswAAABACBbMAAAAQAgWzAAAAECIcpn0p5L7zHRyytGsqLdgwQInpio8menz2rt3b7GfEwBEVaNGDSe2atUqJ+arlvrOO+84MZUcZ2Y2YsQIJzZz5kwn5ksOjJq87Uvki6WCoaogqBIBU1NTZf9evXo5senTp8u2GRkZTkx9t9WqVUv2B3B4+IUZAAAACMGCGQAAAAjBghkAAAAIwYIZAAAACMGCGQAAAAhRLnfJyM/Pl3GV4a3KTderV6/Yz8lM73wRC5UJHcuxfNngABBF06ZNnZial5KSkmR/tSPGtddeK9uqXTIaNGjgxHbt2iX7q12F1Hzvm1fVLhe+0tpVq1Z1YtWrV3dizz//vOyvzuv777+XbXNycmQ8yjkBOHz8wgwAAACEYMEMAAAAhGDBDAAAAIRgwQwAAACEKJdJf77kOlWGOiEhwYm1bdtW9n/77beLdF4qYcRX1lXFY0n6A4DippLuVFnon376Sfb/5ptvIh9LJa2NHj3aiZ166qmyv0qOW7p0qROLZV5V79XMbO3atU7sxhtvdGLjx4+PfKzHHntMxs844wwnppIsW7VqFflYAA6NFRgAAAAQggUzAAAAEIIFMwAAABCCBTMAAAAQggUzAAAAEKJc7pLx0ksvyfhxxx3nxDZs2ODEpkyZUuznZGa2ZcsWJ+bL0N62bZsTmz17duRjUQYbQHHr2LGjE1O7EiUmJsr+qjS2jyp5fdlll0XuH1XlypVlvFq1ak5MzeFm/t0zimLmzJkyrsqTp6WlObE1a9YU9ykBFRq/MAMAAAAhWDADAAAAIVgwAwAAACFYMAMAAAAh4gKywwAAAAAvfmEGAAAAQrBgBgAAAEKwYAYAAABCsGAGAAAAQrBgBgAAAEKwYAYAAABCsGAGAAAAQrBgBgAAAEKwYI4oLi7O7rjjjoI/P//88xYXF2dLly4tsXMCSpOlS5daXFycPfjggyV9KqjAmKtREcTFxdmIESMO2Y7xX3zK7YL5wCA58L8qVapYs2bNbMSIEbZu3bqSPj3gsHz//fc2cOBAy87OtipVqlhWVpb17t3bHnvssZI+NeCwMFcDhZXkPH/PPffY66+/fsSPUxZVKukTONL++te/WqNGjWz37t322Wef2ZNPPmmTJk2y2bNnW9WqVUv69IDIpk+fbj169LCGDRvaFVdcYRkZGbZixQr74osv7JFHHrFrr722pE8ROGzM1UDxz/OXXHKJDRo0yBITEyO1v+eee2zgwIF27rnnHsbZl2/lfsF85plnWseOHc3M7PLLL7datWrZyJEj7Y033rDBgweX8NkdOTt27LDk5OSSPg0Uo7vvvtvS0tLsyy+/tOrVqxf6u7y8vJI5qaNs586dLJ7KKeZqoPjn+fj4eIuPjw9tEwSB7d6925KSkmJ+/Yqk3D6S4dOzZ08zM1uyZIl1797dunfv7rQZOnSo5eTkHNbrP/HEE9a6dWtLTEy0zMxMu+aaa2zz5s0Ffz9ixAhLSUmxnTt3On0HDx5sGRkZtm/fvoLY5MmTrWvXrpacnGzVqlWzvn372pw5c5zzTUlJsUWLFtlZZ51l1apVs4suuuiwzh+l16JFi6x169bOJGpmVqdOnYL//8Czba+//rq1adPGEhMTrXXr1vbuu+86/VatWmWXXnqp1a1bt6DdP//5z0Jt9u7da//7v/9rHTp0sLS0NEtOTrauXbva1KlTD3nOQRDYlVdeaQkJCTZhwoSC+IsvvmgdOnSwpKQkq1mzpg0aNMhWrFhRqG/37t2tTZs29vXXX9upp55qVatWtVtvvfWQx0T5wFyNiijqPH/AoeZ59QxzTk6O9evXz9577z3r2LGjJSUl2dNPP21xcXG2Y8cOGzNmTMEjUkOHDi3md1h2VbgF86JFi8zMrFatWsX+2nfccYddc801lpmZaQ899JANGDDAnn76aTv99NPtp59+MjOzCy+80Hbs2GHvvPNOob47d+60t956ywYOHFjwX4Njx461vn37WkpKit133332l7/8xX744Qc75ZRTnAf4f/75Z+vTp4/VqVPHHnzwQRswYECxvz+UrOzsbPv6669t9uzZh2z72Wef2dVXX22DBg2y+++/33bv3m0DBgywjRs3FrRZt26ddenSxT744AMbMWKEPfLII9akSRO77LLLbNSoUQXttm7dav/f//f/Wffu3e2+++6zO+64w9avX299+vSxmTNnes9h3759NnToUHvhhRds4sSJ9pvf/MbMfvkF5Xe/+501bdrURo4caTfccIN9+OGHduqppxZasJiZbdy40c4880xr3769jRo1ynr06BHTNUPZxVyNiqi453mfefPm2eDBg6137972yCOPWPv27W3s2LGWmJhoXbt2tbFjx9rYsWNt+PDhxfG2yoegnHruuecCMws++OCDYP369cGKFSuC8ePHB7Vq1QqSkpKClStXBt26dQu6devm9B0yZEiQnZ1dKGZmwe233+68/pIlS4IgCIK8vLwgISEhOP3004N9+/YVtBs9enRgZsE///nPIAiCYP/+/UFWVlYwYMCAQq//yiuvBGYWfPrpp0EQBMG2bduC6tWrB1dccUWhdmvXrg3S0tIKxYcMGRKYWXDzzTfHeplQhrz//vtBfHx8EB8fH5x44onBTTfdFLz33nvB3r17C7UzsyAhISFYuHBhQWzWrFmBmQWPPfZYQeyyyy4L6tWrF2zYsKFQ/0GDBgVpaWnBzp07gyAIgp9//jnYs2dPoTb5+flB3bp1g0svvbQgtmTJksDMggceeCD46aefggsvvDBISkoK3nvvvYI2S5cuDeLj44O777670Ot9//33QaVKlQrFu3XrFphZ8NRTT8V6qVCGMFcD/6e45/mDx38QBEF2dnZgZsG7777rHD85OTkYMmRIsb+v8qDc/8Lcq1cvS09PtwYNGtigQYMsJSXFJk6caFlZWcV6nA8++MD27t1rN9xwgx1zzP9d1iuuuMJSU1MLfqWIi4uz888/3yZNmmTbt28vaPfyyy9bVlaWnXLKKWZmNmXKFNu8ebMNHjzYNmzYUPC/+Ph469y5s/zn8KuuuqpY3xNKl969e9vnn39u/fv3t1mzZtn9999vffr0saysLHvzzTcLte3Vq5fl5uYW/PnYY4+11NRUW7x4sZn98qjEv//9bzv77LMtCIJCY6xPnz62ZcsW++abb8zsl2fgEhISzMxs//79tmnTJvv555+tY8eOBW1+be/evXb++efb22+/bZMmTbLTTz+94O8mTJhg+/fvtwsuuKDQMTMyMqxp06bOuE5MTLRhw4YVzwVEqcZcDRTvPB+mUaNG1qdPn2I///Ks3Cf9Pf7449asWTOrVKmS1a1b15o3b15okiwuy5YtMzOz5s2bF4onJCRY48aNC/7e7Jd/6hs1apS9+eab9tvf/ta2b99ukyZNsuHDh1tcXJyZmS1YsMDM/u85voOlpqYW+nOlSpWsfv36xfZ+UDp16tTJJkyYYHv37rVZs2bZxIkT7eGHH7aBAwfazJkzrVWrVmZm1rBhQ6dvjRo1LD8/38zM1q9fb5s3b7ZnnnnGnnnmGXmsXyeYjBkzxh566CGbO3duwT9Zm/0y6R7s73//u23fvt0mT57sPHe6YMECC4LAmjZtKo9ZuXLlQn/OysoqWKyjfGOuBn5RXPN8GDV3I1y5XzCfcMIJBZnXB4uLi7MgCJz4rxM5joQuXbpYTk6OvfLKK/bb3/7W3nrrLdu1a5ddeOGFBW32799vZr88G5eRkeG8RqVKhT+6xMTEI/LlgtIpISHBOnXqZJ06dbJmzZrZsGHD7NVXX7Xbb7/dzMybFX1gvB8YXxdffLENGTJEtj322GPN7JcEvaFDh9q5555rf/7zn61OnToWHx9vf//73wueM/21Pn362Lvvvmv333+/de/e3apUqVLwd/v377e4uDibPHmyPMeUlJRCfyZru+JgrgYKK+o8H4a5NXblfsEcpkaNGvKfLn79C0NU2dnZZvbLg/SNGzcuiO/du9eWLFlivXr1KtT+ggsusEceecS2bt1qL7/8suXk5FiXLl0K/v7AP7PUqVPH6Qv82oFFxpo1ayL3SU9Pt2rVqtm+ffsOOb5ee+01a9y4sU2YMKHgVzUzK5i0D9alSxf7/e9/b/369bPzzz/fJk6cWLBoyM3NtSAIrFGjRtasWbPI54uKjbkaFd3hzPOH49dzPAqr0P+Zm5uba3PnzrX169cXxGbNmmXTpk2L+bV69eplCQkJ9uijjxb6r7tnn33WtmzZYn379i3U/sILL7Q9e/bYmDFj7N1337ULLrig0N/36dPHUlNT7Z577in0T+AH/PqcUTFMnTpV/nIwadIkM3P/iTlMfHy8DRgwwP7973/LbOxfj68Dv2L8+tj//e9/7fPPP/e+fq9evWz8+PH27rvv2iWXXFLwK9xvfvMbi4+PtzvvvNN5L0EQRMruRsXDXI2Kojjn+cORnJzs7FaEX1ToX5gvvfRSGzlypPXp08cuu+wyy8vLs6eeespat25tW7dujem10tPT7ZZbbrE777zTzjjjDOvfv7/NmzfPnnjiCevUqZNdfPHFhdoff/zx1qRJE7vttttsz549hf6Jz+yX596efPJJu+SSS+z444+3QYMGWXp6ui1fvtzeeecdO/nkk2306NFFvgYoO6699lrbuXOnnXfeedaiRQvbu3evTZ8+veBXr1iT4+69916bOnWqde7c2a644gpr1aqVbdq0yb755hv74IMPbNOmTWZm1q9fP5swYYKdd9551rdvX1uyZIk99dRT1qpVq0LJUAc799xz7bnnnrPf/e53lpqaak8//bTl5ubaXXfdZbfccostXbrUzj33XKtWrZotWbLEJk6caFdeeaX96U9/KtJ1QvnDXI2Korjn+Vh16NDBPvjgAxs5cqRlZmZao0aNrHPnzkf0mGVGCezMcVQc2Erlyy+/DG334osvBo0bNw4SEhKC9u3bB++9995hbVV0wOjRo4MWLVoElStXDurWrRtcddVVQX5+vjz2bbfdFphZ0KRJE+/5TZ06NejTp0+QlpYWVKlSJcjNzQ2GDh0afPXVVwVthgwZEiQnJ4e+T5R9kydPDi699NKgRYsWQUpKSpCQkBA0adIkuPbaa4N169YVtDOz4JprrnH6Z2dnO9sFrVu3LrjmmmuCBg0aBJUrVw4yMjKC0047LXjmmWcK2uzfvz+45557guzs7CAxMTE47rjjgrffftu5T369rdyvPfHEE4GZBX/6058KYv/+97+DU045JUhOTg6Sk5ODFi1aBNdcc00wb968gjbdunULWrdufbiXC2UEczXwf4p7nvdtK9e3b195/Llz5wannnpqkJSUFJgZW8z9SlwQRHg6HAAAAKigKvQzzAAAAMChsGAGAAAAQrBgBgAAAEKwYAYAAABCsGAGAAAAQrBgBgAAAEKwYAYAAABCRK70V5bqi6enp8v4Oeec48S2bNnixFasWBH5WCtXrnRilSrpy5qQkODEUlJSZNtu3bo5sU8++cSJffPNN4c6xVKvJLcCL0vjGmUL47r4NWzY0ImtWrVKtt23b1+xH3/AgAEy/u9//7vYj1XUz/BIjT/Gdck6/fTTnViDBg2cmCrTbmbWtm1bJ/aPf/xDtp0/f74TU59BeSjnEeU98AszAAAAEIIFMwAAABCCBTMAAAAQIi6I+PBJST871KZNGxnv27evE/M9Q6yeF1ax+Ph42T8/P9+J7dmzx4nt3LlT9k9LS3NivnNVtm/f7sQqV64s286bN8+J/etf/4p8rKOJZ+JQHpXHcV3U5xfbt2/vxHbt2iXbZmZmOrGXX37ZiflyVh544AEntn79eieWm5sr+1900UVO7Jhj9G9MEyZMcGIvvfSSExs+fLjsf+6558q4or6f1Gewf//+yK8Zi/I4rksjlcdkZjZ69Ggntnr1aifmWxv06NHDic2cOVO2Pe6440LO8NDU/XKkxmVR8QwzAAAAUEQsmAEAAIAQLJgBAACAECyYAQAAgBAsmAEAAIAQZWaXjFtuuUXGVYb1smXLZFuVdZ2dne3E1G4YZjoTtVmzZpHameldLho1aiTbqt035syZ48SSk5Nl/7p16zoxtXOGmdnkyZOd2NHMbiXrGuVReRzXRZ0XNmzY4MQWLFgQ+Viqv2+XCxWP5fzV98h3330n29asWdOJVa1aNdLxzfTcrHbp8Dma1dfK47gujR577DEZ7969uxNTu1yoe8XMbNCgQU7so48+km3ff/99JzZmzBjZtqxjlwwAAACgiFgwAwAAACFYMAMAAAAhWDADAAAAIUpl0l/9+vWd2A033CDbrly50on99NNPsq1KulPHqlGjhuw/d+7cSK/pk5GR4cQaNmwo286aNcuJxVKGu3Hjxk4sNTVVtv3rX/8q40cLSSQojyryuPYlLKlyv2oONzOrVKlSpGOpctdmOpmvSpUqTsw3hyq+MtyqDLFKukpISJD9W7du7cSee+452fa+++5zYupa/fzzz7J/UVXkcR2LYcOGyfjxxx/vxFSSfkpKiuy/b98+J1a7du3I/ZW1a9fKeGJiohPbvHmzE9u0aZPsf9NNNzmxvLw82baky2iT9AcAAAAUEQtmAAAAIAQLZgAAACAEC2YAAAAgBAtmAAAAIESp3CWjXbt2TuzGG2+UbRcuXOjEfKWpt2zZ4sTi4+OdWL169WT/tLQ0J7ZkyRIn5stOVWW4f/zxR9k26u4b6vzNdMluH3bJAIpfRR7XX331lYyr+Wrbtm2yrdq9Ipb3pXaJ2LFjhxOrVq2a7K/O1Ze1r143KSnJiandNMzMkpOTnZjve6hRo0YyfjDftSrquKzI49rn5JNPdmJ33HGHbLt3714npnbAUqXVzfTOFWqXDLUjjJnZ/PnznZhv9xe1DlGfgW/NM2PGDCd2zTXXyLYljV0yAAAAgCJiwQwAAACEYMEMAAAAhGDBDAAAAISIVnv0KFNJGL4kOJUA4Su9qB6WV2Umly5dKvur8pUtWrRwYr5z/e677yId30yfq0osadKkieyvHuJftmyZbAsAh0vNQTVr1pRtVeK1bw5UyUUqMceXiKf6q3nxp59+kv1V0qAvaU8lXa1bt86JNW3aVPZX5+BL+opaQvhIJf3BNXDgQCe2atUq2VYl46nPyleyfePGjU5MJc76+qt7U5VxN4s+1nzl6XNycpzYKaecItt+9tlnTizqHHC08AszAAAAEIIFMwAAABCCBTMAAAAQggUzAAAAEKJUJv2pCkdr166VbVWFnZNOOkm2/de//uXE1AP46kF3M/2w/ebNm2VbRSV2+BJWKlVyPxr1EL+v6pM6VwAobqqCqS/hTCV079q1S7ZVydMq5qsypqrn7d6924n55nuV4Ld161bZVlWBjSUhXCU+rly5UrZVc/6iRYucGMl9xc+XZK82BFAJrj5qHeK7L2rUqOHE1qxZ48RURUEzs8zMTCfmSxBUGw2otYnvHlKJsxdddJFsq5L+StsY5hdmAAAAIAQLZgAAACAEC2YAAAAgBAtmAAAAIAQLZgAAACBEqdwlQ2WB+rI4586d68S6d+8u2z7zzDNOLD4+3on5SrWqrGnVX5W1NjNLSkpyYr5M2CVLljgxlWHuy9r98ccfnZjK5DaLXv4SAA52/PHHR26rdgqqU6eObKt2lFAZ+r45VM3Nag5Wmfy+uG9XItVWfY+o45vp3TcSEhJk29zcXCemdsmgNHbx6927t4yrXS586wi1s1Ys6wi1FqpevboT27Nnj+yfn5/vxHzl4dU6QI1L344c6hqkpqbKtmUBvzADAAAAIVgwAwAAACFYMAMAAAAhWDADAAAAIUpl0l/VqlWdmO8B+Ly8PCd27LHHyrbnnnuuE1MlpH3lS9XD8ioR0Ee19SXtZWRkODH1sL0qS2umE2F8yTUk/aEoVAninJwc2bZt27ZObPz48ZGPVdSxqhKhSIIqmlatWjkx3zVVn59KeDIzq127thNT830syczqe0TN6762vu+GWrVqObH169c7MV/S4IYNG5yY77qoktvvv/++E2NcF79u3brJuPq+9SW3qXLTKhFQJfmb6VLwKunUV65aJfj5El/V3Kreqy/BUCW5qk0dzPS4Vps6lCR+YQYAAABCsGAGAAAAQrBgBgAAAEKwYAYAAABCsGAGAAAAQpTKXTIUXya8is+ePVu2VdmZKuvZdyyVCap2vvBlp6qsZ18mrXpdlR2rMrHN9HvwZV2rDO9169bJtqi4br75Zhm/6KKLnNiKFStk29atWzsxNdamTp0q+8eyI0YsZe8VtctAr169ZNsPP/ww8uuWN5mZmU7Mt0ODypr3tVXlftWOGKtXr5b91a5CaocAX1lfVe5Y7V5kpne5UO9V7fxhZrZ8+fLI59W+fXsZPxi7ZBQ/3/e1+h5Wu32Z+ctQH0ztpmGmx3XUnTfMzJo2berEtm3bJtv6dic7mG/No+ZrX9uOHTs6MXbJAAAAAMoQFswAAABACBbMAAAAQAgWzAAAAECIUpn0p5LQ1q5dK9uqB+CnTJki26qEDV/SnKIe1o+aCGhmVqmSe7kXLVok26rX2LlzpxP77LPPZH+V4OhLeIqlvDdKjirrbFb05J709HQnNm3aNCemkkXMzP7nf/7HiTVs2FC2VUl/H3zwgRN78803Zf8//OEPTmzp0qWybdQEP998oZJmOnXqJNtW5KS/3NxcJ+YrtavGmq+srkocVUlzjRs3lv1VGW01B2dnZ8v+qjSxek0zfQ+qpFOVSGgWPUHRTJcQRvFTCW++OUUlsvmS29T3bSxzuDqH5ORkJ+b7vlD91X3hE0vitTov3zVUSX8vvvhi5GMdDfzCDAAAAIRgwQwAAACEYMEMAAAAhGDBDAAAAIQolUl/6kFxX2JI586dndi9994r21511VVOTD2A7quIpyr3qES8WB72V5UGzczq1KkT6bxWrVol+6uElY0bN0Y+1sqVK2VbRKMSLlRiRyyJfLEkhqiKVHfeeadsO2zYMCf20EMPObErr7xS9v/9738f+bzU/aLGmq+i3pIlS5zYRx99JNuOHz/eiQ0ePNiJ1atXT/ZX59WzZ0/Z1jfnVAQqwdhXKVRVJNu0aZNsq+ZLlaStjm+m52tf9TxFJfj5Ep7UnK+O5buHVaK7b75WSZYofuo6+z4/NS58Y0V9j6v7wrcOUecQtSKfmU7I9a1ZVFuVIOj7Hou6eYGZP3m3NOEXZgAAACAEC2YAAAAgBAtmAAAAIAQLZgAAACAEC2YAAAAgRKncJUOVD/WVCVW7XKiStmY661TFfKWifdmdB/Pt6KGytn2ZsCprddu2bU5s9uzZsv8ll1zixH788UfZtn79+k7sm2++kW0rMvWZ+DKho+5oEcvOFzk5OTL+3HPPObHu3bs7MbVLjJlZ27ZtndiaNWucmNpNw0zvMrFs2TLZVu3eoq6rb/cXdQ/5dq5QcbUrje9Yareedu3aybYVxfHHH+/E1Bj2zaELFy50Yr7rr8qQ79q1y4n5dtlQn7U6V19ZYBX33a9Ry7D7zlWNa19b9Z2hynv77kFEU7NmTSfmK/keyzy+e/duJ6Z2mfDtXKHKqOfn50eKmZk1a9bMialdOsz0WFPrIN9ONeoe8h2rQYMGMl6a8AszAAAAEIIFMwAAABCCBTMAAAAQggUzAAAAEKLEk/5UYo1K8PMl3KlSqc2bN5dtU1JSIh1LPVQfi1jKr/qohBVV7tj3YP+cOXOcmK/UqkoYqSiilrA28yf4HQnXX3+9E/Ml7an3sGDBAif28ccfy/533HGHE7v00kud2IwZM2T/zMxMJ1a3bl3ZVo1XVdbVV+pVJcx8++23sq1KLlFJh0lJSbK/uo8bNWok2/rmnPJGJeaohLfatWvL/u+8844T8yUBnXrqqU5M3YO+sry+hOqo1OfvO5b6zlDfLb75Ws3BvmRIlZAbS+ItolFzRSzlrmP5vlDf7WoNYKbHpUquU+dvpjcl8N0rqq263333hboGvu9XNd7Vddm6davsfzTwCzMAAAAQggUzAAAAEIIFMwAAABCCBTMAAAAQosST/lRVP/Wwuy8JSFWk8yUcqcRBlZjhq+ajRK0e6OOrMqUebFcJN77Egry8PCemEqbM/Ne2IlAJCE2aNJFtzz//fCemqi+amR133HGRju+rkNS0aVMn9pe//EW2Pffcc53Y4MGDnZiv0qO630aNGuXE/vCHP8j+Y8eOdWIXX3yxbLt27Vonpj6DWCoo+qrKqYRixZekG0v1rqImmJUVqvqZ+qx885qar1X1PjNd6cxX8VVR87j6vvF9zrGMNXUNVPU+XyKeaqtiZjrBqmPHjk7siy++kP0RjRorviq+sSTCqSRj1dY3B6p7QCX4qfM30+PaNweqc1CxLVu2yP6+c1DUHFqnTh0nRtIfAAAAUEqxYAYAAABCsGAGAAAAQrBgBgAAAEKwYAYAAABClPguGbVq1YrUzpedumHDBifWrl072VbtBpCWlubEVMarmc4OVZncPiq7VZXrNjNbvXp1pPNSpSPN9Ln6MlbT09NlvKKaOHGijKtdSh5//HHZdvbs2U7spJNOcmKqrLSZ2cqVK51Yv379ZNv27ds7sXXr1jkxXwlitStIixYtnJgvw1/dA76yrtWrV3diKjvat8tCUXdOUDvF+HZJUPebb6cZdb3LIzVfquvv2zVE7SqzefNm2TZq2XrffK36+3YuiNrft3OBiqvvtmnTpsn+mzZtcmInnniibKvuLd/OPjh86rvZN9eoMai+w830d7a6L2Ipw63mdt+aSe1045sD1bHUOsK3A5PafUN9B5jpuT0jI8OJLVy4UPY/GviFGQAAAAjBghkAAAAIwYIZAAAACMGCGQAAAAhR4kl/qkykerDelwSkHipXr+l7XfWwvu9he1VCWvX3ldZWyRq+c1UP26v+vmNt3LjRidWvX1+29b3fikAl7eXm5sq2M2fOdGKDBg2SbVXCiBorvvLNvlKliio3rZJAfO9r0aJFTkwlLKmELzNd2thXvlSNV1/SlxLLWFXHUqWNfQlqKhEmltK45ZGvNPTBfMl1qoSvKgNvpj9rdXxfwpKar1Vb37mq7xzf56/OVX3f+K5fXl6eE6tdu7Zsq8oQ+5K/cfhUcpvv+1Z9j6t52Uwneqvva9+GAiq+c+dOJ6YSSc30uIol6U+NtaVLl0bu36VLF9lW3UNqvihJ/MIMAAAAhGDBDAAAAIRgwQwAAACEYMEMAAAAhGDBDAAAAIQo8V0yopa19WWy5+fnOzFfJrLaZUJl+Pt25IiaIe7LmFc7H6jjm+nykevXr498Tr4yyIrKsFWZuOVxN40JEyY4sf79+8u2UUtQm+lsejXWfVnXCQkJTsyXyayyntVYmzt3ruyvxvuaNWuc2Lx582R/NS5856pEva/M9I4GsZSnj6U0srqPq1atKtu2a9cu8uuWZVE/a99nsnz5cifWsWNH2VbNjWpc+46lvjNiKZet4r6xqsaKmoN9JazVvRnLfBvLPYRo1Lzo+25Xn5XazcRMl8FWY0XttGSmv0fUd4BvByb13RTLLmR16tSJdE5meqcQtXuNmb4GvjLaJYVfmAEAAIAQLJgBAACAECyYAQAAgBAsmAEAAIAQJZ70p8pPqgfIfUl/KmnKV25alZ+MWprbTCcNqiQgX2JQLOVzVWlilfRXs2ZN2d+XjKb4yr1WBB9++KETa9CggWx70003ObGLL75Ytm3btm3RTkyIJQlIjTVfcpRK7FBJICQW+Z1xxhklfQpHhRqDalz5kj7VfNuoUSPZVs23sYxr9Z0RS2lsxZccpa6Lmld9pX7VfOErbazuw/KYkF3SVOK1L0lbfa7ffvtt5LaxlDZXn7Waw33rDTV+Ylmb+JL2lAULFjixunXrRm6bnp4e+VhHA78wAwAAACFYMAMAAAAhWDADAAAAIVgwAwAAACFKPOlPJbepZAn1ULtZbNW4Vq9e7cTUA/RpaWmyf15enhNTCSe+5CjV1lfNR10D1d+XLPDuu+86MV81MnUN1MP2sSQSlkf3339/pJiPqpDUqlUr2VZVP6tXr55sG7Uaku8eUolMURNLzHRFNl8iqYrv3r3bifkSsdR5+RKe1P2i3oMvuUpVqfIda+rUqU7s5ptvlm3LMvX+VXKdb6ycffbZTsyX2KPGRdSxGst5+caaShD0HUvN+Srmuy5ZWVkyrkQd1ygaldzm23xAfdZbt26N3DZq0qiZ3ihBbQigKviamR177LFObMOGDbKtou6XjIwM2VZV9ozlflOJlyWJX5gBAACAECyYAQAAgBAsmAEAAIAQLJgBAACAECyYAQAAgBAlvktG1DLYmzdvlv1VW1/W9cKFCyOdU35+voyrc1CZ3Dt27JD91bn6sm5Vhq6K+bK2VYatb/cO9RnEUv4S0ahdVlTMzOzjjz8+wmcDxE7NKyrr3Teue/fu7cTmz58v20YtTRxLaeuoZeDN9G4UvmOp66LmUF8J4o0bNzoxtauOmZ7za9asKdvi8KndKHzf14pvXKnXUOPK932t+teoUcOJ+caEKjnvKy+v4mp907x5c9l/+vTpTsy3+4faJcN3XiWldJ0NAAAAUMqwYAYAAABCsGAGAAAAQrBgBgAAAEKUeNKferBdJUuo5Dqf77//XsbVw+6qrHBmZqbs36BBAyemztX3oLoqIawS7sz8iYcHS0pKknFVclu9f19bX3lxABWXSsxRMZWcZ6YT+erXry/bqqQlNS+q45vppC11Xr752ve6ikrmS01NjXws9T3gSxBUSX+xJD4iGnWdfYl4iq+ss/q+VcmosSTeq/HjO1c1rnzJjCqukv5q1ap1qFM8JHVvkPQHAAAAlCEsmAEAAIAQLJgBAACAECyYAQAAgBAsmAEAAIAQJb5LhspaVjs/qB0mzHQW5aBBg2TblStXOrFVq1Y5MV+56Z07dzoxVS7bl9mpslZ9ZbybNGnixNSOHr7yqQ8//HDk81LZ3L5rAKDiUrsVqd0kfFn3qizutGnTZNvk5ORI/X07RPh2KTiYbwemWMpoRy1tvH79etn/5JNPdmINGzaUbdVuR2oHJxTNli1bnJja4cLMbNOmTU6sbdu2kY+l1kG+XVqi7iKmyq2bmTVr1syJqZ0vfNQuG75dtRo3buzE8vLyZFs1Z6idbkoSvzADAAAAIVgwAwAAACFYMAMAAAAhWDADAAAAIUo86U8lUagEP19y2xdffOHELrvsMtlWJb1lZGREPpZ64D+WMpPr1q1zYiqxxEwnEagkhHnz5sn+iq/U5tatW52YL7kBQMWlkttUwpJvrnn22Wed2L333lv0Eyvj1HfWfffdJ9uq7xGVEI6i2bBhgxNTSadmOkn+lFNOkW3V97ham/hKo6vNB6pVq+bEfIl4viRXJer6Rp2TmdlZZ53lxFQZbzOd5Fva8AszAAAAEIIFMwAAABCCBTMAAAAQggUzAAAAEKLEk/5UlTn1oLlq5/PVV18V6ZzKK1+1RFVtMDMz04l98803xX5OAMoOlVyUn5/vxHxJaGpe8VGJUL7qZ0XhqxQYy7HUa6jzVwmSZmY5OTmRjx+1qiCKRlXx9V1nVZ34mWeekW1/+9vfOrFatWo5MV9lXpVgmJaW5sR81ftU9TzfWFMJfuoa+BIJJ02a5MS6desm26qEyv/+97+ybUnhF2YAAAAgBAtmAAAAIAQLZgAAACAEC2YAAAAgBAtmAAAAIESJ75KhdmhQfNnFsVBluIvjdY8WlTWrMmZj6R/rawCouKKW8PXNKbGUvz1a81Jx7LxR1NdYv369E/OVRlalhVesWOHE1M4JZro0M1zLli1zYrF8zm+//XbkePv27Z3YscceK/vXqFHDidWrV8+JqfWOmdnevXudmK+MthqXH374oRP74osvZH+lS5cuMq5271DHL0n8wgwAAACEYMEMAAAAhGDBDAAAAIRgwQwAAACEKPGkP0U9/J2YmFjk1y1LCX5KUZNgfNdQJQeoUp8AKrbOnTs7MZUIqErqmvkTmcojX8ltRSVt+RKxVOKkKlfcq1cv2f/f//535POqyHJzc51Yw4YNZdvly5c7MZWcZ6ZLyc+cOTNSrDzwlRdX90DNmjWP9OnEhF+YAQAAgBAsmAEAAIAQLJgBAACAECyYAQAAgBAsmAEAAIAQJb5Lxrp165yYyqJUWaiIzfz582W8UaNGTmzz5s1H+GwAlDXTpk1zYmrXhq1bt8r+33zzTbGfU2kVyy4ZTz31lBPzlRFXuxotWrTIib3xxhuRjw/Xe++958SaN28u265du9aJqd0wfNROM0erNHwYNYZVLJZznTp1qowvWLDAif3nP/+J/LpHA78wAwAAACFYMAMAAAAhWDADAAAAIVgwAwAAACHigiAISvokAAAAgNKKX5gBAACAECyYAQAAgBAsmAEAAIAQLJgBAACAECyYAQAAgBAsmAEAAIAQLJgBAACAECyYI4qLi7M77rij4M/PP/+8xcXF2dKlS0vsnACfoUOHWkpKyiHbde/e3bp3715sx+3evbu1adOm2F4P+DXGNfCLuLg4GzFixCHbsVYpPuV2wXxgkBz4X5UqVaxZs2Y2YsQIW7duXUmfHuB44oknLC4uzjp37lzSp1Im3XPPPfb666+X9GngIIzromFcVzzff/+9DRw40LKzs61KlSqWlZVlvXv3tscee+yIH5vx5lduF8wH/PWvf7WxY8fa6NGj7aSTTrInn3zSTjzxRNu5c2dJnxpQyLhx4ywnJ8dmzJhhCxcuLOnTKXOY6EsnxnXRMK4rlunTp1vHjh1t1qxZdsUVV9jo0aPt8ssvt2OOOcYeeeSRmF/vkksusV27dll2dnak9ow3v0olfQJH2plnnmkdO3Y0M7PLL7/catWqZSNHjrQ33njDBg8eXMJnd+Ts2LHDkpOTS/o0ENGSJUts+vTpNmHCBBs+fLiNGzfObr/99pI+LaBIGNdAbO6++25LS0uzL7/80qpXr17o7/Ly8mJ+vfj4eIuPjw9tEwSB7d6925KSkmJ+/Yqk3P/CfLCePXua2S8Tue85t6FDh1pOTs5hvf4TTzxhrVu3tsTERMvMzLRrrrnGNm/eXPD3I0aMsJSUFPkL9+DBgy0jI8P27dtXEJs8ebJ17drVkpOTrVq1ata3b1+bM2eOc74pKSm2aNEiO+uss6xatWp20UUXHdb5o2SMGzfOatSoYX379rWBAwfauHHjnDZLly61uLg4e/DBB+2ZZ56x3NxcS0xMtE6dOtmXX355yGPMnDnT0tPTrXv37rZ9+3Zvuz179tjtt99uTZo0scTERGvQoIHddNNNtmfPnsjv5+uvv7aTTjrJkpKSrFGjRvbUU085bfLy8uyyyy6zunXrWpUqVaxdu3Y2ZswYp92OHTvsxhtvtAYNGlhiYqI1b97cHnzwQQuCoKBNXFyc7dixw8aMGVPwGNbQoUMjny+ODMY14xqxWbRokbVu3dpZLJuZ1alTx4m9/vrr1qZNG0tMTLTWrVvbu+++W+jv1TPMOTk51q9fP3vvvfesY8eOlpSUZE8//TTj7RAq3IJ50aJFZmZWq1atYn/tO+64w6655hrLzMy0hx56yAYMGGBPP/20nX766fbTTz+ZmdmFF15oO3bssHfeeadQ3507d9pbb71lAwcOLPivwbFjx1rfvn0tJSXF7rvvPvvLX/5iP/zwg51yyinOA/w///yz9enTx+rUqWMPPvigDRgwoNjfH46ccePG2W9+8xtLSEiwwYMH24IFC7yLhZdeeskeeOABGz58uN111122dOlS+81vflMwxpQvv/zSevbsaccdd5xNnjzZmzi1f/9+69+/vz344IN29tln22OPPWbnnnuuPfzww3bhhRdGei/5+fl21llnWYcOHez++++3+vXr21VXXWX//Oc/C9rs2rXLunfvbmPHjrWLLrrIHnjgAUtLS7OhQ4cW+mfHIAisf//+9vDDD9sZZ5xhI0eOtObNm9uf//xn++Mf/1jQbuzYsZaYmGhdu3a1sWPH2tixY2348OGRzhdHDuOacY3YZGdn29dff22zZ88+ZNvPPvvMrr76ahs0aJDdf//9tnv3bhswYIBt3LjxkH3nzZtngwcPtt69e9sjjzxi7du3Z7wdSlBOPffcc4GZBR988EGwfv36YMWKFcH48eODWrVqBUlJScHKlSuDbt26Bd26dXP6DhkyJMjOzi4UM7Pg9ttvd15/yZIlQRAEQV5eXpCQkBCcfvrpwb59+wrajR49OjCz4J///GcQBEGwf//+ICsrKxgwYECh13/llVcCMws+/fTTIAiCYNu2bUH16tWDK664olC7tWvXBmlpaYXiQ4YMCcwsuPnmm2O9TCgFvvrqq8DMgilTpgRB8MsYqV+/fnD99dcXardkyZLAzIJatWoFmzZtKoi/8cYbgZkFb731VkFsyJAhQXJychAEQfDZZ58FqampQd++fYPdu3cXes2D74GxY8cGxxxzTPCf//ynULunnnoqMLNg2rRpoe+lW7dugZkFDz30UEFsz549Qfv27YM6deoEe/fuDYIgCEaNGhWYWfDiiy8WtNu7d29w4oknBikpKcHWrVuDIAiC119/PTCz4K677ip0nIEDBwZxcXHBwoULC2LJycnBkCFDQs8PRw/j+heMa8Ti/fffD+Lj44P4+PjgxBNPDG666abgvffeKxhjB5hZkJCQUGiszJo1KzCz4LHHHiuIHbxWCYIgyM7ODswsePfdd53jM978yv0vzL169bL09HRr0KCBDRo0yFJSUmzixImWlZVVrMf54IMPbO/evXbDDTfYMcf832W94oorLDU1teAX5bi4ODv//PNt0qRJhf758OWXX7asrCw75ZRTzMxsypQptnnzZhs8eLBt2LCh4H/x8fHWuXNnmzp1qnMOV111VbG+Jxwd48aNs7p161qPHj3M7JcxcuGFF9r48eMLPZ5zwIUXXmg1atQo+HPXrl3NzGzx4sVO26lTp1qfPn3stNNOswkTJlhiYmLoubz66qvWsmVLa9GiRaFxd+BRJjXuDlapUqVCv0okJCTY8OHDLS8vz77++mszM5s0aZJlZGQUyiOoXLmyXXfddbZ9+3b75JNPCtrFx8fbddddV+gYN954owVBYJMnTz7k+aBkMK5/wbhGLHr37m2ff/659e/f32bNmmX333+/9enTx7KysuzNN98s1LZXr16Wm5tb8Odjjz3WUlNT5T1zsEaNGlmfPn2K/fzLs3K/YH788cdtypQpNnXqVPvhhx9s8eLFR2SQLFu2zMzMmjdvXiiekJBgjRs3Lvh7s1++GHbt2lUw+Ldv326TJk2y888/3+Li4szMbMGCBWb2yzPX6enphf73/vvvOw//V6pUyerXr1/s7wtH1r59+2z8+PHWo0cPW7JkiS1cuNAWLlxonTt3tnXr1tmHH37o9GnYsGGhPx9YZOTn5xeK79692/r27WvHHXecvfLKK5aQkHDI81mwYIHNmTPHGXPNmjUzs2hJJ5mZmU7C6YH+Bx4lWrZsmTVt2rTQf1yambVs2bLg7w/838zMTKtWrVpoO5QujGvGNQ5fp06dbMKECZafn28zZsywW265xbZt22YDBw60H374oaDdwfeM2S/3zcH3jNKoUaNiPeeKoNzvknHCCScU7JJxsLi4uEIJFgeoXz+KU5cuXSwnJ8deeeUV++1vf2tvvfWW7dq1q9CzdPv37zezX55hy8jIcF6jUqXCH11iYqIzSaP0++ijj2zNmjU2fvx4Gz9+vPP348aNs9NPP71QzJfxfPBYTkxMtLPOOsveeOMNe/fdd61fv36HPJ/9+/db27ZtbeTIkfLvGzRocMjXABjXQNElJCRYp06drFOnTtasWTMbNmyYvfrqqwU7zUS9ZxR2xIhduV8wh6lRo4b8p4vD+a/7A3sczps3zxo3blwQ37t3ry1ZssR69epVqP0FF1xgjzzyiG3dutVefvlly8nJsS5duhT8/YF/ZqlTp47TF+XHuHHjrE6dOvb44487fzdhwgSbOHGiPfXUU4c1ucXFxdm4cePsnHPOsfPPP98mT558yOpnubm5NmvWLDvttNMK/rUjVqtXr3a2NZw/f76ZWcHuM9nZ2fbdd9/Z/v37C/2H3ty5cwv+/sD//eCDD2zbtm2Ffo07uN2B94vSgXHNuEbxOvDD35o1a47ocRhvfhX6J8nc3FybO3eurV+/viA2a9YsmzZtWsyv1atXL0tISLBHH3200H/dPfvss7Zlyxbr27dvofYXXnih7dmzx8aMGWPvvvuuXXDBBYX+vk+fPpaammr33HOPzBL/9TmjbNq1a5dNmDDB+vXrZwMHDnT+N2LECNu2bZvz3FosEhISbMKECdapUyc7++yzbcaMGaHtL7jgAlu1apX94x//kOe7Y8eOQx7z559/tqeffrrgz3v37rWnn37a0tPTrUOHDmZmdtZZZ9natWvt5ZdfLtTvscces5SUFOvWrVtBu3379tno0aMLHePhhx+2uLg4O/PMMwtiycnJhbZwRMlgXDOucfimTp0qfyGeNGmSmbmPfRY3xptfhf6F+dJLL7WRI0danz597LLLLrO8vDx76qmnrHXr1rZ169aYXis9Pd1uueUWu/POO+2MM86w/v3727x58+yJJ56wTp062cUXX1yo/fHHH29NmjSx2267zfbs2eNsbZSammpPPvmkXXLJJXb88cfboEGDLD093ZYvX27vvPOOnXzyyc5ki7LlzTfftG3btln//v3l33fp0sXS09Nt3Lhxkbe+UpKSkuztt9+2nj172plnnmmffPKJtWnTRra95JJL7JVXXrHf//73NnXqVDv55JNt3759NnfuXHvllVcK9u0Mk5mZaffdd58tXbrUmjVrZi+//LLNnDnTnnnmGatcubKZmV155ZX29NNP29ChQ+3rr7+2nJwce+2112zatGk2atSogl/dzj77bOvRo4fddttttnTpUmvXrp29//779sYbb9gNN9xQKOGlQ4cO9sEHH9jIkSMtMzPTGjVqRDnmEsC4Zlzj8F177bW2c+dOO++886xFixa2d+9emz59esG/RA8bNuyIHp/xFqLkNug4sg5spfLll1+GtnvxxReDxo0bBwkJCUH79u2D995777C2lTtg9OjRQYsWLYLKlSsHdevWDa666qogPz9fHvu2224LzCxo0qSJ9/ymTp0a9OnTJ0hLSwuqVKkS5ObmBkOHDg2++uqrgja/3moJZcfZZ58dVKlSJdixY4e3zdChQ4PKlSsHGzZsKNh+64EHHnDaHTw+1ZjYsGFD0KpVqyAjIyNYsGBBEATu9ltB8Ms2WPfdd1/QunXrIDExMahRo0bQoUOH4M477wy2bNkS+p66desWtG7dOvjqq6+CE088MahSpUqQnZ0djB492mm7bt26YNiwYUHt2rWDhISEoG3btsFzzz3ntNu2bVvwhz/8IcjMzAwqV64cNG3aNHjggQeC/fv3F2o3d+7c4NRTTw2SkpICM2NrpBLCuGZc4/BNnjw5uPTSS4MWLVoEKSkpQUJCQtCkSZPg2muvDdatW1fQzsyCa665xumfnZ1daIz4tpXr27evPD7jzS8uCCI8HQ4AAABUUBX6GWYAAADgUFgwAwAAACFYMAMAAAAhWDADAAAAIVgwAwAAACFYMAMAAAAhWDADAAAAISJX+qO+OI6UktwKvDyM6/j4eCe2b9++Ir1mpUru1NCsWTPZtkGDBk6sfv36sq0q61qvXj0nlpycLPurths2bJBtP/nkEyf2xBNPOLGdO3fK/kXFuEZ5xLgufmpeHDx4sGz7ww8/OLGTTjrJic2bN0/2X758uRPr1KmTbPvBBx84sc8++0y2LeuijGt+YQYAAABCsGAGAAAAQrBgBgAAAEKwYAYAAABCxAURn+AvrQ/bx3JeUZMVVBKVmdmrr77qxNQD9JUrV5b9d+3a5cR69eol215wwQVObP78+bKtcswx7n8L+d5/SSZxlPTxS+u4VtRnama2f/9+J1alShUndvPNN8v+7dq1c2Lt27d3YjVr1pT9U1NTZbwo1qxZI+Pq3tyyZYtsq+IrV650Yuedd57sr8ZGLGOVcY3yiHHt6tixoxPLzs6Wbbt06eLE1Hztu86LFy92YikpKU7s+++/l/2rVq0q40pWVpYTS0tLc2Kffvqp7P/ll186sc2bN0c+/tFE0h8AAABQRCyYAQAAgBAsmAEAAIAQLJgBAACAECyYAQAAgBBlZpeMWHYIiIXK/Fflc83MEhISnJi6LtWqVZP9f/75Zye2e/du2Xbbtm1O7M4773RiCxculP3LErKuo0lMTJTxPXv2OLFBgwY5sbFjx8r+agypcenbjULdFzVq1JBt1T2gdtnw3UPqvli7dq1sm5GR4cQ2btzoxI4//njZv6gY1zgSVNl6dV8dKWVlXBd1l5uLLrrIiak5xcwsOTnZifnmpUWLFjkxtUvGvn37Ih9LzcG+MaGuizq+md6ZS72uKu1tpudx3/fIpk2bnNh7770n2x4J7JIBAAAAFBELZgAAACAEC2YAAAAgBAtmAAAAIISbPVBKxZLcp8pUmpmdf/75TiwzM9OJqYfqzfRD+Bs2bHBiKinDzCw/Pz9yW5WMeN999zkxX7nscePGObHZs2fLtigbfvrpp8htVRLH9u3bZVuVSKfG5axZs2R/lXDiK6Ndq1atSMf3JWCoeUCV9jbT10C9L19p761bt8o4yi6VPO4ba0VNbjvzzDOdmC/B9IwzznBiqiyxmf7OueWWW5zYjz/+KPuvXr1axsubWD6/Sy65xImdfvrpTuy1116T/ZcuXerEkpKSIh9fJcL51jyqtLRax/iS/tS85ttUQV1D9b4WLFgg+6v3oMp4m5l1797diakk7a+++kr2Pxr4hRkAAAAIwYIZAAAACMGCGQAAAAjBghkAAAAIwYIZAAAACFFmSmP7qHLRzZo1k2337t3rxHbs2OHEfO9V7Qagyjk2bdpU9l+xYoUT82XSqjLIKsPfVy45Pj7eif3www+yrcqwPprKSqnVkhZLefhHHnnEiQ0cOFD2nzNnjhOrX7++E/v+++9l/9q1azsx3+4v9erVc2KrVq1yYlWrVpX969at68R8ZbTVbjfqvvjLX/4i+997770yHhXjuvRR91AsOzCdd955Mv7oo486MXUP+XYTUOPSd15qXFeuXNmJqfvSTH8PdO7cWbZVO+uU5XGtdukxMzvrrLOcWJ06dZzYvHnzZH+1jlA7PJj5y1AfrKjlzn2ltXfv3h35nNRnrebmXbt2yf5qZye1jjLT3xnqvBYvXiz7F3X3F0pjAwAAAEXEghkAAAAIwYIZAAAACMGCGQAAAAhRZpL+Lr74YhlXSRjr1q07IuegHlZXSXe+krq+RChFva5KAlDJImY6uUUlXJmZzZw504nddNNNhzjD4lOWk0iOJt+5qus3fvx4J+YrGa+SKFq2bOnEYkn685U/VeeqSv36klBatWrlxFRpbTOzGjVqRH5dpahjg3Fd+qgka1/C0pVXXunEVJK5mVl+fr4TU3OwL+FJJe2pEshmupS7GmsqEc1MJ76lpaXJtup6leVxfdJJJ8l4bm6uE1Ofn2+sLF++3In5vu/VZ62S43xJfyquzlVtcuDjmxfVsdT7Usmhvra+Y6lk1mXLljkxlYxpZjZ9+nQZj4qkPwAAAKCIWDADAAAAIVgwAwAAACFYMAMAAAAhomehlTBfJSJfwk9UKonA9/C3qry0Z88eJ+ar3qce7PclBqhjqZivv3oPvspDxx9/vIyjdIkl2UZVefIlrPjG68F8yRaq8pNKbDEz27lzpxNTiVC+Sn+qf/Xq1WXbW2+91Yk9/PDDTuyLL76Q/YcOHerEnn/+edkWpY+aG9U90KVLF9n/f/7nf5yYLxFP3UOqAqUav2Y6edtXxVXdWyrBKysrS/ZfunSpE1PfY2ZmI0aMkPGyqkGDBjKuEtFUFV+VcGmmE59VRT1f3DdfRqWS84qayOdrq/j6q3PwfY+oSn1qzRRLMmNx4xdmAAAAIAQLZgAAACAEC2YAAAAgBAtmAAAAIAQLZgAAACBEmdklw5edqjJOfRmbUUtd+jJWo+5SEEsmta/Up4qrmMoiNdPv1ZfxWtQMXRS/WHZvURo2bOjEfFn3vvjBYil37csQV2NQjT+1I4yZzpD27fKxYMECGT/YWWedJePvvPOOE2OXjPJHlbU20ztH+O4V9Z2jdsTwZfirssC+cR3LzkyK2pEjPT098rHKsnr16sn4li1bnJgqIe77/FUZct+uROqz9o1BRY0htQ7wfa/HssuEaqvGuq+0utqtScXM9K4y6viqnZkew+vXr5dtDxe/MAMAAAAhWDADAAAAIVgwAwAAACFYMAMAAAAhykzSn6/Mo3rY3fdQ+IYNG5xYLIlUKpFOJWf5HmpXbX3JTeq81PF9yVXqAXjfe1WJWDVq1HBisSQmoGjUZ+VLEFVtVcLaRRddJPurhCF1r/iSaVVyiRqrZvpcY0nWiFqq1cysT58+Tuztt992YqoErpnZCy+8EPlYKH188/DB5s2bJ+MqOS6WcsNqrPvOSc2tsYx1dV6+5C6VuOY7r3/84x9O7Jlnnol8XiWpbt26Tsw3h6lrpa6Tr7S2Wods3LhRtlVx9Vn7Pn/1HqImbpvFto5QbVVMlVs3M2vUqJETU0mPZvq6qPe6bds22b9Vq1ZO7JNPPpFtDxe/MAMAAAAhWDADAAAAIVgwAwAAACFYMAMAAAAhWDADAAAAIcrMLhm+Mo9q54CMjAzZNi8vL1J/384VKmtaZRf7ynirY8Wym4A6vi8TWu1ysXXrVtlWZaJmZ2c7MXbJKDvuvffeSDEzvUtA9erVnZhv5wpVAthHjWE1/qZOnSr79+rVy4n5xuXQoUOd2LXXXnuIM/w/Tz75ZOS2KH1i2QFJUTvFqJLzZrq0sm9uV9R3jm83gai2b98u42pHBfXdWNadeOKJTqxbt26y7UsvveTEGjdu7MT69u0r+z/44INOzLdzhPpcYylXHXVc+dqpHVV8ZbzVDkLqvkhISJD91U4fPXr0kG2//vprJ/buu+86sQ4dOsj+6juLXTIAAACAo4gFMwAAABCCBTMAAAAQggUzAAAAEKJUJv2lpKQ4MZUsZKYTO3xJd+p1fYlwijoHlazhK2EcSxKIot6rL0GxTp06TmzHjh2yrTpf1R8lq6hJTD6qNHZaWlqkmJkePyoxxEyP1xkzZjgxlbRqphM7fEl/OTk5Tqxfv35OTJXLNoue5IvyaeHChU7suOOOk23XrFnjxKpWrerEfOWOVdyXTKvuodq1azsxlYhoZlarVi0n9sMPP8i2Zdnrr7/uxHyJeEOGDHFiN9xwgxP78ssvZX/13Zqeni7bRi337Ev69H2PH8y3NlDrI19pbDWGfa+rqDGcm5sr2/7ud79zYrfeeqsT++9//yv7v/POO5HP63DxCzMAAAAQggUzAAAAEIIFMwAAABCCBTMAAAAQolQm/akkpFgq4vmoh+1VsoWvao2qxqMqEPrOSSUM+ZKI1PtVMV8ypEoCWb58uWz7008/OTH1sD9KJ5V0p8aFL+FIVflSCbLq/vG13bx5s2yrqgWqsdqkSRPZX90DqnKZmU5keeGFF5xYzZo1ZX8S/Cq2b775xomdf/75sq0aK1GTs8x09TXf/bZx40Ynpr6z9uzZI/urZDRV7bM8mjlzZuS4+r5ctGiR7P/b3/7WiY0ZM0a29c1XB/PN12rNoZLrfJsfqO8G35pHUXOwGr9mOvFaJfKZmV144YVO7JFHHol8XkcDvzADAAAAIVgwAwAAACFYMAMAAAAhWDADAAAAIVgwAwAAACFK5S4ZqiSkb5cMlXHqy9BfsWKFE6tbt64T82U3q+xStSOGL7s1an8fdQ18JS3Vzge+UpuK2vkApVMsY1BZvXq1E2vUqJET27Rpk+yvyurOmTNHtlXltVUZdlW+10xnY/vuV5U5rl63V69esv8HH3wg4yi71BzqK/WrdqPw7ZyixrXavcZHjUvf3K52dVHz/a5duyIf/8cff4zctqyI5bNWHn744chtBw4c6MSaNWsm26r1ifqsfOeqymirnTN840f1r1evnmyrXlf1933fZGVlObFXXnlFtv3qq69k/GCx3FexrK+i4BdmAAAAIAQLZgAAACAEC2YAAAAgBAtmAAAAIESZSfrzPVSukv58D3qr8p9NmzZ1Ylu3bpX9VcKRSgJRD8qb6SSEWEpjq/KV69atk/1nz57txJo3by7bqmQuX5Ilyp+oJdt9Y1WNyzZt2si277//vhObNm2aE7vttttkf5UIo0q7m+n7VSWMqJKsZiT9lUexJH2p7yFfCeG9e/c6MTXWfOWuVVvfd14srxuV73ukLCvuhK8wvo0GorZV48dX2lyNi6SkJCfmS/pT/Tds2CDbqmREdayqVavK/r7XLQq1eYKZ//upOLEqAgAAAEKwYAYAAABCsGAGAAAAQrBgBgAAAEKUyqQ/VWXO90C3SuzZsmWLbLtq1SonppIGfceKmkTg668STmJ5UF0le/iSUNTD9p07d5ZtVaU334P1KH2iVrTyfaYqQU8lnPgSplSVM1Vlz8wsPT3dibVs2dKJqWpmZrqime99qUQolXDjS1hBxdatWzcn5kvuUvdL9erVnVi1atVkf5W46kv6iyUhN6r8/Pwi9a/o1PXzfTeruVHNYbVr15b9fRVXD+abr9UY9CWNqvWVSib0bRLgS1yMSq15fO/raCR58gszAAAAEIIFMwAAABCCBTMAAAAQggUzAAAAEIIFMwAAABCiVO6SoTIzVelIM7O6des6sUWLFsm2KrtT7ZLhyxhVWZgqkzmW8qexZHaqtr7+27Ztc2KqpKWZvrZq5wOUTlHL/d53330yXqdOHSemSuX6Ssarce0rV926dWsn1q5dOyemxq/vdX27XKxcudKJqaztopYVRtmmSmCbmXXv3t2JqZ2WzMxSU1OdmPoe8+2coO4t3w4DakcFdQ/GsvuL2n2mrDuapbHVzhW+71C15lBrE98cqj5/NYepMWmmx5oaq2b+nVoO5ltb+HYsK4pYytsXN35hBgAAAEKwYAYAAABCsGAGAAAAQrBgBgAAAEKUmWwXXwKESmxQZaHNzCpXruzEYkkMUA/W+x7MV1RihjonM/1gv0oW8CV2qCQEXwnhXbt2OTFVnhxlW79+/WRcJWaoUqe+sTZz5kwntnHjRtm2RYsWTkyVG/YlrCi+hGB1vzRv3tyJ3X///ZGPhaMnapK0audrq/z1r3+V8Vjme1UGW5VA9pWwjiV53Fd2/mAqkcznpJNOkvFvvvkm8mtUZOr71vf5Rf1cfN/XUcug+46jxpqvrUoGVPeA717zrW/KKn5hBgAAAEKwYAYAAABCsGAGAAAAQrBgBgAAAEKwYAYAAABClMpdMlSZRV8mtMrC9O2SUa1aNSemMo59pRdVdqmK+fqrrFlfJqyislN9pVbXrl3rxOrVqyfbqvcQS1lVHB2x7AZw7rnnOrGsrCzZX5WQVveVun/MzN59910nNn/+fNn2iiuucGJdunRxYr7dCNTuHb6s8fT0dCe2ZMkSJ/bKK6/I/hVZUXeeOFLU5x9Lqdxrr73Wif3xj3+Ubb/99lsnVqtWLdlWXRcV8+1woeK+kt1qvMeye8jq1aud2GmnnSbbjh49WsZxaGqsmunPSn2P++41taOF2s3Ct0uHul98bZWo66DyiF+YAQAAgBAsmAEAAIAQLJgBAACAECyYAQAAgBClMukvIyPDicWSBLRw4ULZViUtqQfgVUld3zmoh/VjSULxvS/1uqoEsC85TyVd+c5LXUOVRICSFUvC1cSJE53Yd999J9uqz79+/fqRXtNMJ/0df/zxsq16XZX46it3rfiSm1RyzPLlyyO/bnnju06xJB5HTTjzUWPNd15FPVbfvn2d2PXXX+/EzjnnHNn/2WefdWL5+fmyrUraU2PYl/SnElR9n4u6hqo0ty8hfNu2bU5MlaxHdFu3bnVidevWlW0zMzMjtd28ebPsrxL01DogagltM7M2bdrIuHpfKiHcl6Ba1CThkk4yPhi/MAMAAAAhWDADAAAAIVgwAwAAACFYMAMAAAAhykzSn+/h79atWzux//znP7Lt+eefH+n4sSRbqAfwfVVvYqlSpR6sV/1r1qwp+6tqh77zUskhqtoiip+vGlQsiaObNm1yYqpK2b/+9S/Z/95773ViM2bMcGK7du2S/YcNG+bEunXrJtuq5BL1ur7KU+p6xXK/vffee7Jt1P6xfC6ljW8OPZpVuo7E9Rs8eLCM33zzzU6sXbt2TuxPf/qT7J+SkuLEFi1aJNuq7yyV9KfamenER19CuEpeV7E9e/bI/uozqFGjhmyLwnwJqhdffLETmzJlimyr5jb1ujt27JD91aYEDRs2dGIbN26U/bdv3+7EfMmsKplQjUtfgqFK/p48ebJsWxbmVn5hBgAAAEKwYAYAAABCsGAGAAAAQrBgBgAAAEKwYAYAAABClMpdMtQODb4Mb1X+1pcdmpqaGun4sZSQjqWd2n3D19aXIX0wX3bzli1bnJgqiWqmy2D7MqxRmC9rWmVCq3EVS/nSUaNGybgaA2r3lKlTp8r+agw++uijTkxlV5uZXXrppU7MV1pdXQN1X/pKWKsdXXyfgSpD/P7778u2FVnt2rWdmG/+UfPKkdKlSxcndssttzixRo0ayf4PP/ywE7v77rudmCqXbWa2fv16J9a0aVPZVu00osoF++4LdW/5dipSOxeo70ffsdR94SujXVGoOURdU9/uP8qSJUtk/Mwzz3RiK1ascGJ5eXmyf1ZWlhNT5++7V9V86/v81TpArbnUrlxmZvXr13diaucMM7OvvvpKxksTfmEGAAAAQrBgBgAAAEKwYAYAAABCsGAGAAAAQpTKpD+VLKFKRZuZrVq1KvLr1qlTx4mpZA1fIlYsCVqKSnjyJRiqhAOViKMSQMx0gt/KlStlW5Uw4LveKMyXjBo1aVMlXJnpcr/Dhw+XbVWp0b/85S9OrG3btrL/1q1bndiVV17pxNauXSv7b9682Yn5EmzT09OdmCrLqhKTzPTc4CvZre6B7777TrZVykKp1lioEuZmZr/97W+d2OLFi2VbVS5azYsNGjSQ/VUJ5+zsbNlWjSGVtKnKwJuZ3XHHHZHOyzcvKur8fXGVTOtLvFZtfXPImjVrnJjvfonKlyBYUfjm8YM1adJExtVn4vv8atWq5cTUOsa3IUBOTo4TU2XY1b1qppP2fNS9qZJhfXOwSjz0Jc6S9AcAAACUcSyYAQAAgBAsmAEAAIAQLJgBAACAECyYAQAAgBClcpcMtUODL4t3/vz5kV9XlQtW2Z2qhLWZ3k1CZaf6suvV+/KVpFTHUq/r66/azp07V7atXr26E1M7JyC6s88+24mlpaU5MV/WtSrB+s0338i2rVq1cmK9evVyYuvWrZP9f/zxRyemsr59u8Scd955Tsx3v+7YscOJqbLAbdq0kf1VqVa1+42Z2SuvvCLjFdV1110n42rnCDVXmukdTTZt2uTEfGNVfa6+z0mVfFflsjt16iT7q50j1G4Evp0v1I4svvLwCxcudGKqtLFvNwZ1LDXWzfQuB2rnBd/OBaptedsR5kjJzMyUcfVZqx24zPTOSO3atXNin3/+uexfr149J6Z2X/HN92rNoMafmVnnzp2dmFpz+XYfysjIcGLq+8rMrFIldznqu4YlhV+YAQAAgBAsmAEAAIAQLJgBAACAECyYAQAAgBClMulPJQz5EvFmzZoV+XVVaWCVsOJLllClKqOW1DTTiXwqFovu3bvLuCqZrZK7zHQiDaWxo5kzZ46Mb9y40YmpJCZfIp1K7EhKSpJtfUlLB8vKypLxDRs2OLGWLVs6MV8JY3Vv+pKj1Lhq1KiRE1OJSWZmPXv2dGLHHnusbKtKIysq2cSs9CWcFNUXX3wh46oEtfr8zXQinfqsW7RoIfura3366afLtio5SZ2Xryy0mgPVd4tvDlbzpa+0tRrD6r5U95pZ9HM10wmZqjz9ihUrZH+V4HfXXXfJtmWZ+lxj+b5Wn4lvDp45c6YT831+ar7Mzc11YmpDATOzxMREJxbL5gXq8/e9L1VeW93vvvWCuofVWDXT3y+LFi2SbZWift5R8AszAAAAEIIFMwAAABCCBTMAAAAQggUzAAAAEKJUJv2pB7V9lYh8FaWUyZMnOzH1UPpPP/0k+6ukKfVgve9cY3kAXVVaU0kIL774ouyvEmEWL14s23bt2tWJUfnJ1b59eyeWk5Mj26prrfpv2bJF9leVk3wJR77knoMdf/zxMt68eXMnpqqJ+cavGiu+ZEZVVfKNN95wYioZ18zstddeixSLRXlL7vO56qqr/v/27tclsjAK4/jZJmgxiGVQg0nFYnGKCAa7yWZRNFgEsSgTJwgmk9Fm9E+wCBoFQcTgDwwqgiCY3bx7nnP2vTur6zjfTzy8d+bOzJ2ZlwvPObKufleiwOTKyoqrqdCgekwzPb0uChiq32b1WUWhTXW9qsCRCnOb6ZB4RF3D6jsQvdbLy0tXi6bKqUClCn2dnp7K41UgeW9vT65tZ+rzrxLwVVPqooYAatqo+g010+//wMBA8XOpyZyqFk32Vd+raGJwX1+fq6nvsJpeGIkC4SrorkJ/UUj3Xwf8FO4wAwAAAAk2zAAAAECCDTMAAACQYMMMAAAAJNgwAwAAAIkv2SVDdQNQaU0zs+vr6+LHbTQaf31O30E0ZlIlhKOEbyfb2tpytaibiOqIodZGyV6VcL66upJrVaeWWq3malH3F5V6VqNSo84X6nVFo60fHx9dbXFxUa5VVJq7Slea6DV8N1GSXFEddTY2NuRaVZ+fn3e19fV1efzExETxeanPr8rrKhV1ZNnZ2XG17e1tufbp6cnVlpeXXW1ubk4e39/f72pqBLaZ2eHhoaupjhpjY2Py+N3dXVnvBFU64oyPj7ta1KlIdX6IRlur6011Senu7pbHn52duZr6rkSjsVWnkOi5bm5uXE118FLjus30+xX956kuZMpndMOIcIcZAAAASLBhBgAAABJsmAEAAIAEG2YAAAAg8SVDfyqYo0bqmpnd398XP64agdruI6CrjImMQmMqXFLlfe0Ux8fHrjY7OyvXqhCF+qzUSFMzs9XV1eLzUp/16+urq0WjVlWQTl0T6jHNdJDm5eVFrp2ZmXG15+dnuVaJgjT4VZVgjLouqxx/cHBQVItMT0/LugoIRkE4RY2MPzo6crVohHCr1Ljpk5MTuVaFo1QY08ysp6fH1VTo7OHh4U+n+K21el2r60+NJTfTv0sXFxdyrRqDPTg46Gpq3LaZ2ejoqKup/VEUcFRro8Cd+m+Ympoqfq6uri5Xi77D0X/GV8IdZgAAACDBhhkAAABIsGEGAAAAEmyYAQAAgAQbZgAAACDx470wNvoRI0kja2trrjYyMiLXLi0tFT9up3fJiOzv77ua6pKxublZfmIV/M9Rl61e1yrdbGY2OTnpasPDw0U1M7Pe3l5Xi8aXRiOvfxd1mHh7e3M1lfCORqur7iF3d3dF51RVq8n3z9TO1zUQ6eTremhoSNZV55Lz83O5tlarudrCwoKrNZtNebx6/9UY79vbW3l8vV4vOicz/RrUaOxovLzqiKH+b8yqdUv6CCXXNXeYAQAAgAQbZgAAACDBhhkAAABIsGEGAAAAEsWhPwAAAKATcYcZAAAASLBhBgAAABJsmAEAAIAEG2YAAAAgwYYZAAAASLBhBgAAABJsmAEAAIAEG2YAAAAgwYYZAAAASPwE3tse9QMU9Z8AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plot more images\n",
        "torch.manual_seed(42)\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "rows, cols = 4, 4\n",
        "for i in range(1, rows * cols + 1):\n",
        "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
        "    img, label = train_data[random_idx]\n",
        "    fig.add_subplot(rows, cols, i)\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "    plt.title(class_names[label])\n",
        "    plt.axis(False);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f356fbe9-95b1-4f81-a82d-dc15b3adc06a",
      "metadata": {
        "id": "f356fbe9-95b1-4f81-a82d-dc15b3adc06a"
      },
      "source": [
        "Hmmm, this dataset doesn't look too aesthetic.\n",
        "\n",
        "But the principles we're going to learn on how to build a model for it will be similar across a wide range of computer vision problems.\n",
        "\n",
        "In essence, taking pixel values and building a model to find patterns in them to use on future pixel values.\n",
        "\n",
        "Plus, even for this small dataset (yes, even 60,000 images in deep learning is considered quite small), could you write a program to classify each one of them?\n",
        "\n",
        "You probably could.\n",
        "\n",
        "But I think coding a model in PyTorch would be faster."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43cdd23d-bd1f-4e8c-ba20-22d2b6ac14b1",
      "metadata": {
        "id": "43cdd23d-bd1f-4e8c-ba20-22d2b6ac14b1"
      },
      "source": [
        "### Prepare DataLoader\n",
        "\n",
        "Now we've got a dataset ready to go.\n",
        "\n",
        "The next step is to prepare it with a [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) or `DataLoader` for short.\n",
        "\n",
        "The `DataLoader` does what you think it might do. It helps load data into a machine learning model for training and for inference. It turns a large `Dataset` into a Python iterable of smaller chunks. These smaller chunks are called **batches** or **mini-batches** and can be set by the `batch_size` parameter.\n",
        "\n",
        "Why do this?\n",
        "\n",
        "Because it's more computationally efficient.\n",
        "\n",
        "In an ideal world you could do the forward pass and backward pass across all of your data at once. But once you start using really large datasets, unless you've got infinite computing power, it's easier to break them up into batches.\n",
        "\n",
        "It also gives your model more opportunities to improve.\n",
        "\n",
        "With **mini-batches** (small portions of the data), gradient descent is performed more often per epoch (once per mini-batch rather than once per epoch).\n",
        "\n",
        "What's a good batch size?\n",
        "\n",
        "[32 is a good place to start](https://twitter.com/ylecun/status/989610208497360896?s=20&t=N96J_jotN--PYuJk2WcjMw) for a fair amount of problems.\n",
        "\n",
        "But since this is a value you can set (a **hyperparameter**) you can try all different kinds of values, though generally powers of 2 are used most often (e.g. 32, 64, 128, 256, 512).\n",
        "\n",
        "![an example of what a batched dataset looks like](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-batching-fashionmnist.png)\n",
        "*Batching FashionMNIST with a batch size of 32 and shuffle turned on. A similar batching process will occur for other datasets but will differ depending on the batch size.*\n",
        "\n",
        "Let's create `DataLoader`'s for our training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "bb2dbf90-a326-43cb-b25b-71af142fafeb",
      "metadata": {
        "id": "bb2dbf90-a326-43cb-b25b-71af142fafeb",
        "outputId": "35d83294-29e8-409d-ce32-900234dce8e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x7d1208173970>, <torch.utils.data.dataloader.DataLoader object at 0x7d1208173490>)\n",
            "Length of train dataloader: 1875 batches of 32\n",
            "Length of test dataloader: 313 batches of 32\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Setup the batch size hyperparameter\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Turn datasets into iterables (batches)\n",
        "train_dataloader = DataLoader(train_data, # dataset to turn into iterable\n",
        "    batch_size=BATCH_SIZE, # how many samples per batch?\n",
        "    shuffle=True # shuffle data every epoch?\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False # don't necessarily have to shuffle the testing data\n",
        ")\n",
        "\n",
        "# Let's check out what we've created\n",
        "print(f\"Dataloaders: {train_dataloader, test_dataloader}\")\n",
        "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
        "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7a925ee7-484b-4149-be8f-3ad790172a5f",
      "metadata": {
        "id": "7a925ee7-484b-4149-be8f-3ad790172a5f",
        "outputId": "0553be92-30dd-4256-d361-19fd49a4ed2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 1, 28, 28]), torch.Size([32]))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Check out what's inside the training dataloader\n",
        "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
        "train_features_batch.shape, train_labels_batch.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fee4cf8-ab73-4c81-8e5e-3c81691e799c",
      "metadata": {
        "id": "4fee4cf8-ab73-4c81-8e5e-3c81691e799c"
      },
      "source": [
        "And we can see that the data remains unchanged by checking a single sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c863d66a-49be-43be-84dc-372a5d6fc2c2",
      "metadata": {
        "id": "c863d66a-49be-43be-84dc-372a5d6fc2c2",
        "outputId": "df4ce945-ca51-453b-be00-06afc7df5ee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size: torch.Size([1, 28, 28])\n",
            "Label: 6, label size: torch.Size([])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Show a sample\n",
        "torch.manual_seed(42)\n",
        "random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
        "img, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
        "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.title(class_names[label])\n",
        "plt.axis(\"Off\");\n",
        "print(f\"Image size: {img.shape}\")\n",
        "print(f\"Label: {label}, label size: {label.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db1695cf-f53d-4c7c-ad39-dfed76533125",
      "metadata": {
        "id": "db1695cf-f53d-4c7c-ad39-dfed76533125"
      },
      "source": [
        "### Build a NN model\n",
        "\n",
        "Now that we have loaded data. It is time to build a **deep neural network model** by subclassing `nn.Module`.\n",
        "\n",
        "Our baseline will consist of three [`nn.Linear()`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layers. We'll also put non-linear functions (nn.ReLU()) in between each linear layer.\n",
        "\n",
        "Since we're working with image data, we're going to use a different layer to start things off. And that's the [`nn.Flatten()`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) layer.\n",
        "\n",
        "`nn.Flatten()` compresses the dimensions of a tensor into a single vector.\n",
        "\n",
        "This is easier to understand when you see it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "405319f1-f242-4bd9-90f5-3abdc50782ac",
      "metadata": {
        "id": "405319f1-f242-4bd9-90f5-3abdc50782ac",
        "outputId": "539b3e94-8862-42e6-b29b-5b4d630721d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape before flattening: torch.Size([1, 28, 28]) -> [color_channels, height, width]\n",
            "Shape after flattening: torch.Size([1, 784]) -> [color_channels, height*width]\n"
          ]
        }
      ],
      "source": [
        "# Create a flatten layer\n",
        "flatten_model = nn.Flatten() # all nn modules function as a model (can do a forward pass)\n",
        "\n",
        "# Get a single sample\n",
        "x = train_features_batch[0] # (1, 28, 28)\n",
        "\n",
        "# Flatten the sample\n",
        "output = flatten_model(x) # perform forward pass\n",
        "\n",
        "# Print out what happened\n",
        "print(f\"Shape before flattening: {x.shape} -> [color_channels, height, width]\")\n",
        "print(f\"Shape after flattening: {output.shape} -> [color_channels, height*width]\")\n",
        "\n",
        "# Try uncommenting below and see what happens\n",
        "#print(x)\n",
        "#print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86bb7806-fca6-45af-8111-3e00e38f5be9",
      "metadata": {
        "id": "86bb7806-fca6-45af-8111-3e00e38f5be9"
      },
      "source": [
        "The `nn.Flatten()` layer took our shape from `[color_channels, height, width]` to `[color_channels, height*width]`.\n",
        "\n",
        "Why do we do this?\n",
        "\n",
        "Because we've now turned our pixel data from height and width dimensions into one long **feature vector**. And `nn.Linear()` layers like their inputs to be in the form of feature vectors.\n",
        "\n",
        "Let's create our first model using `nn.Flatten()` as the first layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "1449f427-6859-41ae-8133-50b58ffbce72",
      "metadata": {
        "id": "1449f427-6859-41ae-8133-50b58ffbce72"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "class FashionMNISTModel(nn.Module):\n",
        "    def __init__(self, input_shape: int, hidden_units_1: int, hidden_units_2: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.layer_stack = nn.Sequential(\n",
        "            nn.Flatten(), # flatten inputs into single vector\n",
        "            nn.Linear(in_features=input_shape, out_features=hidden_units_1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=hidden_units_1, out_features=hidden_units_2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=hidden_units_2, out_features=output_shape),\n",
        "            # Don't add a softmax layer here, since it will be included in the cross entropy loss function!\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.layer_stack(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d1b50bf-d00b-485c-be00-b3e4de156fab",
      "metadata": {
        "id": "4d1b50bf-d00b-485c-be00-b3e4de156fab"
      },
      "source": [
        "Wonderful!\n",
        "\n",
        "We've got a baseline model class we can use, now let's instantiate a model.\n",
        "\n",
        "We'll need to set the following parameters:\n",
        "* `input_shape=784` - this is how many features you've got going in the model, in our case, it's one for every pixel in the target image (28 pixels high by 28 pixels wide = 784 features).\n",
        "* `hidden_units_1=300` - number of units/neurons in the first hidden layer.\n",
        "* `hidden_units_2=100` - number of units/neurons in the second hidden layer.\n",
        "* `output_shape=len(class_names)` - since we're working with a multi-class classification problem, we need an output neuron per class in our dataset.\n",
        "\n",
        "Let's create an instance of our model and send to the CPU for now (we'll run a small test for running `fashionMNISTModel` on CPU vs. a similar model on GPU soon)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "dd18384a-76f9-4b5a-a013-fda077f16865",
      "metadata": {
        "id": "dd18384a-76f9-4b5a-a013-fda077f16865",
        "outputId": "6f97d86f-d6dc-4146-96c3-ccd7ff382a79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FashionMNISTModel(\n",
              "  (layer_stack): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=784, out_features=300, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=300, out_features=100, bias=True)\n",
              "    (4): ReLU()\n",
              "    (5): Linear(in_features=100, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Need to setup model with input parameters\n",
        "fashionMNISTModel = FashionMNISTModel(input_shape=784, # one for every pixel (28x28)\n",
        "    hidden_units_1=300, # how many units in the first hiden layer\n",
        "    hidden_units_2=100, # how many units in the second hiden layer\n",
        "    output_shape=len(class_names) # one for every class\n",
        ")\n",
        "fashionMNISTModel.to(\"cpu\") # keep model on CPU to begin with"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03243179-1cdc-45d9-8b8c-82538ac02e9c",
      "metadata": {
        "id": "03243179-1cdc-45d9-8b8c-82538ac02e9c"
      },
      "source": [
        "### Setup loss, optimizer, and evaluation metrics\n",
        "\n",
        "Since we're working on a classification problem, let's bring in our [`helper_functions.py` script](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/helper_functions.py) and subsequently the `accuracy_fn()` we defined in [notebook 02](https://www.learnpytorch.io/02_pytorch_classification/).\n",
        "\n",
        "> **Note:** Rather than importing and using our own accuracy function or evaluation metric(s), you could import various evaluation metrics from the [TorchMetrics package](https://torchmetrics.readthedocs.io/en/latest/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "31c91f17-d810-46a4-97c3-c734f93430b1",
      "metadata": {
        "id": "31c91f17-d810-46a4-97c3-c734f93430b1",
        "outputId": "cdadb0d5-b9b8-4259-af8a-62e850812180",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading helper_functions.py\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  # Note: you need the \"raw\" GitHub URL for this to work\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(request.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ce3d13b8-f018-4b44-8bba-375074dc4c5f",
      "metadata": {
        "id": "ce3d13b8-f018-4b44-8bba-375074dc4c5f"
      },
      "outputs": [],
      "source": [
        "# Import accuracy metric\n",
        "from helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy()\n",
        "\n",
        "# Setup loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss() # includes softmax!\n",
        "optimizer = torch.optim.SGD(params=fashionMNISTModel.parameters(), lr=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4109f867-83f2-4394-a925-8acdc63ccffe",
      "metadata": {
        "id": "4109f867-83f2-4394-a925-8acdc63ccffe"
      },
      "source": [
        "### Creating a function to time our experiments\n",
        "\n",
        "Loss function and optimizer ready!\n",
        "\n",
        "It's time to start training a model.\n",
        "\n",
        "But how about we do a little experiment while we train.\n",
        "\n",
        "I mean, let's make a timing function to measure the time it takes our model to train on CPU versus using a GPU.\n",
        "\n",
        "We'll train this model on the CPU but the next one on the GPU and see what happens.\n",
        "\n",
        "Our timing function will import the [`timeit.default_timer()` function](https://docs.python.org/3/library/timeit.html#timeit.default_timer) from the Python [`timeit` module](https://docs.python.org/3/library/timeit.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "31adc3fe-ce90-4b4e-b0d4-3613abae5714",
      "metadata": {
        "id": "31adc3fe-ce90-4b4e-b0d4-3613abae5714"
      },
      "outputs": [],
      "source": [
        "from timeit import default_timer as timer\n",
        "def print_train_time(start: float, end: float, device: torch.device = None):\n",
        "    \"\"\"Prints difference between start and end time.\n",
        "\n",
        "    Args:\n",
        "        start (float): Start time of computation (preferred in timeit format).\n",
        "        end (float): End time of computation.\n",
        "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        float: time between start and end in seconds (higher is longer).\n",
        "    \"\"\"\n",
        "    total_time = end - start\n",
        "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
        "    return total_time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07b9560e-f5dc-45d6-b3b2-ddae17a71b34",
      "metadata": {
        "id": "07b9560e-f5dc-45d6-b3b2-ddae17a71b34"
      },
      "source": [
        "### Creating a training loop and training a model on batches of data\n",
        "\n",
        "Beautiful!\n",
        "\n",
        "Looks like we've got all of the pieces of the puzzle ready to go, a timer, a loss function, an optimizer, a model and most importantly, some data.\n",
        "\n",
        "Let's now create a training loop and a testing loop to train and evaluate our model.\n",
        "\n",
        "We'll be using the same steps as the previous notebook(s), though since our data is now in batch form, we'll add another loop to loop through our data batches.\n",
        "\n",
        "Our data batches are contained within our `DataLoader`s, `train_dataloader` and `test_dataloader` for the training and test data splits respectively.\n",
        "\n",
        "A batch is `BATCH_SIZE` samples of `X` (features) and `y` (labels), since we're using `BATCH_SIZE=32`, our batches have 32 samples of images and targets.\n",
        "\n",
        "And since we're computing on batches of data, our loss and evaluation metrics will be calculated **per batch** rather than across the whole dataset.\n",
        "\n",
        "This means we'll have to divide our loss and accuracy values by the number of batches in each dataset's respective dataloader.\n",
        "\n",
        "Let's step through it:\n",
        "1. Loop through epochs.\n",
        "2. Loop through training batches, perform training steps, calculate the train loss *per batch*.\n",
        "3. Loop through testing batches, perform testing steps, calculate the test loss *per batch*.\n",
        "4. Print out what's happening.\n",
        "5. Time it all (for fun).\n",
        "\n",
        "A fair few steps but...\n",
        "\n",
        "...if in doubt, code it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "c07bbf10-81e3-47f0-990d-9a4a838276ab",
      "metadata": {
        "id": "c07bbf10-81e3-47f0-990d-9a4a838276ab",
        "outputId": "727ce1ba-d961-41ad-de5b-fc951343a4cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606,
          "referenced_widgets": [
            "1c919e376d864fb294279638fdeae304",
            "69d799775eda40f2b25c2e397071d660",
            "77cb12b7edcc4937bc1edf0ea09759d5",
            "a8339910e5fa4ed7911e3825e61ce44e",
            "d0d8db2f15a6428da4c4aac405d73794",
            "6828dab460d142aabd6ced801148c095",
            "28f50172dc284acb9d09bb7dacf0a51e",
            "bf7c054cf7374d9c944e39410a8166bd",
            "3cee61ca41894450b0e769a12cb7e9f9",
            "b1d78ba7fb334f3e856d4cad0bd2f63d",
            "b6a295fd826d4c4fb63a0c8b5783094c"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c919e376d864fb294279638fdeae304"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "-------\n",
            "Looked at 0/60000 samples\n",
            "Looked at 12800/60000 samples\n",
            "Looked at 25600/60000 samples\n",
            "Looked at 38400/60000 samples\n",
            "Looked at 51200/60000 samples\n",
            "\n",
            "Train loss: 0.59401 | Test loss: 0.44606, Test acc: 83.61%\n",
            "\n",
            "Epoch: 1\n",
            "-------\n",
            "Looked at 0/60000 samples\n",
            "Looked at 12800/60000 samples\n",
            "Looked at 25600/60000 samples\n",
            "Looked at 38400/60000 samples\n",
            "Looked at 51200/60000 samples\n",
            "\n",
            "Train loss: 0.40143 | Test loss: 0.41204, Test acc: 85.25%\n",
            "\n",
            "Epoch: 2\n",
            "-------\n",
            "Looked at 0/60000 samples\n",
            "Looked at 12800/60000 samples\n",
            "Looked at 25600/60000 samples\n",
            "Looked at 38400/60000 samples\n",
            "Looked at 51200/60000 samples\n",
            "\n",
            "Train loss: 0.35793 | Test loss: 0.38085, Test acc: 86.36%\n",
            "\n",
            "Train time on cpu: 38.274 seconds\n"
          ]
        }
      ],
      "source": [
        "# Import tqdm for progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Set the seed and start the timer\n",
        "torch.manual_seed(42)\n",
        "train_time_start_on_cpu = timer()\n",
        "\n",
        "# Set the number of epochs (we'll keep this small for faster training times)\n",
        "epochs = 3\n",
        "\n",
        "# Create training and testing loop\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\\n-------\")\n",
        "    ### Training\n",
        "    train_loss = 0\n",
        "    # Add a loop to loop through training batches\n",
        "    for batch, (X, y) in enumerate(train_dataloader):\n",
        "        fashionMNISTModel.train()\n",
        "        # 1. Forward pass\n",
        "        y_pred = fashionMNISTModel(X)\n",
        "\n",
        "        # 2. Calculate loss (per batch)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss # accumulatively add up the loss per epoch\n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print out how many samples have been seen\n",
        "        if batch % 400 == 0:\n",
        "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
        "\n",
        "    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
        "    train_loss /= len(train_dataloader)\n",
        "\n",
        "    ### Testing\n",
        "    # Setup variables for accumulatively adding up loss and accuracy\n",
        "    test_loss, test_acc = 0, 0\n",
        "    fashionMNISTModel.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in test_dataloader:\n",
        "            # 1. Forward pass\n",
        "            test_pred = fashionMNISTModel(X)\n",
        "\n",
        "            # 2. Calculate loss (accumatively)\n",
        "            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n",
        "\n",
        "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
        "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
        "\n",
        "        # Calculations on test metrics need to happen inside torch.inference_mode()\n",
        "        # Divide total test loss by length of test dataloader (per batch)\n",
        "        test_loss /= len(test_dataloader)\n",
        "\n",
        "        # Divide total accuracy by length of test dataloader (per batch)\n",
        "        test_acc /= len(test_dataloader)\n",
        "\n",
        "    ## Print out what's happening\n",
        "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n",
        "\n",
        "# Calculate training time\n",
        "train_time_end_on_cpu = timer()\n",
        "total_train_time_fashionMNISTModel = print_train_time(start=train_time_start_on_cpu,\n",
        "                                           end=train_time_end_on_cpu,\n",
        "                                           device=str(next(fashionMNISTModel.parameters()).device))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b02a939-a3a1-4a9d-bb9d-62928def2ded",
      "metadata": {
        "id": "7b02a939-a3a1-4a9d-bb9d-62928def2ded"
      },
      "source": [
        "Nice! Looks like our baseline model did fairly well.\n",
        "\n",
        "It didn't take too long to train either, even just on the CPU, I wonder if it'll speed up on the GPU?\n",
        "\n",
        "Let's write some code to evaluate our model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7442511b-bfe9-4ec7-9f5b-9c808f8e560b",
      "metadata": {
        "id": "7442511b-bfe9-4ec7-9f5b-9c808f8e560b"
      },
      "source": [
        "### 4. Make predictions and get results\n",
        "\n",
        "Since we're going to be building a few models, it's a good idea to write some code to evaluate them all in similar ways.\n",
        "\n",
        "Namely, let's create a function that takes in a trained model, a `DataLoader`, a loss function and an accuracy function.\n",
        "\n",
        "The function will use the model to make predictions on the data in the `DataLoader` and then we can evaluate those predictions using the loss function and accuracy function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "8317dd04-9de2-4fd7-97bd-1e202621397d",
      "metadata": {
        "id": "8317dd04-9de2-4fd7-97bd-1e202621397d",
        "outputId": "d3a120e3-6a27-415d-f528-17f39689203a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model_name': 'FashionMNISTModel',\n",
              " 'model_loss': 0.38085290789604187,\n",
              " 'model_acc': 86.36182108626198}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "def eval_model(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               accuracy_fn):\n",
        "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
        "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
        "        loss_fn (torch.nn.Module): The loss function of model.\n",
        "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
        "\n",
        "    Returns:\n",
        "        (dict): Results of model making predictions on data_loader.\n",
        "    \"\"\"\n",
        "    loss, acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "            # Make predictions with the model\n",
        "            y_pred = model(X)\n",
        "\n",
        "            # Accumulate the loss and accuracy values per batch\n",
        "            loss += loss_fn(y_pred, y)\n",
        "            acc += accuracy_fn(y_true=y,\n",
        "                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)\n",
        "\n",
        "        # Scale loss and acc to find the average loss/acc per batch\n",
        "        loss /= len(data_loader)\n",
        "        acc /= len(data_loader)\n",
        "\n",
        "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
        "            \"model_loss\": loss.item(),\n",
        "            \"model_acc\": acc}\n",
        "\n",
        "# Calculate model 0 results on test dataset\n",
        "fashionMNISTModel_results = eval_model(model=fashionMNISTModel, data_loader=test_dataloader,\n",
        "    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n",
        ")\n",
        "fashionMNISTModel_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a39c3042-1262-4d1f-b33e-c8e2ba6781d3",
      "metadata": {
        "id": "a39c3042-1262-4d1f-b33e-c8e2ba6781d3"
      },
      "source": [
        "Looking good!\n",
        "\n",
        "We can use this dictionary to compare the baseline model results to other models later on."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b76784d-4cdb-43d2-a6da-8e4da9a812a9",
      "metadata": {
        "id": "3b76784d-4cdb-43d2-a6da-8e4da9a812a9"
      },
      "source": [
        "### 5. Setup device agnostic-code (for using a GPU if there is one)\n",
        "We've seen how long it takes to train ma PyTorch model on 60,000 samples on CPU.\n",
        "\n",
        "> **Note:** Model training time is dependent on hardware used. Generally, more processors means faster training and smaller models on smaller datasets will often train faster than large models and large datasets.\n",
        "\n",
        "Now let's setup some [device-agnostic code](https://pytorch.org/docs/stable/notes/cuda.html#best-practices) for our models and data to run on GPU if it's available.\n",
        "\n",
        "If you're running this notebook on Google Colab, and you don't a GPU turned on yet, it's now time to turn one on via `Runtime -> Change runtime type -> Hardware accelerator -> GPU`. If you do this, your runtime will likely reset and you'll have to run all of the cells above by going `Runtime -> Run before`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "17b69fe9-f974-4538-922c-20c5cc8220cc",
      "metadata": {
        "id": "17b69fe9-f974-4538-922c-20c5cc8220cc",
        "outputId": "e97d248c-2ab3-45c6-c9c9-49bdec938f6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# Setup device agnostic code\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "fashionMNISTModel.to(device) # move model to GPU if available, else keep on\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eb30af6-a355-49a2-a59f-25169fd27a6e",
      "metadata": {
        "id": "1eb30af6-a355-49a2-a59f-25169fd27a6e"
      },
      "source": [
        "### Functionizing training and test loops\n",
        "\n",
        "So far we've been writing train and test loops over and over.\n",
        "\n",
        "Let's write them again but this time we'll put them in functions so they can be called again and again.\n",
        "\n",
        "And because we're using device-agnostic code now, we'll be sure to call `.to(device)` on our feature (`X`) and target (`y`) tensors.\n",
        "\n",
        "For the training loop we'll create a function called `train_step()` which takes in a model, a `DataLoader` a loss function and an optimizer.\n",
        "\n",
        "The testing loop will be similar but it'll be called `test_step()` and it'll take in a model, a `DataLoader`, a loss function and an evaluation function.\n",
        "\n",
        "> **Note:** Since these are functions, you can customize them in any way you like. What we're making here can be considered barebones training and testing functions for our specific classification use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "3d239ed2-4028-4603-8db3-ffca2b727819",
      "metadata": {
        "id": "3d239ed2-4028-4603-8db3-ffca2b727819"
      },
      "outputs": [],
      "source": [
        "def train_step(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               accuracy_fn,\n",
        "               device: torch.device = device):\n",
        "    train_loss, train_acc = 0, 0\n",
        "    for batch, (X, y) in enumerate(data_loader):\n",
        "        # Send data to GPU\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # 2. Calculate loss\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss\n",
        "        train_acc += accuracy_fn(y_true=y,\n",
        "                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -> pred labels\n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate loss and accuracy per epoch and print out what's happening\n",
        "    train_loss /= len(data_loader)\n",
        "    train_acc /= len(data_loader)\n",
        "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
        "\n",
        "def test_step(data_loader: torch.utils.data.DataLoader,\n",
        "              model: torch.nn.Module,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              accuracy_fn,\n",
        "              device: torch.device = device):\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model.eval() # put model in eval mode\n",
        "    # Turn on inference context manager\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "            # Send data to GPU\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # 1. Forward pass\n",
        "            test_pred = model(X)\n",
        "\n",
        "            # 2. Calculate loss and accuracy\n",
        "            test_loss += loss_fn(test_pred, y)\n",
        "            test_acc += accuracy_fn(y_true=y,\n",
        "                y_pred=test_pred.argmax(dim=1) # Go from logits -> pred labels\n",
        "            )\n",
        "\n",
        "        # Adjust metrics and print out\n",
        "        test_loss /= len(data_loader)\n",
        "        test_acc /= len(data_loader)\n",
        "        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e44121b6-c4be-4909-9175-dc9bd8dc6273",
      "metadata": {
        "id": "e44121b6-c4be-4909-9175-dc9bd8dc6273"
      },
      "source": [
        "Woohoo!\n",
        "\n",
        "Now we've got some functions for training and testing our model, let's run them.\n",
        "\n",
        "We'll do so inside another loop for each epoch.\n",
        "\n",
        "That way for each epoch we're going a training and a testing step.\n",
        "\n",
        "> **Note:** You can customize how often you do a testing step. Sometimes people do them every five epochs or 10 epochs or in our case, every epoch.\n",
        "\n",
        "Let's also time things to see how long our code takes to run on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "2bb8094b-01a0-4b84-9526-ba8888d04901",
      "metadata": {
        "id": "2bb8094b-01a0-4b84-9526-ba8888d04901",
        "outputId": "d31c2eac-fd06-43b1-916a-e8d7da91a3ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "92e1437d16554378b7645b0618aaa433",
            "b8c1f0401bd34817819b8f6aca8dcea0",
            "0b0683e477274b45a22d5b4cb99ba14d",
            "e8ebc2dcfe6842529ac3a99f94252aae",
            "2a2f72864d374f63955f86b092a8e127",
            "00bc1b8e560845d988b2f38242fdd088",
            "bbf6a85cf3c043559cc92c07592d1b85",
            "38b820d701a0460db7faec9d54f7b1c8",
            "891b0c76e4484fde99b228c7bb0c68e2",
            "c9a6f74d8dd14d0a91ca40eeb4d3a8fb",
            "a53fffa625b2404fb1ccd06d3678325d"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92e1437d16554378b7645b0618aaa433"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "---------\n",
            "Train loss: 0.33117 | Train accuracy: 87.74%\n",
            "Test loss: 0.35833 | Test accuracy: 86.93%\n",
            "\n",
            "Epoch: 1\n",
            "---------\n",
            "Train loss: 0.31017 | Train accuracy: 88.39%\n",
            "Test loss: 0.35096 | Test accuracy: 87.11%\n",
            "\n",
            "Epoch: 2\n",
            "---------\n",
            "Train loss: 0.29311 | Train accuracy: 88.95%\n",
            "Test loss: 0.34792 | Test accuracy: 87.48%\n",
            "\n",
            "Epoch: 3\n",
            "---------\n",
            "Train loss: 0.28259 | Train accuracy: 89.27%\n",
            "Test loss: 0.33441 | Test accuracy: 87.65%\n",
            "\n",
            "Epoch: 4\n",
            "---------\n",
            "Train loss: 0.27020 | Train accuracy: 89.76%\n",
            "Test loss: 0.36551 | Test accuracy: 86.96%\n",
            "\n",
            "Epoch: 5\n",
            "---------\n",
            "Train loss: 0.26005 | Train accuracy: 90.16%\n",
            "Test loss: 0.33912 | Test accuracy: 87.40%\n",
            "\n",
            "Epoch: 6\n",
            "---------\n",
            "Train loss: 0.25030 | Train accuracy: 90.58%\n",
            "Test loss: 0.32857 | Test accuracy: 88.19%\n",
            "\n",
            "Epoch: 7\n",
            "---------\n",
            "Train loss: 0.24237 | Train accuracy: 90.78%\n",
            "Test loss: 0.32875 | Test accuracy: 88.34%\n",
            "\n",
            "Epoch: 8\n",
            "---------\n",
            "Train loss: 0.23374 | Train accuracy: 91.10%\n",
            "Test loss: 0.34243 | Test accuracy: 88.11%\n",
            "\n",
            "Epoch: 9\n",
            "---------\n",
            "Train loss: 0.22692 | Train accuracy: 91.41%\n",
            "Test loss: 0.33135 | Test accuracy: 88.29%\n",
            "\n",
            "Epoch: 10\n",
            "---------\n",
            "Train loss: 0.22154 | Train accuracy: 91.57%\n",
            "Test loss: 0.33411 | Test accuracy: 88.28%\n",
            "\n",
            "Epoch: 11\n",
            "---------\n",
            "Train loss: 0.21375 | Train accuracy: 91.83%\n",
            "Test loss: 0.34733 | Test accuracy: 88.02%\n",
            "\n",
            "Epoch: 12\n",
            "---------\n",
            "Train loss: 0.20705 | Train accuracy: 92.12%\n",
            "Test loss: 0.31951 | Test accuracy: 89.08%\n",
            "\n",
            "Epoch: 13\n",
            "---------\n",
            "Train loss: 0.20171 | Train accuracy: 92.31%\n",
            "Test loss: 0.32162 | Test accuracy: 88.93%\n",
            "\n",
            "Epoch: 14\n",
            "---------\n",
            "Train loss: 0.19715 | Train accuracy: 92.47%\n",
            "Test loss: 0.33873 | Test accuracy: 88.35%\n",
            "\n",
            "Epoch: 15\n",
            "---------\n",
            "Train loss: 0.18992 | Train accuracy: 92.69%\n",
            "Test loss: 0.32815 | Test accuracy: 89.16%\n",
            "\n",
            "Epoch: 16\n",
            "---------\n",
            "Train loss: 0.18668 | Train accuracy: 92.89%\n",
            "Test loss: 0.35374 | Test accuracy: 88.32%\n",
            "\n",
            "Epoch: 17\n",
            "---------\n",
            "Train loss: 0.17927 | Train accuracy: 93.03%\n",
            "Test loss: 0.33047 | Test accuracy: 89.01%\n",
            "\n",
            "Epoch: 18\n",
            "---------\n",
            "Train loss: 0.17625 | Train accuracy: 93.26%\n",
            "Test loss: 0.35198 | Test accuracy: 88.55%\n",
            "\n",
            "Epoch: 19\n",
            "---------\n",
            "Train loss: 0.17260 | Train accuracy: 93.40%\n",
            "Test loss: 0.35197 | Test accuracy: 88.87%\n",
            "\n",
            "Epoch: 20\n",
            "---------\n",
            "Train loss: 0.16858 | Train accuracy: 93.49%\n",
            "Test loss: 0.36596 | Test accuracy: 88.80%\n",
            "\n",
            "Epoch: 21\n",
            "---------\n",
            "Train loss: 0.16385 | Train accuracy: 93.77%\n",
            "Test loss: 0.33486 | Test accuracy: 89.76%\n",
            "\n",
            "Epoch: 22\n",
            "---------\n",
            "Train loss: 0.15949 | Train accuracy: 93.83%\n",
            "Test loss: 0.32742 | Test accuracy: 89.94%\n",
            "\n",
            "Epoch: 23\n",
            "---------\n",
            "Train loss: 0.15599 | Train accuracy: 93.96%\n",
            "Test loss: 0.35291 | Test accuracy: 89.30%\n",
            "\n",
            "Epoch: 24\n",
            "---------\n",
            "Train loss: 0.15384 | Train accuracy: 94.20%\n",
            "Test loss: 0.36819 | Test accuracy: 89.02%\n",
            "\n",
            "Epoch: 25\n",
            "---------\n",
            "Train loss: 0.14650 | Train accuracy: 94.41%\n",
            "Test loss: 0.35961 | Test accuracy: 89.16%\n",
            "\n",
            "Epoch: 26\n",
            "---------\n",
            "Train loss: 0.14460 | Train accuracy: 94.46%\n",
            "Test loss: 0.36410 | Test accuracy: 89.77%\n",
            "\n",
            "Epoch: 27\n",
            "---------\n",
            "Train loss: 0.14228 | Train accuracy: 94.50%\n",
            "Test loss: 0.36779 | Test accuracy: 89.30%\n",
            "\n",
            "Epoch: 28\n",
            "---------\n",
            "Train loss: 0.13891 | Train accuracy: 94.61%\n",
            "Test loss: 0.36832 | Test accuracy: 89.52%\n",
            "\n",
            "Epoch: 29\n",
            "---------\n",
            "Train loss: 0.13266 | Train accuracy: 94.92%\n",
            "Test loss: 0.38050 | Test accuracy: 88.79%\n",
            "\n",
            "Train time on cuda: 306.177 seconds\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Measure time\n",
        "from timeit import default_timer as timer\n",
        "train_time_start_on_gpu = timer()\n",
        "\n",
        "epochs = 30\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\\n---------\")\n",
        "    train_step(data_loader=train_dataloader,\n",
        "        model=fashionMNISTModel,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        accuracy_fn=accuracy_fn\n",
        "    )\n",
        "    test_step(data_loader=test_dataloader,\n",
        "        model=fashionMNISTModel,\n",
        "        loss_fn=loss_fn,\n",
        "        accuracy_fn=accuracy_fn\n",
        "    )\n",
        "\n",
        "train_time_end_on_gpu = timer()\n",
        "total_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,\n",
        "                                            end=train_time_end_on_gpu,\n",
        "                                            device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "719b8eb9-9a7f-42ed-a49f-5eedc6fdd720",
      "metadata": {
        "id": "719b8eb9-9a7f-42ed-a49f-5eedc6fdd720"
      },
      "source": [
        "Excellent!\n",
        "\n",
        "Our model trained but the training time took longer?\n",
        "\n",
        "> **Note:** The training time on CUDA vs CPU will depend largely on the quality of the CPU/GPU you're using. Read on for a more explained answer.\n",
        "\n",
        "> **Question:** \"I used a a GPU but my model didn't train faster, why might that be?\"\n",
        ">\n",
        "> **Answer:** Well, one reason could be because your dataset and model are both so small (like the dataset and model we're working with) the benefits of using a GPU are outweighed by the time it actually takes to transfer the data there.\n",
        ">\n",
        "> There's a small bottleneck between copying data from the CPU memory (default) to the GPU memory.\n",
        ">\n",
        "> So for smaller models and datasets, the CPU might actually be the optimal place to compute on.\n",
        ">\n",
        "> But for larger datasets and models, the speed of computing the GPU can offer usually far outweighs the cost of getting the data there.\n",
        ">\n",
        "> However, this is largely dependant on the hardware you're using. With practice, you will get used to where the best place to train your models is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "f3665d99-1adc-4d9f-bfc6-e5601a80691c",
      "metadata": {
        "id": "f3665d99-1adc-4d9f-bfc6-e5601a80691c",
        "outputId": "55ebf14f-8028-42c5-a38c-992f48f92ea3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model_name': 'FashionMNISTModel',\n",
              " 'model_loss': 0.3805044889450073,\n",
              " 'model_acc': 88.7879392971246}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# Move values to device\n",
        "torch.manual_seed(42)\n",
        "def eval_model(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               accuracy_fn,\n",
        "               device: torch.device = device):\n",
        "    \"\"\"Evaluates a given model on a given dataset.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
        "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
        "        loss_fn (torch.nn.Module): The loss function of model.\n",
        "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
        "        device (str, optional): Target device to compute on. Defaults to device.\n",
        "\n",
        "    Returns:\n",
        "        (dict): Results of model making predictions on data_loader.\n",
        "    \"\"\"\n",
        "    loss, acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "            # Send data to the target device\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_pred = model(X)\n",
        "            loss += loss_fn(y_pred, y)\n",
        "            acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "        # Scale loss and acc\n",
        "        loss /= len(data_loader)\n",
        "        acc /= len(data_loader)\n",
        "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
        "            \"model_loss\": loss.item(),\n",
        "            \"model_acc\": acc}\n",
        "\n",
        "# Calculate model 1 results with device-agnostic code\n",
        "fashionMNISTModel_results = eval_model(model=fashionMNISTModel, data_loader=test_dataloader,\n",
        "    loss_fn=loss_fn, accuracy_fn=accuracy_fn,\n",
        "    device=device\n",
        ")\n",
        "fashionMNISTModel_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "a9e916cf-f873-4481-a983-bac26ce4cac2",
      "metadata": {
        "id": "a9e916cf-f873-4481-a983-bac26ce4cac2",
        "outputId": "d730e522-ceba-4d12-971d-7f3222398315",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model_name': 'FashionMNISTModel',\n",
              " 'model_loss': 0.3805044889450073,\n",
              " 'model_acc': 88.7879392971246}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# Check baseline results\n",
        "fashionMNISTModel_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "340cbf14-e83f-4981-8a93-5fedb6b51418",
      "metadata": {
        "id": "340cbf14-e83f-4981-8a93-5fedb6b51418"
      },
      "source": [
        "Woah, in this case, it looks like adding non-linearities to our model made it perform worse than the baseline.\n",
        "\n",
        "That's a thing to note in machine learning, sometimes the thing you thought should work doesn't.\n",
        "\n",
        "And then the thing you thought might not work does.\n",
        "\n",
        "It's part science, part art.\n",
        "\n",
        "From the looks of things, it seems like our model is **overfitting** on the training data.\n",
        "\n",
        "Overfitting means our model is learning the training data well but those patterns aren't generalizing to the testing data.\n",
        "\n",
        "Two of the main to fix overfitting include:\n",
        "1. Using a smaller or different model (some models fit certain kinds of data better than others).\n",
        "2. Using a larger dataset (the more data, the more chance a model has to learn generalizable patterns).\n",
        "\n",
        "There are more, but I'm going to leave that as a challenge for you to explore.\n",
        "\n",
        "Try searching online, \"ways to prevent overfitting in machine learning\" and see what comes up.\n",
        "\n",
        "In the meantime, let's take a look at number 1: using a different model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a20f25e-cc16-4f85-a69b-62008c01d0ed",
      "metadata": {
        "id": "0a20f25e-cc16-4f85-a69b-62008c01d0ed"
      },
      "source": [
        "Nice!\n",
        "\n",
        "Our biggest model yet!\n",
        "\n",
        "What we've done is a common practice in machine learning.\n",
        "\n",
        "Find a model architecture somewhere and replicate it with code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "058b01ac-3f6a-4472-bcbf-3377974e3254",
      "metadata": {
        "id": "058b01ac-3f6a-4472-bcbf-3377974e3254",
        "outputId": "e9722202-7847-4b8a-fbb0-3cabc7394ba6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image batch shape: torch.Size([32, 3, 64, 64]) -> [batch_size, color_channels, height, width]\n",
            "Single image shape: torch.Size([3, 64, 64]) -> [color_channels, height, width]\n",
            "Single image pixel values:\n",
            "tensor([[[ 1.9269,  1.4873,  0.9007,  ...,  1.8446, -1.1845,  1.3835],\n",
            "         [ 1.4451,  0.8564,  2.2181,  ...,  0.3399,  0.7200,  0.4114],\n",
            "         [ 1.9312,  1.0119, -1.4364,  ..., -0.5558,  0.7043,  0.7099],\n",
            "         ...,\n",
            "         [-0.5610, -0.4830,  0.4770,  ..., -0.2713, -0.9537, -0.6737],\n",
            "         [ 0.3076, -0.1277,  0.0366,  ..., -2.0060,  0.2824, -0.8111],\n",
            "         [-1.5486,  0.0485, -0.7712,  ..., -0.1403,  0.9416, -0.0118]],\n",
            "\n",
            "        [[-0.5197,  1.8524,  1.8365,  ...,  0.8935, -1.5114, -0.8515],\n",
            "         [ 2.0818,  1.0677, -1.4277,  ...,  1.6612, -2.6223, -0.4319],\n",
            "         [-0.1010, -0.4388, -1.9775,  ...,  0.2106,  0.2536, -0.7318],\n",
            "         ...,\n",
            "         [ 0.2779,  0.7342, -0.3736,  ..., -0.4601,  0.1815,  0.1850],\n",
            "         [ 0.7205, -0.2833,  0.0937,  ..., -0.1002, -2.3609,  2.2465],\n",
            "         [-1.3242, -0.1973,  0.2920,  ...,  0.5409,  0.6940,  1.8563]],\n",
            "\n",
            "        [[-0.7978,  1.0261,  1.1465,  ...,  1.2134,  0.9354, -0.0780],\n",
            "         [-1.4647, -1.9571,  0.1017,  ..., -1.9986, -0.7409,  0.7011],\n",
            "         [-1.3938,  0.8466, -1.7191,  ..., -1.1867,  0.1320,  0.3407],\n",
            "         ...,\n",
            "         [ 0.8206, -0.3745,  1.2499,  ..., -0.0676,  0.0385,  0.6335],\n",
            "         [-0.5589, -0.3393,  0.2347,  ...,  2.1181,  2.4569,  1.3083],\n",
            "         [-0.4092,  1.5199,  0.2401,  ..., -0.2558,  0.7870,  0.9924]]])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Create sample batch of random numbers with same size as image batch\n",
        "images = torch.randn(size=(32, 3, 64, 64)) # [batch_size, color_channels, height, width]\n",
        "test_image = images[0] # get a single image for testing\n",
        "print(f\"Image batch shape: {images.shape} -> [batch_size, color_channels, height, width]\")\n",
        "print(f\"Single image shape: {test_image.shape} -> [color_channels, height, width]\")\n",
        "print(f\"Single image pixel values:\\n{test_image}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25818e83-89de-496d-8b56-af4fc9f2acc5",
      "metadata": {
        "id": "25818e83-89de-496d-8b56-af4fc9f2acc5"
      },
      "source": [
        "### 11. Save and load best performing model\n",
        "\n",
        "Let's finish this section off by saving and loading in our best performing model.\n",
        "\n",
        "Recall from [notebook 01](https://www.learnpytorch.io/01_pytorch_workflow/#5-saving-and-loading-a-pytorch-model) we can save and load a PyTorch model using a combination of:\n",
        "* `torch.save` - a function to save a whole PyTorch model or a model's `state_dict()`.\n",
        "* `torch.load` - a function to load in a saved PyTorch object.\n",
        "* `torch.nn.Module.load_state_dict()` - a function to load a saved `state_dict()` into an existing model instance.\n",
        "\n",
        "You can see more of these three in the [PyTorch saving and loading models documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html).\n",
        "\n",
        "For now, let's save our `model_2`'s `state_dict()` then load it back in and evaluate it to make sure the save and load went correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "d058e8fa-560f-4350-a154-49593ff403c9",
      "metadata": {
        "id": "d058e8fa-560f-4350-a154-49593ff403c9",
        "outputId": "c5bf34d7-fa62-4e88-b71a-75f506cc3e62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model to: models/pytorch_fashionMNISTModel.pth\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Create models directory (if it doesn't already exist), see: https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True, # create parent directories if needed\n",
        "                 exist_ok=True # if models directory already exists, don't error\n",
        ")\n",
        "\n",
        "# Create model save path\n",
        "MODEL_NAME = \"pytorch_fashionMNISTModel.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "\n",
        "# Save the model state dict\n",
        "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
        "torch.save(obj=fashionMNISTModel.state_dict(), # only saving the state_dict() only saves the learned parameters\n",
        "           f=MODEL_SAVE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1542284-8132-42ba-b00d-57e9b9037e4e",
      "metadata": {
        "id": "a1542284-8132-42ba-b00d-57e9b9037e4e"
      },
      "source": [
        "Now we've got a saved model `state_dict()` we can load it back in using a combination of `load_state_dict()` and `torch.load()`.\n",
        "\n",
        "Since we're using `load_state_dict()`, we'll need to create a new instance of `FashionMNISTModelV2()` with the same input parameters as our saved model `state_dict()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "634a8f7a-3013-4b45-b365-49b286d3c478",
      "metadata": {
        "id": "634a8f7a-3013-4b45-b365-49b286d3c478",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa6af2d6-769f-483c-bbfd-37f7fcf66744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-19642cc231e6>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  loaded_model.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n"
          ]
        }
      ],
      "source": [
        "# Create a new instance of FashionMNISTModel (the same class as our saved state_dict())\n",
        "# Note: loading model will error if the shapes here aren't the same as the saved version\n",
        "loaded_model = FashionMNISTModel(input_shape=784, # one for every pixel (28x28)\n",
        "                                hidden_units_1=300, # how many units in the first hiden layer\n",
        "                                hidden_units_2=100, # how many units in the second hiden layer\n",
        "                                output_shape=len(class_names) # one for every class\n",
        ")\n",
        "\n",
        "# Load in the saved state_dict()\n",
        "loaded_model.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
        "\n",
        "# Send model to GPU\n",
        "loaded_model = loaded_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feeaebf4-6040-4fa5-852d-5eb8d2bbb94c",
      "metadata": {
        "id": "feeaebf4-6040-4fa5-852d-5eb8d2bbb94c"
      },
      "source": [
        "And now we've got a loaded model we can evaluate it with `eval_model()` to make sure its parameters work similarly to `model_2` prior to saving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "3e3bcd06-d99b-47bc-8828-9e3903285599",
      "metadata": {
        "id": "3e3bcd06-d99b-47bc-8828-9e3903285599",
        "outputId": "3b54fa27-2c0d-4c94-892b-d7c60cdcd8c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model_name': 'FashionMNISTModel',\n",
              " 'model_loss': 0.3805044889450073,\n",
              " 'model_acc': 88.7879392971246}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# Evaluate loaded model\n",
        "torch.manual_seed(42)\n",
        "\n",
        "loaded_model_results = eval_model(\n",
        "    model=loaded_model,\n",
        "    data_loader=test_dataloader,\n",
        "    loss_fn=loss_fn,\n",
        "    accuracy_fn=accuracy_fn\n",
        ")\n",
        "\n",
        "loaded_model_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2b37855-c0da-4834-a2d4-a0faa8410b65",
      "metadata": {
        "id": "c2b37855-c0da-4834-a2d4-a0faa8410b65"
      },
      "source": [
        "Do these results look the same as `fashionMNISTModel_results`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "68544254-c99a-47ec-a32f-9816c21a993e",
      "metadata": {
        "id": "68544254-c99a-47ec-a32f-9816c21a993e",
        "outputId": "6838673a-4489-4f0d-d00f-896fe54e86ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model_name': 'FashionMNISTModel',\n",
              " 'model_loss': 0.3805044889450073,\n",
              " 'model_acc': 88.7879392971246}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "fashionMNISTModel_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ee07f93-4344-4c7a-8b1d-92a56034e7b2",
      "metadata": {
        "id": "0ee07f93-4344-4c7a-8b1d-92a56034e7b2"
      },
      "source": [
        "We can find out if two tensors are close to each other using `torch.isclose()` and passing in a tolerance level of closeness via the parameters `atol` (absolute tolerance) and `rtol` (relative tolerance).\n",
        "\n",
        "If our model's results are close, the output of `torch.isclose()` should be true."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "48dcf0ba-7e00-4406-8aaa-41918856361a",
      "metadata": {
        "id": "48dcf0ba-7e00-4406-8aaa-41918856361a",
        "outputId": "21677aef-9ee8-45d4-deaf-6b448942abb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# Check to see if results are close to each other (if they are very far away, there may be an error)\n",
        "torch.isclose(torch.tensor(fashionMNISTModel_results[\"model_loss\"]),\n",
        "              torch.tensor(loaded_model_results[\"model_loss\"]),\n",
        "              atol=1e-08, # absolute tolerance\n",
        "              rtol=0.0001) # relative tolerance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression Task Using DNN"
      ],
      "metadata": {
        "id": "Y6-EoMW5_Cob"
      },
      "id": "Y6-EoMW5_Cob"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using PyTorch to build, train, evaluate, and use a regression neural network to make predictions is quite similar to what we did for classification. The main differences are the fact that the output layer has a single neuron (since we only want to predict a single value) and uses NO activation function, and the loss function is the mean squared error. Since the dataset is quite noisy, we just use a single hidden layer with fewer neurons than before, to avoid overfitting."
      ],
      "metadata": {
        "id": "BZnV3k8c_A8T"
      },
      "id": "BZnV3k8c_A8T"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "FjAs3IDNE-3C"
      },
      "id": "FjAs3IDNE-3C"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# np.random.seed(42)\n",
        "# torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "HNPcno0HFBPP"
      },
      "id": "HNPcno0HFBPP",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting a dataset\n",
        "Let's load, split and scale the California housing dataset. To better understand this task, read https://www.kaggle.com/camnugent/california-housing-prices and https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html"
      ],
      "metadata": {
        "id": "mjVivmhp81pn"
      },
      "id": "mjVivmhp81pn"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "JLSmxxqy82Cl"
      },
      "id": "JLSmxxqy82Cl",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input and output shapes of the data\n"
      ],
      "metadata": {
        "id": "10xc6frU_KXF"
      },
      "id": "10xc6frU_KXF"
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X_train.shape: {X_train.shape}\")\n",
        "print(f\"Y_train.shape: {y_train.shape}\")\n",
        "print(f\"X_test_full.shape: {X_test.shape}\")\n",
        "print(f\"Y_test_full.shape: {y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Q9N7WDA_Q_g",
        "outputId": "f92bbbe5-b0e4-478c-a90b-677e9cda5e23"
      },
      "id": "5Q9N7WDA_Q_g",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train.shape: (15480, 8)\n",
            "Y_train.shape: (15480,)\n",
            "X_test_full.shape: (5160, 8)\n",
            "Y_test_full.shape: (5160,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See the input feature names."
      ],
      "metadata": {
        "id": "5O6GfTVtA7oT"
      },
      "id": "5O6GfTVtA7oT"
    },
    {
      "cell_type": "code",
      "source": [
        "housing.feature_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2NXjZUGA-1v",
        "outputId": "f85e49f2-33de-4db5-e351-321d5f9b5966"
      },
      "id": "z2NXjZUGA-1v",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['MedInc',\n",
              " 'HouseAge',\n",
              " 'AveRooms',\n",
              " 'AveBedrms',\n",
              " 'Population',\n",
              " 'AveOccup',\n",
              " 'Latitude',\n",
              " 'Longitude']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the first example input and output."
      ],
      "metadata": {
        "id": "wzn_ZPK1AFei"
      },
      "id": "wzn_ZPK1AFei"
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg_OkeIdAINA",
        "outputId": "db0a1fa7-d001-4c80-b6e4-c4ed231792bc"
      },
      "id": "wg_OkeIdAINA",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.17648852,  0.66640687, -0.06085431, -0.2811182 , -0.49654414,\n",
              "       -0.04828325, -0.8607415 ,  0.73099911])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9OVCKhgAK2X",
        "outputId": "efac5975-d09f-477e-e650-cbf18bc370ca"
      },
      "id": "s9OVCKhgAK2X",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.285"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare DataLoader"
      ],
      "metadata": {
        "id": "3LsxF7KFB1wP"
      },
      "id": "3LsxF7KFB1wP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to convert Numpy ndarray to PyTorch Dataset first."
      ],
      "metadata": {
        "id": "WxyNFu-2FkqA"
      },
      "id": "WxyNFu-2FkqA"
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train_tensor = torch.Tensor(X_train) # transform to PyTorch tensor\n",
        "# y_train_tensor = torch.Tensor(y_train)\n",
        "# X_test_tensor = torch.Tensor(X_test)\n",
        "# y_test_tensor = torch.Tensor(y_test)\n",
        "\n",
        "X_train_tensor = torch.from_numpy(X_train).type(torch.float)\n",
        "y_train_tensor = torch.from_numpy(y_train).type(torch.float)\n",
        "X_test_tensor = torch.from_numpy(X_test).type(torch.float)\n",
        "y_test_tensor= torch.from_numpy(y_test).type(torch.float)\n",
        "\n",
        "train_data = TensorDataset(X_train_tensor,y_train_tensor) # create your dataset\n",
        "test_data = TensorDataset(X_test_tensor,y_test_tensor) # create your dataset"
      ],
      "metadata": {
        "id": "tWgGU6nECMQm"
      },
      "id": "tWgGU6nECMQm",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the training set and testing set are in PyTorch Dataset, we can prepare the data loader."
      ],
      "metadata": {
        "id": "Iars7wj9FsRd"
      },
      "id": "Iars7wj9FsRd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the batch size hyperparameter\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Turn datasets into iterables (batches)\n",
        "train_dataloader = DataLoader(train_data, # dataset to turn into iterable\n",
        "    batch_size=BATCH_SIZE, # how many samples per batch?\n",
        "    shuffle=True # shuffle data every epoch?\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False # don't necessarily have to shuffle the testing data\n",
        ")\n",
        "\n",
        "# Let's check out what we've created\n",
        "print(f\"Dataloaders: {train_dataloader, test_dataloader}\")\n",
        "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
        "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTYadS9nFSuI",
        "outputId": "65e8e1db-a8c6-42a5-f248-3cbe18c141fb"
      },
      "id": "MTYadS9nFSuI",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x7d1207aab850>, <torch.utils.data.dataloader.DataLoader object at 0x7d1207aabd00>)\n",
            "Length of train dataloader: 484 batches of 32\n",
            "Length of test dataloader: 162 batches of 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build a NN model"
      ],
      "metadata": {
        "id": "p0woTwGkB5JT"
      },
      "id": "p0woTwGkB5JT"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "# class CaliforniaHousingRegressionModel(nn.Module):\n",
        "#     def __init__(self, input_shape: int, hidden_units_1: int, hidden_units_2: int, hidden_units_3: int, output_shape: int):\n",
        "#         super().__init__()\n",
        "#         self.layer_stack = nn.Sequential(\n",
        "#             nn.Linear(in_features=input_shape, out_features=hidden_units_1),\n",
        "#             nn.Linear(in_features=hidden_units_1, out_features=hidden_units_2),\n",
        "#             nn.Linear(in_features=hidden_units_2, out_features=hidden_units_3),\n",
        "#             nn.Linear(in_features=hidden_units_3, out_features=output_shape)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x: torch.Tensor):\n",
        "#         return self.layer_stack(x)\n",
        "\n",
        "# class CaliforniaHousingRegressionModel(nn.Module):\n",
        "#     def __init__(self, input_shape: int, output_shape: int):\n",
        "#         super().__init__()\n",
        "#         self.layer_stack = nn.Sequential(\n",
        "#             nn.Linear(in_features=input_shape, out_features=output_shape)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x: torch.Tensor):\n",
        "#         return self.layer_stack(x)\n",
        "class CaliforniaHousingRegressionModel(nn.Module):\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.layer_stack = nn.Sequential(\n",
        "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
        "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "-MQ_Qsx8CMwp"
      },
      "id": "-MQ_Qsx8CMwp",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to setup model with input parameters\n",
        "# californiaHousingRegressionModel = CaliforniaHousingRegressionModel(input_shape=8, # 8 input features\n",
        "#     hidden_units_1=500, # how many units in the hiden layer\n",
        "#     hidden_units_2=1000, # how many units in the hiden layer\n",
        "#     hidden_units_3=200, # how many units in the hiden layer\n",
        "#     output_shape=1\n",
        "# )\n",
        "\n",
        "# californiaHousingRegressionModel = CaliforniaHousingRegressionModel(input_shape=8, # 8 input features\n",
        "#     output_shape=1\n",
        "# )\n",
        "\n",
        "californiaHousingRegressionModel = CaliforniaHousingRegressionModel(input_shape=8, # 8 input features\n",
        "                                                                    hidden_units=10, # how many units in the hiden layer\n",
        "                                                                    output_shape=1\n",
        ")\n",
        "californiaHousingRegressionModel.to(\"cpu\") # keep model on CPU to begin with"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZguRSay_DrvK",
        "outputId": "6ee6a0d1-a8aa-456f-85a4-a853376b377d"
      },
      "id": "ZguRSay_DrvK",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CaliforniaHousingRegressionModel(\n",
              "  (layer_stack): Sequential(\n",
              "    (0): Linear(in_features=8, out_features=10, bias=True)\n",
              "    (1): Linear(in_features=10, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred = californiaHousingRegressionModel(X_train_tensor)\n",
        "Y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57Ce-S_FfgHD",
        "outputId": "cfe3ab53-1b71-498e-eb22-beea01980e2f"
      },
      "id": "57Ce-S_FfgHD",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1068],\n",
              "        [ 0.2214],\n",
              "        [ 0.1596],\n",
              "        ...,\n",
              "        [ 0.0484],\n",
              "        [-0.3736],\n",
              "        [ 0.1737]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup loss, optimizer, and evaluation metrics"
      ],
      "metadata": {
        "id": "V30VKQivB83H"
      },
      "id": "V30VKQivB83H"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup loss function and optimizer\n",
        "loss_fn = nn.MSELoss() # Mean Square Error Loss\n",
        "optimizer = torch.optim.Adam(params=californiaHousingRegressionModel.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "GiX9YkBXCNMj"
      },
      "id": "GiX9YkBXCNMj",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a function to time our experiments\n",
        "\n",
        "Loss function and optimizer ready!\n",
        "\n",
        "It's time to start training a model.\n",
        "\n",
        "But how about we do a little experiment while we train.\n",
        "\n",
        "I mean, let's make a timing function to measure the time it takes our model to train on CPU versus using a GPU.\n",
        "\n",
        "We'll train this model on the CPU but the next one on the GPU and see what happens.\n",
        "\n",
        "Our timing function will import the [`timeit.default_timer()` function](https://docs.python.org/3/library/timeit.html#timeit.default_timer) from the Python [`timeit` module](https://docs.python.org/3/library/timeit.html)."
      ],
      "metadata": {
        "id": "cjexPHJ9GXOe"
      },
      "id": "cjexPHJ9GXOe"
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "def print_train_time(start: float, end: float, device: torch.device = None):\n",
        "    \"\"\"Prints difference between start and end time.\n",
        "\n",
        "    Args:\n",
        "        start (float): Start time of computation (preferred in timeit format).\n",
        "        end (float): End time of computation.\n",
        "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        float: time between start and end in seconds (higher is longer).\n",
        "    \"\"\"\n",
        "    total_time = end - start\n",
        "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
        "    return total_time"
      ],
      "metadata": {
        "id": "r7egwXeeGavG"
      },
      "id": "r7egwXeeGavG",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and test"
      ],
      "metadata": {
        "id": "3paGzZBsCH7Q"
      },
      "id": "3paGzZBsCH7Q"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tqdm for progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Set the seed and start the timer\n",
        "torch.manual_seed(42)\n",
        "train_time_start_on_cpu = timer()\n",
        "\n",
        "# Set the number of epochs (we'll keep this small for faster training times)\n",
        "epochs = 1000\n",
        "\n",
        "# Create empty loss lists to track values\n",
        "train_loss_values = []\n",
        "test_loss_values = []\n",
        "epoch_count = []\n",
        "\n",
        "# Create training and testing loop\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\\n-------\")\n",
        "    ### Training\n",
        "    train_loss = 0\n",
        "    # Add a loop to loop through training batches\n",
        "    for batch, (X, y) in enumerate(train_dataloader):\n",
        "        californiaHousingRegressionModel.train()\n",
        "        # 1. Forward pass\n",
        "        y_pred = californiaHousingRegressionModel(X)\n",
        "\n",
        "        # 2. Calculate loss (per batch)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss # accumulatively add up the loss per epoch\n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print out how many samples have been seen\n",
        "        # if batch % 100 == 0:\n",
        "        #     print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
        "\n",
        "    # Calculate loss per epoch and print out what's happening\n",
        "    train_loss /= len(train_dataloader)\n",
        "    print(f\"Train loss: {train_loss:.5f}\")\n",
        "\n",
        "    ### Testing\n",
        "    # Setup variables for accumulatively adding up loss\n",
        "    test_loss = 0\n",
        "    californiaHousingRegressionModel.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in test_dataloader:\n",
        "            # 1. Forward pass\n",
        "            test_pred = californiaHousingRegressionModel(X)\n",
        "\n",
        "            # 2. Calculate loss (accumatively)\n",
        "            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n",
        "\n",
        "        # Calculations on test metrics need to happen inside torch.inference_mode()\n",
        "        # Divide total test loss by length of test dataloader (per batch)\n",
        "        test_loss /= len(test_dataloader)\n",
        "\n",
        "    ## Print out what's happening\n",
        "    if epoch % 10 == 0:\n",
        "      epoch_count.append(epoch)\n",
        "      train_loss_values.append(train_loss.detach().numpy())\n",
        "      test_loss_values.append(test_loss.detach().numpy())\n",
        "      print(f\"Epoch: {epoch} | MSE Train Loss: {train_loss} | MSE Test Loss: {test_loss} \")\n",
        "\n",
        "# Calculate training time\n",
        "train_time_end_on_cpu = timer()\n",
        "total_train_time_californiaHousingRegressionModel = print_train_time(start=train_time_start_on_cpu,\n",
        "                                           end=train_time_end_on_cpu,\n",
        "                                           device=str(next(californiaHousingRegressionModel.parameters()).device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fea3b2ef577f46a69c69b84dbf7ea50e",
            "ff1ea5f67002439c9aa4aa5eafa389e9",
            "405c4f89d7cf433a90a19b6e430775b3",
            "187c3495931f4fbeac925e605afb5419",
            "5fb1ff518a104a7298c844ce912affef",
            "4054a98bd5b94a9f989b170e17dedd6d",
            "d19dfac458d4465faf9877c83e08263c",
            "078b62523dd345eabc2c52dd146edab0",
            "540a5629bff84cceb8bc42137fd7e96c",
            "be474ede7489437e82d4f61f617e7a3d",
            "74f6055ac4094a06afae151b4d5b27c9"
          ]
        },
        "id": "0vI8PeTSCOCh",
        "outputId": "b1e459d1-545e-47c8-be45-4f467d05ee59"
      },
      "id": "0vI8PeTSCOCh",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fea3b2ef577f46a69c69b84dbf7ea50e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "-------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([24])) that is different to the input size (torch.Size([24, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 2.86925\n",
            "Epoch: 0 | MSE Train Loss: 2.869252920150757 | MSE Test Loss: 1.3594112396240234 \n",
            "Epoch: 1\n",
            "-------\n",
            "Train loss: 1.34009\n",
            "Epoch: 2\n",
            "-------\n",
            "Train loss: 1.33506\n",
            "Epoch: 3\n",
            "-------\n",
            "Train loss: 1.33421\n",
            "Epoch: 4\n",
            "-------\n",
            "Train loss: 1.33452\n",
            "Epoch: 5\n",
            "-------\n",
            "Train loss: 1.33544\n",
            "Epoch: 6\n",
            "-------\n",
            "Train loss: 1.33535\n",
            "Epoch: 7\n",
            "-------\n",
            "Train loss: 1.33822\n",
            "Epoch: 8\n",
            "-------\n",
            "Train loss: 1.33499\n",
            "Epoch: 9\n",
            "-------\n",
            "Train loss: 1.33578\n",
            "Epoch: 10\n",
            "-------\n",
            "Train loss: 1.33496\n",
            "Epoch: 10 | MSE Train Loss: 1.3349586725234985 | MSE Test Loss: 1.329413652420044 \n",
            "Epoch: 11\n",
            "-------\n",
            "Train loss: 1.33514\n",
            "Epoch: 12\n",
            "-------\n",
            "Train loss: 1.33470\n",
            "Epoch: 13\n",
            "-------\n",
            "Train loss: 1.33472\n",
            "Epoch: 14\n",
            "-------\n",
            "Train loss: 1.33486\n",
            "Epoch: 15\n",
            "-------\n",
            "Train loss: 1.33554\n",
            "Epoch: 16\n",
            "-------\n",
            "Train loss: 1.33446\n",
            "Epoch: 17\n",
            "-------\n",
            "Train loss: 1.33522\n",
            "Epoch: 18\n",
            "-------\n",
            "Train loss: 1.33508\n",
            "Epoch: 19\n",
            "-------\n",
            "Train loss: 1.33742\n",
            "Epoch: 20\n",
            "-------\n",
            "Train loss: 1.33449\n",
            "Epoch: 20 | MSE Train Loss: 1.3344882726669312 | MSE Test Loss: 1.330377459526062 \n",
            "Epoch: 21\n",
            "-------\n",
            "Train loss: 1.33495\n",
            "Epoch: 22\n",
            "-------\n",
            "Train loss: 1.33499\n",
            "Epoch: 23\n",
            "-------\n",
            "Train loss: 1.33547\n",
            "Epoch: 24\n",
            "-------\n",
            "Train loss: 1.33623\n",
            "Epoch: 25\n",
            "-------\n",
            "Train loss: 1.33526\n",
            "Epoch: 26\n",
            "-------\n",
            "Train loss: 1.33669\n",
            "Epoch: 27\n",
            "-------\n",
            "Train loss: 1.33528\n",
            "Epoch: 28\n",
            "-------\n",
            "Train loss: 1.33552\n",
            "Epoch: 29\n",
            "-------\n",
            "Train loss: 1.33509\n",
            "Epoch: 30\n",
            "-------\n",
            "Train loss: 1.33517\n",
            "Epoch: 30 | MSE Train Loss: 1.3351675271987915 | MSE Test Loss: 1.3291596174240112 \n",
            "Epoch: 31\n",
            "-------\n",
            "Train loss: 1.33751\n",
            "Epoch: 32\n",
            "-------\n",
            "Train loss: 1.33514\n",
            "Epoch: 33\n",
            "-------\n",
            "Train loss: 1.33476\n",
            "Epoch: 34\n",
            "-------\n",
            "Train loss: 1.33537\n",
            "Epoch: 35\n",
            "-------\n",
            "Train loss: 1.33526\n",
            "Epoch: 36\n",
            "-------\n",
            "Train loss: 1.33522\n",
            "Epoch: 37\n",
            "-------\n",
            "Train loss: 1.33567\n",
            "Epoch: 38\n",
            "-------\n",
            "Train loss: 1.33596\n",
            "Epoch: 39\n",
            "-------\n",
            "Train loss: 1.33424\n",
            "Epoch: 40\n",
            "-------\n",
            "Train loss: 1.33489\n",
            "Epoch: 40 | MSE Train Loss: 1.3348920345306396 | MSE Test Loss: 1.3302353620529175 \n",
            "Epoch: 41\n",
            "-------\n",
            "Train loss: 1.33506\n",
            "Epoch: 42\n",
            "-------\n",
            "Train loss: 1.33545\n",
            "Epoch: 43\n",
            "-------\n",
            "Train loss: 1.33550\n",
            "Epoch: 44\n",
            "-------\n",
            "Train loss: 1.33588\n",
            "Epoch: 45\n",
            "-------\n",
            "Train loss: 1.33527\n",
            "Epoch: 46\n",
            "-------\n",
            "Train loss: 1.33474\n",
            "Epoch: 47\n",
            "-------\n",
            "Train loss: 1.33572\n",
            "Epoch: 48\n",
            "-------\n",
            "Train loss: 1.33505\n",
            "Epoch: 49\n",
            "-------\n",
            "Train loss: 1.33411\n",
            "Epoch: 50\n",
            "-------\n",
            "Train loss: 1.33545\n",
            "Epoch: 50 | MSE Train Loss: 1.3354548215866089 | MSE Test Loss: 1.329041838645935 \n",
            "Epoch: 51\n",
            "-------\n",
            "Train loss: 1.33572\n",
            "Epoch: 52\n",
            "-------\n",
            "Train loss: 1.33526\n",
            "Epoch: 53\n",
            "-------\n",
            "Train loss: 1.33495\n",
            "Epoch: 54\n",
            "-------\n",
            "Train loss: 1.33452\n",
            "Epoch: 55\n",
            "-------\n",
            "Train loss: 1.33460\n",
            "Epoch: 56\n",
            "-------\n",
            "Train loss: 1.33616\n",
            "Epoch: 57\n",
            "-------\n",
            "Train loss: 1.33466\n",
            "Epoch: 58\n",
            "-------\n",
            "Train loss: 1.33504\n",
            "Epoch: 59\n",
            "-------\n",
            "Train loss: 1.33539\n",
            "Epoch: 60\n",
            "-------\n",
            "Train loss: 1.33531\n",
            "Epoch: 60 | MSE Train Loss: 1.33530592918396 | MSE Test Loss: 1.330478310585022 \n",
            "Epoch: 61\n",
            "-------\n",
            "Train loss: 1.33580\n",
            "Epoch: 62\n",
            "-------\n",
            "Train loss: 1.33476\n",
            "Epoch: 63\n",
            "-------\n",
            "Train loss: 1.33502\n",
            "Epoch: 64\n",
            "-------\n",
            "Train loss: 1.33536\n",
            "Epoch: 65\n",
            "-------\n",
            "Train loss: 1.33504\n",
            "Epoch: 66\n",
            "-------\n",
            "Train loss: 1.33531\n",
            "Epoch: 67\n",
            "-------\n",
            "Train loss: 1.33528\n",
            "Epoch: 68\n",
            "-------\n",
            "Train loss: 1.33934\n",
            "Epoch: 69\n",
            "-------\n",
            "Train loss: 1.33459\n",
            "Epoch: 70\n",
            "-------\n",
            "Train loss: 1.33479\n",
            "Epoch: 70 | MSE Train Loss: 1.3347939252853394 | MSE Test Loss: 1.330322504043579 \n",
            "Epoch: 71\n",
            "-------\n",
            "Train loss: 1.33507\n",
            "Epoch: 72\n",
            "-------\n",
            "Train loss: 1.33524\n",
            "Epoch: 73\n",
            "-------\n",
            "Train loss: 1.33478\n",
            "Epoch: 74\n",
            "-------\n",
            "Train loss: 1.33475\n",
            "Epoch: 75\n",
            "-------\n",
            "Train loss: 1.33481\n",
            "Epoch: 76\n",
            "-------\n",
            "Train loss: 1.33497\n",
            "Epoch: 77\n",
            "-------\n",
            "Train loss: 1.33448\n",
            "Epoch: 78\n",
            "-------\n",
            "Train loss: 1.33430\n",
            "Epoch: 79\n",
            "-------\n",
            "Train loss: 1.33569\n",
            "Epoch: 80\n",
            "-------\n",
            "Train loss: 1.33622\n",
            "Epoch: 80 | MSE Train Loss: 1.3362191915512085 | MSE Test Loss: 1.3313542604446411 \n",
            "Epoch: 81\n",
            "-------\n",
            "Train loss: 1.33477\n",
            "Epoch: 82\n",
            "-------\n",
            "Train loss: 1.33487\n",
            "Epoch: 83\n",
            "-------\n",
            "Train loss: 1.33398\n",
            "Epoch: 84\n",
            "-------\n",
            "Train loss: 1.33484\n",
            "Epoch: 85\n",
            "-------\n",
            "Train loss: 1.33490\n",
            "Epoch: 86\n",
            "-------\n",
            "Train loss: 1.33556\n",
            "Epoch: 87\n",
            "-------\n",
            "Train loss: 1.33439\n",
            "Epoch: 88\n",
            "-------\n",
            "Train loss: 1.33504\n",
            "Epoch: 89\n",
            "-------\n",
            "Train loss: 1.33553\n",
            "Epoch: 90\n",
            "-------\n",
            "Train loss: 1.33462\n",
            "Epoch: 90 | MSE Train Loss: 1.33462393283844 | MSE Test Loss: 1.3312309980392456 \n",
            "Epoch: 91\n",
            "-------\n",
            "Train loss: 1.33627\n",
            "Epoch: 92\n",
            "-------\n",
            "Train loss: 1.33514\n",
            "Epoch: 93\n",
            "-------\n",
            "Train loss: 1.33488\n",
            "Epoch: 94\n",
            "-------\n",
            "Train loss: 1.33611\n",
            "Epoch: 95\n",
            "-------\n",
            "Train loss: 1.33465\n",
            "Epoch: 96\n",
            "-------\n",
            "Train loss: 1.33550\n",
            "Epoch: 97\n",
            "-------\n",
            "Train loss: 1.33573\n",
            "Epoch: 98\n",
            "-------\n",
            "Train loss: 1.33511\n",
            "Epoch: 99\n",
            "-------\n",
            "Train loss: 1.33537\n",
            "Epoch: 100\n",
            "-------\n",
            "Train loss: 1.33480\n",
            "Epoch: 100 | MSE Train Loss: 1.3348037004470825 | MSE Test Loss: 1.3292949199676514 \n",
            "Epoch: 101\n",
            "-------\n",
            "Train loss: 1.33428\n",
            "Epoch: 102\n",
            "-------\n",
            "Train loss: 1.33485\n",
            "Epoch: 103\n",
            "-------\n",
            "Train loss: 1.33752\n",
            "Epoch: 104\n",
            "-------\n",
            "Train loss: 1.33457\n",
            "Epoch: 105\n",
            "-------\n",
            "Train loss: 1.33537\n",
            "Epoch: 106\n",
            "-------\n",
            "Train loss: 1.33462\n",
            "Epoch: 107\n",
            "-------\n",
            "Train loss: 1.33532\n",
            "Epoch: 108\n",
            "-------\n",
            "Train loss: 1.33439\n",
            "Epoch: 109\n",
            "-------\n",
            "Train loss: 1.33528\n",
            "Epoch: 110\n",
            "-------\n",
            "Train loss: 1.33549\n",
            "Epoch: 110 | MSE Train Loss: 1.3354905843734741 | MSE Test Loss: 1.3299899101257324 \n",
            "Epoch: 111\n",
            "-------\n",
            "Train loss: 1.33440\n",
            "Epoch: 112\n",
            "-------\n",
            "Train loss: 1.33569\n",
            "Epoch: 113\n",
            "-------\n",
            "Train loss: 1.33463\n",
            "Epoch: 114\n",
            "-------\n",
            "Train loss: 1.33474\n",
            "Epoch: 115\n",
            "-------\n",
            "Train loss: 1.33458\n",
            "Epoch: 116\n",
            "-------\n",
            "Train loss: 1.33471\n",
            "Epoch: 117\n",
            "-------\n",
            "Train loss: 1.33433\n",
            "Epoch: 118\n",
            "-------\n",
            "Train loss: 1.33735\n",
            "Epoch: 119\n",
            "-------\n",
            "Train loss: 1.33512\n",
            "Epoch: 120\n",
            "-------\n",
            "Train loss: 1.33493\n",
            "Epoch: 120 | MSE Train Loss: 1.3349252939224243 | MSE Test Loss: 1.3309968709945679 \n",
            "Epoch: 121\n",
            "-------\n",
            "Train loss: 1.33526\n",
            "Epoch: 122\n",
            "-------\n",
            "Train loss: 1.33495\n",
            "Epoch: 123\n",
            "-------\n",
            "Train loss: 1.33443\n",
            "Epoch: 124\n",
            "-------\n",
            "Train loss: 1.33569\n",
            "Epoch: 125\n",
            "-------\n",
            "Train loss: 1.33408\n",
            "Epoch: 126\n",
            "-------\n",
            "Train loss: 1.33486\n",
            "Epoch: 127\n",
            "-------\n",
            "Train loss: 1.33505\n",
            "Epoch: 128\n",
            "-------\n",
            "Train loss: 1.33492\n",
            "Epoch: 129\n",
            "-------\n",
            "Train loss: 1.33452\n",
            "Epoch: 130\n",
            "-------\n",
            "Train loss: 1.33483\n",
            "Epoch: 130 | MSE Train Loss: 1.3348263502120972 | MSE Test Loss: 1.3320316076278687 \n",
            "Epoch: 131\n",
            "-------\n",
            "Train loss: 1.33448\n",
            "Epoch: 132\n",
            "-------\n",
            "Train loss: 1.33472\n",
            "Epoch: 133\n",
            "-------\n",
            "Train loss: 1.33461\n",
            "Epoch: 134\n",
            "-------\n",
            "Train loss: 1.33435\n",
            "Epoch: 135\n",
            "-------\n",
            "Train loss: 1.33491\n",
            "Epoch: 136\n",
            "-------\n",
            "Train loss: 1.33509\n",
            "Epoch: 137\n",
            "-------\n",
            "Train loss: 1.33497\n",
            "Epoch: 138\n",
            "-------\n",
            "Train loss: 1.33598\n",
            "Epoch: 139\n",
            "-------\n",
            "Train loss: 1.33519\n",
            "Epoch: 140\n",
            "-------\n",
            "Train loss: 1.33446\n",
            "Epoch: 140 | MSE Train Loss: 1.3344634771347046 | MSE Test Loss: 1.329377293586731 \n",
            "Epoch: 141\n",
            "-------\n",
            "Train loss: 1.33448\n",
            "Epoch: 142\n",
            "-------\n",
            "Train loss: 1.33574\n",
            "Epoch: 143\n",
            "-------\n",
            "Train loss: 1.33439\n",
            "Epoch: 144\n",
            "-------\n",
            "Train loss: 1.33468\n",
            "Epoch: 145\n",
            "-------\n",
            "Train loss: 1.33528\n",
            "Epoch: 146\n",
            "-------\n",
            "Train loss: 1.33468\n",
            "Epoch: 147\n",
            "-------\n",
            "Train loss: 1.33511\n",
            "Epoch: 148\n",
            "-------\n",
            "Train loss: 1.33491\n",
            "Epoch: 149\n",
            "-------\n",
            "Train loss: 1.33455\n",
            "Epoch: 150\n",
            "-------\n",
            "Train loss: 1.33515\n",
            "Epoch: 150 | MSE Train Loss: 1.3351528644561768 | MSE Test Loss: 1.3294405937194824 \n",
            "Epoch: 151\n",
            "-------\n",
            "Train loss: 1.33580\n",
            "Epoch: 152\n",
            "-------\n",
            "Train loss: 1.33465\n",
            "Epoch: 153\n",
            "-------\n",
            "Train loss: 1.33479\n",
            "Epoch: 154\n",
            "-------\n",
            "Train loss: 1.33446\n",
            "Epoch: 155\n",
            "-------\n",
            "Train loss: 1.33490\n",
            "Epoch: 156\n",
            "-------\n",
            "Train loss: 1.33547\n",
            "Epoch: 157\n",
            "-------\n",
            "Train loss: 1.33478\n",
            "Epoch: 158\n",
            "-------\n",
            "Train loss: 1.33550\n",
            "Epoch: 159\n",
            "-------\n",
            "Train loss: 1.33575\n",
            "Epoch: 160\n",
            "-------\n",
            "Train loss: 1.33450\n",
            "Epoch: 160 | MSE Train Loss: 1.3345036506652832 | MSE Test Loss: 1.3314005136489868 \n",
            "Epoch: 161\n",
            "-------\n",
            "Train loss: 1.33453\n",
            "Epoch: 162\n",
            "-------\n",
            "Train loss: 1.33457\n",
            "Epoch: 163\n",
            "-------\n",
            "Train loss: 1.33509\n",
            "Epoch: 164\n",
            "-------\n",
            "Train loss: 1.33446\n",
            "Epoch: 165\n",
            "-------\n",
            "Train loss: 1.33445\n",
            "Epoch: 166\n",
            "-------\n",
            "Train loss: 1.33514\n",
            "Epoch: 167\n",
            "-------\n",
            "Train loss: 1.33490\n",
            "Epoch: 168\n",
            "-------\n",
            "Train loss: 1.33478\n",
            "Epoch: 169\n",
            "-------\n",
            "Train loss: 1.33491\n",
            "Epoch: 170\n",
            "-------\n",
            "Train loss: 1.33539\n",
            "Epoch: 170 | MSE Train Loss: 1.3353883028030396 | MSE Test Loss: 1.3294028043746948 \n",
            "Epoch: 171\n",
            "-------\n",
            "Train loss: 1.33493\n",
            "Epoch: 172\n",
            "-------\n",
            "Train loss: 1.33480\n",
            "Epoch: 173\n",
            "-------\n",
            "Train loss: 1.33443\n",
            "Epoch: 174\n",
            "-------\n",
            "Train loss: 1.33537\n",
            "Epoch: 175\n",
            "-------\n",
            "Train loss: 1.33455\n",
            "Epoch: 176\n",
            "-------\n",
            "Train loss: 1.33516\n",
            "Epoch: 177\n",
            "-------\n",
            "Train loss: 1.33525\n",
            "Epoch: 178\n",
            "-------\n",
            "Train loss: 1.33457\n",
            "Epoch: 179\n",
            "-------\n",
            "Train loss: 1.33520\n",
            "Epoch: 180\n",
            "-------\n",
            "Train loss: 1.33479\n",
            "Epoch: 180 | MSE Train Loss: 1.3347880840301514 | MSE Test Loss: 1.3297033309936523 \n",
            "Epoch: 181\n",
            "-------\n",
            "Train loss: 1.33509\n",
            "Epoch: 182\n",
            "-------\n",
            "Train loss: 1.33447\n",
            "Epoch: 183\n",
            "-------\n",
            "Train loss: 1.33472\n",
            "Epoch: 184\n",
            "-------\n",
            "Train loss: 1.33485\n",
            "Epoch: 185\n",
            "-------\n",
            "Train loss: 1.33447\n",
            "Epoch: 186\n",
            "-------\n",
            "Train loss: 1.33516\n",
            "Epoch: 187\n",
            "-------\n",
            "Train loss: 1.33449\n",
            "Epoch: 188\n",
            "-------\n",
            "Train loss: 1.33502\n",
            "Epoch: 189\n",
            "-------\n",
            "Train loss: 1.33517\n",
            "Epoch: 190\n",
            "-------\n",
            "Train loss: 1.33442\n",
            "Epoch: 190 | MSE Train Loss: 1.3344157934188843 | MSE Test Loss: 1.3290818929672241 \n",
            "Epoch: 191\n",
            "-------\n",
            "Train loss: 1.33467\n",
            "Epoch: 192\n",
            "-------\n",
            "Train loss: 1.33517\n",
            "Epoch: 193\n",
            "-------\n",
            "Train loss: 1.33607\n",
            "Epoch: 194\n",
            "-------\n",
            "Train loss: 1.33542\n",
            "Epoch: 195\n",
            "-------\n",
            "Train loss: 1.33479\n",
            "Epoch: 196\n",
            "-------\n",
            "Train loss: 1.33524\n",
            "Epoch: 197\n",
            "-------\n",
            "Train loss: 1.33443\n",
            "Epoch: 198\n",
            "-------\n",
            "Train loss: 1.33502\n",
            "Epoch: 199\n",
            "-------\n",
            "Train loss: 1.33522\n",
            "Epoch: 200\n",
            "-------\n",
            "Train loss: 1.33409\n",
            "Epoch: 200 | MSE Train Loss: 1.334085464477539 | MSE Test Loss: 1.3296676874160767 \n",
            "Epoch: 201\n",
            "-------\n",
            "Train loss: 1.33538\n",
            "Epoch: 202\n",
            "-------\n",
            "Train loss: 1.33455\n",
            "Epoch: 203\n",
            "-------\n",
            "Train loss: 1.33516\n",
            "Epoch: 204\n",
            "-------\n",
            "Train loss: 1.33497\n",
            "Epoch: 205\n",
            "-------\n",
            "Train loss: 1.33509\n",
            "Epoch: 206\n",
            "-------\n",
            "Train loss: 1.33437\n",
            "Epoch: 207\n",
            "-------\n",
            "Train loss: 1.33446\n",
            "Epoch: 208\n",
            "-------\n",
            "Train loss: 1.33493\n",
            "Epoch: 209\n",
            "-------\n",
            "Train loss: 1.33536\n",
            "Epoch: 210\n",
            "-------\n",
            "Train loss: 1.33481\n",
            "Epoch: 210 | MSE Train Loss: 1.3348108530044556 | MSE Test Loss: 1.3305997848510742 \n",
            "Epoch: 211\n",
            "-------\n",
            "Train loss: 1.33442\n",
            "Epoch: 212\n",
            "-------\n",
            "Train loss: 1.33511\n",
            "Epoch: 213\n",
            "-------\n",
            "Train loss: 1.33429\n",
            "Epoch: 214\n",
            "-------\n",
            "Train loss: 1.33527\n",
            "Epoch: 215\n",
            "-------\n",
            "Train loss: 1.33421\n",
            "Epoch: 216\n",
            "-------\n",
            "Train loss: 1.33486\n",
            "Epoch: 217\n",
            "-------\n",
            "Train loss: 1.33475\n",
            "Epoch: 218\n",
            "-------\n",
            "Train loss: 1.33516\n",
            "Epoch: 219\n",
            "-------\n",
            "Train loss: 1.33520\n",
            "Epoch: 220\n",
            "-------\n",
            "Train loss: 1.33695\n",
            "Epoch: 220 | MSE Train Loss: 1.3369548320770264 | MSE Test Loss: 1.3288381099700928 \n",
            "Epoch: 221\n",
            "-------\n",
            "Train loss: 1.33497\n",
            "Epoch: 222\n",
            "-------\n",
            "Train loss: 1.33516\n",
            "Epoch: 223\n",
            "-------\n",
            "Train loss: 1.33466\n",
            "Epoch: 224\n",
            "-------\n",
            "Train loss: 1.33504\n",
            "Epoch: 225\n",
            "-------\n",
            "Train loss: 1.33497\n",
            "Epoch: 226\n",
            "-------\n",
            "Train loss: 1.33560\n",
            "Epoch: 227\n",
            "-------\n",
            "Train loss: 1.33501\n",
            "Epoch: 228\n",
            "-------\n",
            "Train loss: 1.33443\n",
            "Epoch: 229\n",
            "-------\n",
            "Train loss: 1.33455\n",
            "Epoch: 230\n",
            "-------\n",
            "Train loss: 1.33430\n",
            "Epoch: 230 | MSE Train Loss: 1.3342992067337036 | MSE Test Loss: 1.3290261030197144 \n",
            "Epoch: 231\n",
            "-------\n",
            "Train loss: 1.33512\n",
            "Epoch: 232\n",
            "-------\n",
            "Train loss: 1.33437\n",
            "Epoch: 233\n",
            "-------\n",
            "Train loss: 1.33491\n",
            "Epoch: 234\n",
            "-------\n",
            "Train loss: 1.33471\n",
            "Epoch: 235\n",
            "-------\n",
            "Train loss: 1.33522\n",
            "Epoch: 236\n",
            "-------\n",
            "Train loss: 1.33483\n",
            "Epoch: 237\n",
            "-------\n",
            "Train loss: 1.33456\n",
            "Epoch: 238\n",
            "-------\n",
            "Train loss: 1.33508\n",
            "Epoch: 239\n",
            "-------\n",
            "Train loss: 1.33465\n",
            "Epoch: 240\n",
            "-------\n",
            "Train loss: 1.33464\n",
            "Epoch: 240 | MSE Train Loss: 1.3346412181854248 | MSE Test Loss: 1.3289132118225098 \n",
            "Epoch: 241\n",
            "-------\n",
            "Train loss: 1.33479\n",
            "Epoch: 242\n",
            "-------\n",
            "Train loss: 1.33472\n",
            "Epoch: 243\n",
            "-------\n",
            "Train loss: 1.33515\n",
            "Epoch: 244\n",
            "-------\n",
            "Train loss: 1.33460\n",
            "Epoch: 245\n",
            "-------\n",
            "Train loss: 1.33474\n",
            "Epoch: 246\n",
            "-------\n",
            "Train loss: 1.33425\n",
            "Epoch: 247\n",
            "-------\n",
            "Train loss: 1.33651\n",
            "Epoch: 248\n",
            "-------\n",
            "Train loss: 1.33486\n",
            "Epoch: 249\n",
            "-------\n",
            "Train loss: 1.33477\n",
            "Epoch: 250\n",
            "-------\n",
            "Train loss: 1.33404\n",
            "Epoch: 250 | MSE Train Loss: 1.3340407609939575 | MSE Test Loss: 1.3323326110839844 \n",
            "Epoch: 251\n",
            "-------\n",
            "Train loss: 1.33456\n",
            "Epoch: 252\n",
            "-------\n",
            "Train loss: 1.33472\n",
            "Epoch: 253\n",
            "-------\n",
            "Train loss: 1.33447\n",
            "Epoch: 254\n",
            "-------\n",
            "Train loss: 1.33468\n",
            "Epoch: 255\n",
            "-------\n",
            "Train loss: 1.33529\n",
            "Epoch: 256\n",
            "-------\n",
            "Train loss: 1.33484\n",
            "Epoch: 257\n",
            "-------\n",
            "Train loss: 1.33458\n",
            "Epoch: 258\n",
            "-------\n",
            "Train loss: 1.33501\n",
            "Epoch: 259\n",
            "-------\n",
            "Train loss: 1.33556\n",
            "Epoch: 260\n",
            "-------\n",
            "Train loss: 1.33454\n",
            "Epoch: 260 | MSE Train Loss: 1.3345377445220947 | MSE Test Loss: 1.329180359840393 \n",
            "Epoch: 261\n",
            "-------\n",
            "Train loss: 1.33428\n",
            "Epoch: 262\n",
            "-------\n",
            "Train loss: 1.33493\n",
            "Epoch: 263\n",
            "-------\n",
            "Train loss: 1.33454\n",
            "Epoch: 264\n",
            "-------\n",
            "Train loss: 1.33478\n",
            "Epoch: 265\n",
            "-------\n",
            "Train loss: 1.33490\n",
            "Epoch: 266\n",
            "-------\n",
            "Train loss: 1.33421\n",
            "Epoch: 267\n",
            "-------\n",
            "Train loss: 1.33472\n",
            "Epoch: 268\n",
            "-------\n",
            "Train loss: 1.33490\n",
            "Epoch: 269\n",
            "-------\n",
            "Train loss: 1.33544\n",
            "Epoch: 270\n",
            "-------\n",
            "Train loss: 1.33474\n",
            "Epoch: 270 | MSE Train Loss: 1.3347407579421997 | MSE Test Loss: 1.3294764757156372 \n",
            "Epoch: 271\n",
            "-------\n",
            "Train loss: 1.33438\n",
            "Epoch: 272\n",
            "-------\n",
            "Train loss: 1.33446\n",
            "Epoch: 273\n",
            "-------\n",
            "Train loss: 1.33633\n",
            "Epoch: 274\n",
            "-------\n",
            "Train loss: 1.33463\n",
            "Epoch: 275\n",
            "-------\n",
            "Train loss: 1.33482\n",
            "Epoch: 276\n",
            "-------\n",
            "Train loss: 1.33468\n",
            "Epoch: 277\n",
            "-------\n",
            "Train loss: 1.33441\n",
            "Epoch: 278\n",
            "-------\n",
            "Train loss: 1.33539\n",
            "Epoch: 279\n",
            "-------\n",
            "Train loss: 1.33495\n",
            "Epoch: 280\n",
            "-------\n",
            "Train loss: 1.33494\n",
            "Epoch: 280 | MSE Train Loss: 1.334943175315857 | MSE Test Loss: 1.3293763399124146 \n",
            "Epoch: 281\n",
            "-------\n",
            "Train loss: 1.33538\n",
            "Epoch: 282\n",
            "-------\n",
            "Train loss: 1.33442\n",
            "Epoch: 283\n",
            "-------\n",
            "Train loss: 1.33433\n",
            "Epoch: 284\n",
            "-------\n",
            "Train loss: 1.33476\n",
            "Epoch: 285\n",
            "-------\n",
            "Train loss: 1.33513\n",
            "Epoch: 286\n",
            "-------\n",
            "Train loss: 1.33545\n",
            "Epoch: 287\n",
            "-------\n",
            "Train loss: 1.33553\n",
            "Epoch: 288\n",
            "-------\n",
            "Train loss: 1.33537\n",
            "Epoch: 289\n",
            "-------\n",
            "Train loss: 1.33453\n",
            "Epoch: 290\n",
            "-------\n",
            "Train loss: 1.33493\n",
            "Epoch: 290 | MSE Train Loss: 1.3349266052246094 | MSE Test Loss: 1.3293513059616089 \n",
            "Epoch: 291\n",
            "-------\n",
            "Train loss: 1.33472\n",
            "Epoch: 292\n",
            "-------\n",
            "Train loss: 1.33456\n",
            "Epoch: 293\n",
            "-------\n",
            "Train loss: 1.33439\n",
            "Epoch: 294\n",
            "-------\n",
            "Train loss: 1.33425\n",
            "Epoch: 295\n",
            "-------\n",
            "Train loss: 1.33486\n",
            "Epoch: 296\n",
            "-------\n",
            "Train loss: 1.33482\n",
            "Epoch: 297\n",
            "-------\n",
            "Train loss: 1.33445\n",
            "Epoch: 298\n",
            "-------\n",
            "Train loss: 1.33486\n",
            "Epoch: 299\n",
            "-------\n",
            "Train loss: 1.33451\n",
            "Epoch: 300\n",
            "-------\n",
            "Train loss: 1.33488\n",
            "Epoch: 300 | MSE Train Loss: 1.334882140159607 | MSE Test Loss: 1.3310799598693848 \n",
            "Epoch: 301\n",
            "-------\n",
            "Train loss: 1.33540\n",
            "Epoch: 302\n",
            "-------\n",
            "Train loss: 1.33412\n",
            "Epoch: 303\n",
            "-------\n",
            "Train loss: 1.33478\n",
            "Epoch: 304\n",
            "-------\n",
            "Train loss: 1.33438\n",
            "Epoch: 305\n",
            "-------\n",
            "Train loss: 1.33338\n",
            "Epoch: 306\n",
            "-------\n",
            "Train loss: 1.33505\n",
            "Epoch: 307\n",
            "-------\n",
            "Train loss: 1.33570\n",
            "Epoch: 308\n",
            "-------\n",
            "Train loss: 1.33492\n",
            "Epoch: 309\n",
            "-------\n",
            "Train loss: 1.33401\n",
            "Epoch: 310\n",
            "-------\n",
            "Train loss: 1.33450\n",
            "Epoch: 310 | MSE Train Loss: 1.334496021270752 | MSE Test Loss: 1.3293536901474 \n",
            "Epoch: 311\n",
            "-------\n",
            "Train loss: 1.33442\n",
            "Epoch: 312\n",
            "-------\n",
            "Train loss: 1.33483\n",
            "Epoch: 313\n",
            "-------\n",
            "Train loss: 1.33422\n",
            "Epoch: 314\n",
            "-------\n",
            "Train loss: 1.33434\n",
            "Epoch: 315\n",
            "-------\n",
            "Train loss: 1.33472\n",
            "Epoch: 316\n",
            "-------\n",
            "Train loss: 1.33499\n",
            "Epoch: 317\n",
            "-------\n",
            "Train loss: 1.33483\n",
            "Epoch: 318\n",
            "-------\n",
            "Train loss: 1.33471\n",
            "Epoch: 319\n",
            "-------\n",
            "Train loss: 1.33452\n",
            "Epoch: 320\n",
            "-------\n",
            "Train loss: 1.33480\n",
            "Epoch: 320 | MSE Train Loss: 1.334801435470581 | MSE Test Loss: 1.3287467956542969 \n",
            "Epoch: 321\n",
            "-------\n",
            "Train loss: 1.33529\n",
            "Epoch: 322\n",
            "-------\n",
            "Train loss: 1.33423\n",
            "Epoch: 323\n",
            "-------\n",
            "Train loss: 1.33451\n",
            "Epoch: 324\n",
            "-------\n",
            "Train loss: 1.33428\n",
            "Epoch: 325\n",
            "-------\n",
            "Train loss: 1.33468\n",
            "Epoch: 326\n",
            "-------\n",
            "Train loss: 1.33593\n",
            "Epoch: 327\n",
            "-------\n",
            "Train loss: 1.33427\n",
            "Epoch: 328\n",
            "-------\n",
            "Train loss: 1.33471\n",
            "Epoch: 329\n",
            "-------\n",
            "Train loss: 1.33448\n",
            "Epoch: 330\n",
            "-------\n",
            "Train loss: 1.33465\n",
            "Epoch: 330 | MSE Train Loss: 1.334653377532959 | MSE Test Loss: 1.3293124437332153 \n",
            "Epoch: 331\n",
            "-------\n",
            "Train loss: 1.33421\n",
            "Epoch: 332\n",
            "-------\n",
            "Train loss: 1.33412\n",
            "Epoch: 333\n",
            "-------\n",
            "Train loss: 1.33487\n",
            "Epoch: 334\n",
            "-------\n",
            "Train loss: 1.33534\n",
            "Epoch: 335\n",
            "-------\n",
            "Train loss: 1.33470\n",
            "Epoch: 336\n",
            "-------\n",
            "Train loss: 1.33431\n",
            "Epoch: 337\n",
            "-------\n",
            "Train loss: 1.33458\n",
            "Epoch: 338\n",
            "-------\n",
            "Train loss: 1.33401\n",
            "Epoch: 339\n",
            "-------\n",
            "Train loss: 1.33463\n",
            "Epoch: 340\n",
            "-------\n",
            "Train loss: 1.33491\n",
            "Epoch: 340 | MSE Train Loss: 1.33490788936615 | MSE Test Loss: 1.3301877975463867 \n",
            "Epoch: 341\n",
            "-------\n",
            "Train loss: 1.33437\n",
            "Epoch: 342\n",
            "-------\n",
            "Train loss: 1.33449\n",
            "Epoch: 343\n",
            "-------\n",
            "Train loss: 1.33489\n",
            "Epoch: 344\n",
            "-------\n",
            "Train loss: 1.33449\n",
            "Epoch: 345\n",
            "-------\n",
            "Train loss: 1.33644\n",
            "Epoch: 346\n",
            "-------\n",
            "Train loss: 1.33478\n",
            "Epoch: 347\n",
            "-------\n",
            "Train loss: 1.33453\n",
            "Epoch: 348\n",
            "-------\n",
            "Train loss: 1.33484\n",
            "Epoch: 349\n",
            "-------\n",
            "Train loss: 1.33438\n",
            "Epoch: 350\n",
            "-------\n",
            "Train loss: 1.33455\n",
            "Epoch: 350 | MSE Train Loss: 1.334548830986023 | MSE Test Loss: 1.3305857181549072 \n",
            "Epoch: 351\n",
            "-------\n",
            "Train loss: 1.33469\n",
            "Epoch: 352\n",
            "-------\n",
            "Train loss: 1.33423\n",
            "Epoch: 353\n",
            "-------\n",
            "Train loss: 1.33476\n",
            "Epoch: 354\n",
            "-------\n",
            "Train loss: 1.33431\n",
            "Epoch: 355\n",
            "-------\n",
            "Train loss: 1.33478\n",
            "Epoch: 356\n",
            "-------\n",
            "Train loss: 1.33464\n",
            "Epoch: 357\n",
            "-------\n",
            "Train loss: 1.33453\n",
            "Epoch: 358\n",
            "-------\n",
            "Train loss: 1.33487\n",
            "Epoch: 359\n",
            "-------\n",
            "Train loss: 1.33439\n",
            "Epoch: 360\n",
            "-------\n",
            "Train loss: 1.33521\n",
            "Epoch: 360 | MSE Train Loss: 1.3352123498916626 | MSE Test Loss: 1.3290103673934937 \n",
            "Epoch: 361\n",
            "-------\n",
            "Train loss: 1.33503\n",
            "Epoch: 362\n",
            "-------\n",
            "Train loss: 1.33458\n",
            "Epoch: 363\n",
            "-------\n",
            "Train loss: 1.33402\n",
            "Epoch: 364\n",
            "-------\n",
            "Train loss: 1.33488\n",
            "Epoch: 365\n",
            "-------\n",
            "Train loss: 1.33555\n",
            "Epoch: 366\n",
            "-------\n",
            "Train loss: 1.33432\n",
            "Epoch: 367\n",
            "-------\n",
            "Train loss: 1.33472\n",
            "Epoch: 368\n",
            "-------\n",
            "Train loss: 1.33610\n",
            "Epoch: 369\n",
            "-------\n",
            "Train loss: 1.33468\n",
            "Epoch: 370\n",
            "-------\n",
            "Train loss: 1.33504\n",
            "Epoch: 370 | MSE Train Loss: 1.3350365161895752 | MSE Test Loss: 1.3290612697601318 \n",
            "Epoch: 371\n",
            "-------\n",
            "Train loss: 1.33476\n",
            "Epoch: 372\n",
            "-------\n",
            "Train loss: 1.33480\n",
            "Epoch: 373\n",
            "-------\n",
            "Train loss: 1.33438\n",
            "Epoch: 374\n",
            "-------\n",
            "Train loss: 1.33469\n",
            "Epoch: 375\n",
            "-------\n",
            "Train loss: 1.33462\n",
            "Epoch: 376\n",
            "-------\n",
            "Train loss: 1.33496\n",
            "Epoch: 377\n",
            "-------\n",
            "Train loss: 1.33508\n",
            "Epoch: 378\n",
            "-------\n",
            "Train loss: 1.33468\n",
            "Epoch: 379\n",
            "-------\n",
            "Train loss: 1.33496\n",
            "Epoch: 380\n",
            "-------\n",
            "Train loss: 1.33499\n",
            "Epoch: 380 | MSE Train Loss: 1.3349868059158325 | MSE Test Loss: 1.328934669494629 \n",
            "Epoch: 381\n",
            "-------\n",
            "Train loss: 1.33496\n",
            "Epoch: 382\n",
            "-------\n",
            "Train loss: 1.33454\n",
            "Epoch: 383\n",
            "-------\n",
            "Train loss: 1.33454\n",
            "Epoch: 384\n",
            "-------\n",
            "Train loss: 1.33454\n",
            "Epoch: 385\n",
            "-------\n",
            "Train loss: 1.33653\n",
            "Epoch: 386\n",
            "-------\n",
            "Train loss: 1.33440\n",
            "Epoch: 387\n",
            "-------\n",
            "Train loss: 1.33428\n",
            "Epoch: 388\n",
            "-------\n",
            "Train loss: 1.33478\n",
            "Epoch: 389\n",
            "-------\n",
            "Train loss: 1.33447\n",
            "Epoch: 390\n",
            "-------\n",
            "Train loss: 1.33499\n",
            "Epoch: 390 | MSE Train Loss: 1.3349932432174683 | MSE Test Loss: 1.328979253768921 \n",
            "Epoch: 391\n",
            "-------\n",
            "Train loss: 1.33480\n",
            "Epoch: 392\n",
            "-------\n",
            "Train loss: 1.33478\n",
            "Epoch: 393\n",
            "-------\n",
            "Train loss: 1.33440\n",
            "Epoch: 394\n",
            "-------\n",
            "Train loss: 1.33446\n",
            "Epoch: 395\n",
            "-------\n",
            "Train loss: 1.33493\n",
            "Epoch: 396\n",
            "-------\n",
            "Train loss: 1.33441\n",
            "Epoch: 397\n",
            "-------\n",
            "Train loss: 1.33509\n",
            "Epoch: 398\n",
            "-------\n",
            "Train loss: 1.33486\n",
            "Epoch: 399\n",
            "-------\n",
            "Train loss: 1.33476\n",
            "Epoch: 400\n",
            "-------\n",
            "Train loss: 1.33573\n",
            "Epoch: 400 | MSE Train Loss: 1.3357280492782593 | MSE Test Loss: 1.3298654556274414 \n",
            "Epoch: 401\n",
            "-------\n",
            "Train loss: 1.33506\n",
            "Epoch: 402\n",
            "-------\n",
            "Train loss: 1.33515\n",
            "Epoch: 403\n",
            "-------\n",
            "Train loss: 1.33464\n",
            "Epoch: 404\n",
            "-------\n",
            "Train loss: 1.33392\n",
            "Epoch: 405\n",
            "-------\n",
            "Train loss: 1.33462\n",
            "Epoch: 406\n",
            "-------\n",
            "Train loss: 1.33481\n",
            "Epoch: 407\n",
            "-------\n",
            "Train loss: 1.33467\n",
            "Epoch: 408\n",
            "-------\n",
            "Train loss: 1.33405\n",
            "Epoch: 409\n",
            "-------\n",
            "Train loss: 1.33464\n",
            "Epoch: 410\n",
            "-------\n",
            "Train loss: 1.33512\n",
            "Epoch: 410 | MSE Train Loss: 1.3351212739944458 | MSE Test Loss: 1.329071044921875 \n",
            "Epoch: 411\n",
            "-------\n",
            "Train loss: 1.33415\n",
            "Epoch: 412\n",
            "-------\n",
            "Train loss: 1.33442\n",
            "Epoch: 413\n",
            "-------\n",
            "Train loss: 1.33484\n",
            "Epoch: 414\n",
            "-------\n",
            "Train loss: 1.33496\n",
            "Epoch: 415\n",
            "-------\n",
            "Train loss: 1.33587\n",
            "Epoch: 416\n",
            "-------\n",
            "Train loss: 1.33499\n",
            "Epoch: 417\n",
            "-------\n",
            "Train loss: 1.33418\n",
            "Epoch: 418\n",
            "-------\n",
            "Train loss: 1.33445\n",
            "Epoch: 419\n",
            "-------\n",
            "Train loss: 1.33435\n",
            "Epoch: 420\n",
            "-------\n",
            "Train loss: 1.33438\n",
            "Epoch: 420 | MSE Train Loss: 1.3343790769577026 | MSE Test Loss: 1.3299684524536133 \n",
            "Epoch: 421\n",
            "-------\n",
            "Train loss: 1.33425\n",
            "Epoch: 422\n",
            "-------\n",
            "Train loss: 1.33457\n",
            "Epoch: 423\n",
            "-------\n",
            "Train loss: 1.33556\n",
            "Epoch: 424\n",
            "-------\n",
            "Train loss: 1.33497\n",
            "Epoch: 425\n",
            "-------\n",
            "Train loss: 1.33475\n",
            "Epoch: 426\n",
            "-------\n",
            "Train loss: 1.33437\n",
            "Epoch: 427\n",
            "-------\n",
            "Train loss: 1.33458\n",
            "Epoch: 428\n",
            "-------\n",
            "Train loss: 1.33488\n",
            "Epoch: 429\n",
            "-------\n",
            "Train loss: 1.33531\n",
            "Epoch: 430\n",
            "-------\n",
            "Train loss: 1.33487\n",
            "Epoch: 430 | MSE Train Loss: 1.3348666429519653 | MSE Test Loss: 1.3300002813339233 \n",
            "Epoch: 431\n",
            "-------\n",
            "Train loss: 1.33499\n",
            "Epoch: 432\n",
            "-------\n",
            "Train loss: 1.33531\n",
            "Epoch: 433\n",
            "-------\n",
            "Train loss: 1.33509\n",
            "Epoch: 434\n",
            "-------\n",
            "Train loss: 1.33466\n",
            "Epoch: 435\n",
            "-------\n",
            "Train loss: 1.33363\n",
            "Epoch: 436\n",
            "-------\n",
            "Train loss: 1.33471\n",
            "Epoch: 437\n",
            "-------\n",
            "Train loss: 1.33458\n",
            "Epoch: 438\n",
            "-------\n",
            "Train loss: 1.33428\n",
            "Epoch: 439\n",
            "-------\n",
            "Train loss: 1.33513\n",
            "Epoch: 440\n",
            "-------\n",
            "Train loss: 1.33437\n",
            "Epoch: 440 | MSE Train Loss: 1.3343671560287476 | MSE Test Loss: 1.3291349411010742 \n",
            "Epoch: 441\n",
            "-------\n",
            "Train loss: 1.33466\n",
            "Epoch: 442\n",
            "-------\n",
            "Train loss: 1.33400\n",
            "Epoch: 443\n",
            "-------\n",
            "Train loss: 1.33386\n",
            "Epoch: 444\n",
            "-------\n",
            "Train loss: 1.33568\n",
            "Epoch: 445\n",
            "-------\n",
            "Train loss: 1.33472\n",
            "Epoch: 446\n",
            "-------\n",
            "Train loss: 1.33467\n",
            "Epoch: 447\n",
            "-------\n",
            "Train loss: 1.33447\n",
            "Epoch: 448\n",
            "-------\n",
            "Train loss: 1.33441\n",
            "Epoch: 449\n",
            "-------\n",
            "Train loss: 1.33435\n",
            "Epoch: 450\n",
            "-------\n",
            "Train loss: 1.33502\n",
            "Epoch: 450 | MSE Train Loss: 1.3350154161453247 | MSE Test Loss: 1.3293315172195435 \n",
            "Epoch: 451\n",
            "-------\n",
            "Train loss: 1.33415\n",
            "Epoch: 452\n",
            "-------\n",
            "Train loss: 1.33502\n",
            "Epoch: 453\n",
            "-------\n",
            "Train loss: 1.33478\n",
            "Epoch: 454\n",
            "-------\n",
            "Train loss: 1.33472\n",
            "Epoch: 455\n",
            "-------\n",
            "Train loss: 1.33421\n",
            "Epoch: 456\n",
            "-------\n",
            "Train loss: 1.33524\n",
            "Epoch: 457\n",
            "-------\n",
            "Train loss: 1.33451\n",
            "Epoch: 458\n",
            "-------\n",
            "Train loss: 1.33495\n",
            "Epoch: 459\n",
            "-------\n",
            "Train loss: 1.33466\n",
            "Epoch: 460\n",
            "-------\n",
            "Train loss: 1.33452\n",
            "Epoch: 460 | MSE Train Loss: 1.334517240524292 | MSE Test Loss: 1.3291606903076172 \n",
            "Epoch: 461\n",
            "-------\n",
            "Train loss: 1.33417\n",
            "Epoch: 462\n",
            "-------\n",
            "Train loss: 1.33461\n",
            "Epoch: 463\n",
            "-------\n",
            "Train loss: 1.33443\n",
            "Epoch: 464\n",
            "-------\n",
            "Train loss: 1.33485\n",
            "Epoch: 465\n",
            "-------\n",
            "Train loss: 1.33432\n",
            "Epoch: 466\n",
            "-------\n",
            "Train loss: 1.33505\n",
            "Epoch: 467\n",
            "-------\n",
            "Train loss: 1.33480\n",
            "Epoch: 468\n",
            "-------\n",
            "Train loss: 1.33486\n",
            "Epoch: 469\n",
            "-------\n",
            "Train loss: 1.33416\n",
            "Epoch: 470\n",
            "-------\n",
            "Train loss: 1.33523\n",
            "Epoch: 470 | MSE Train Loss: 1.3352280855178833 | MSE Test Loss: 1.3290178775787354 \n",
            "Epoch: 471\n",
            "-------\n",
            "Train loss: 1.33434\n",
            "Epoch: 472\n",
            "-------\n",
            "Train loss: 1.33407\n",
            "Epoch: 473\n",
            "-------\n",
            "Train loss: 1.33486\n",
            "Epoch: 474\n",
            "-------\n",
            "Train loss: 1.33368\n",
            "Epoch: 475\n",
            "-------\n",
            "Train loss: 1.33475\n",
            "Epoch: 476\n",
            "-------\n",
            "Train loss: 1.33403\n",
            "Epoch: 477\n",
            "-------\n",
            "Train loss: 1.33470\n",
            "Epoch: 478\n",
            "-------\n",
            "Train loss: 1.33450\n",
            "Epoch: 479\n",
            "-------\n",
            "Train loss: 1.33494\n",
            "Epoch: 480\n",
            "-------\n",
            "Train loss: 1.33383\n",
            "Epoch: 480 | MSE Train Loss: 1.333827257156372 | MSE Test Loss: 1.3288209438323975 \n",
            "Epoch: 481\n",
            "-------\n",
            "Train loss: 1.33506\n",
            "Epoch: 482\n",
            "-------\n",
            "Train loss: 1.33470\n",
            "Epoch: 483\n",
            "-------\n",
            "Train loss: 1.33471\n",
            "Epoch: 484\n",
            "-------\n",
            "Train loss: 1.33466\n",
            "Epoch: 485\n",
            "-------\n",
            "Train loss: 1.33438\n",
            "Epoch: 486\n",
            "-------\n",
            "Train loss: 1.33443\n",
            "Epoch: 487\n",
            "-------\n",
            "Train loss: 1.33505\n",
            "Epoch: 488\n",
            "-------\n",
            "Train loss: 1.33444\n",
            "Epoch: 489\n",
            "-------\n",
            "Train loss: 1.33360\n",
            "Epoch: 490\n",
            "-------\n",
            "Train loss: 1.33404\n",
            "Epoch: 490 | MSE Train Loss: 1.3340351581573486 | MSE Test Loss: 1.329483985900879 \n",
            "Epoch: 491\n",
            "-------\n",
            "Train loss: 1.33407\n",
            "Epoch: 492\n",
            "-------\n",
            "Train loss: 1.33412\n",
            "Epoch: 493\n",
            "-------\n",
            "Train loss: 1.33473\n",
            "Epoch: 494\n",
            "-------\n",
            "Train loss: 1.33412\n",
            "Epoch: 495\n",
            "-------\n",
            "Train loss: 1.33382\n",
            "Epoch: 496\n",
            "-------\n",
            "Train loss: 1.33407\n",
            "Epoch: 497\n",
            "-------\n",
            "Train loss: 1.33465\n",
            "Epoch: 498\n",
            "-------\n",
            "Train loss: 1.33424\n",
            "Epoch: 499\n",
            "-------\n",
            "Train loss: 1.33481\n",
            "Epoch: 500\n",
            "-------\n",
            "Train loss: 1.33409\n",
            "Epoch: 500 | MSE Train Loss: 1.3340893983840942 | MSE Test Loss: 1.3300209045410156 \n",
            "Epoch: 501\n",
            "-------\n",
            "Train loss: 1.33454\n",
            "Epoch: 502\n",
            "-------\n",
            "Train loss: 1.33412\n",
            "Epoch: 503\n",
            "-------\n",
            "Train loss: 1.33452\n",
            "Epoch: 504\n",
            "-------\n",
            "Train loss: 1.33416\n",
            "Epoch: 505\n",
            "-------\n",
            "Train loss: 1.33422\n",
            "Epoch: 506\n",
            "-------\n",
            "Train loss: 1.33550\n",
            "Epoch: 507\n",
            "-------\n",
            "Train loss: 1.33435\n",
            "Epoch: 508\n",
            "-------\n",
            "Train loss: 1.33439\n",
            "Epoch: 509\n",
            "-------\n",
            "Train loss: 1.33466\n",
            "Epoch: 510\n",
            "-------\n",
            "Train loss: 1.33443\n",
            "Epoch: 510 | MSE Train Loss: 1.334433913230896 | MSE Test Loss: 1.329363465309143 \n",
            "Epoch: 511\n",
            "-------\n",
            "Train loss: 1.33464\n",
            "Epoch: 512\n",
            "-------\n",
            "Train loss: 1.33417\n",
            "Epoch: 513\n",
            "-------\n",
            "Train loss: 1.33504\n",
            "Epoch: 514\n",
            "-------\n",
            "Train loss: 1.33485\n",
            "Epoch: 515\n",
            "-------\n",
            "Train loss: 1.33450\n",
            "Epoch: 516\n",
            "-------\n",
            "Train loss: 1.33414\n",
            "Epoch: 517\n",
            "-------\n",
            "Train loss: 1.33424\n",
            "Epoch: 518\n",
            "-------\n",
            "Train loss: 1.33530\n",
            "Epoch: 519\n",
            "-------\n",
            "Train loss: 1.33464\n",
            "Epoch: 520\n",
            "-------\n",
            "Train loss: 1.33474\n",
            "Epoch: 520 | MSE Train Loss: 1.3347351551055908 | MSE Test Loss: 1.3290938138961792 \n",
            "Epoch: 521\n",
            "-------\n",
            "Train loss: 1.33445\n",
            "Epoch: 522\n",
            "-------\n",
            "Train loss: 1.33509\n",
            "Epoch: 523\n",
            "-------\n",
            "Train loss: 1.33492\n",
            "Epoch: 524\n",
            "-------\n",
            "Train loss: 1.33482\n",
            "Epoch: 525\n",
            "-------\n",
            "Train loss: 1.33445\n",
            "Epoch: 526\n",
            "-------\n",
            "Train loss: 1.33391\n",
            "Epoch: 527\n",
            "-------\n",
            "Train loss: 1.33400\n",
            "Epoch: 528\n",
            "-------\n",
            "Train loss: 1.33455\n",
            "Epoch: 529\n",
            "-------\n",
            "Train loss: 1.33534\n",
            "Epoch: 530\n",
            "-------\n",
            "Train loss: 1.33537\n",
            "Epoch: 530 | MSE Train Loss: 1.3353710174560547 | MSE Test Loss: 1.328791618347168 \n",
            "Epoch: 531\n",
            "-------\n",
            "Train loss: 1.33403\n",
            "Epoch: 532\n",
            "-------\n",
            "Train loss: 1.33428\n",
            "Epoch: 533\n",
            "-------\n",
            "Train loss: 1.33458\n",
            "Epoch: 534\n",
            "-------\n",
            "Train loss: 1.33431\n",
            "Epoch: 535\n",
            "-------\n",
            "Train loss: 1.33403\n",
            "Epoch: 536\n",
            "-------\n",
            "Train loss: 1.33509\n",
            "Epoch: 537\n",
            "-------\n",
            "Train loss: 1.33479\n",
            "Epoch: 538\n",
            "-------\n",
            "Train loss: 1.33506\n",
            "Epoch: 539\n",
            "-------\n",
            "Train loss: 1.33508\n",
            "Epoch: 540\n",
            "-------\n",
            "Train loss: 1.33430\n",
            "Epoch: 540 | MSE Train Loss: 1.3343042135238647 | MSE Test Loss: 1.329625129699707 \n",
            "Epoch: 541\n",
            "-------\n",
            "Train loss: 1.33481\n",
            "Epoch: 542\n",
            "-------\n",
            "Train loss: 1.33562\n",
            "Epoch: 543\n",
            "-------\n",
            "Train loss: 1.33483\n",
            "Epoch: 544\n",
            "-------\n",
            "Train loss: 1.33443\n",
            "Epoch: 545\n",
            "-------\n",
            "Train loss: 1.33394\n",
            "Epoch: 546\n",
            "-------\n",
            "Train loss: 1.33482\n",
            "Epoch: 547\n",
            "-------\n",
            "Train loss: 1.33392\n",
            "Epoch: 548\n",
            "-------\n",
            "Train loss: 1.33435\n",
            "Epoch: 549\n",
            "-------\n",
            "Train loss: 1.33516\n",
            "Epoch: 550\n",
            "-------\n",
            "Train loss: 1.33472\n",
            "Epoch: 550 | MSE Train Loss: 1.334718942642212 | MSE Test Loss: 1.3291302919387817 \n",
            "Epoch: 551\n",
            "-------\n",
            "Train loss: 1.33443\n",
            "Epoch: 552\n",
            "-------\n",
            "Train loss: 1.33423\n",
            "Epoch: 553\n",
            "-------\n",
            "Train loss: 1.33518\n",
            "Epoch: 554\n",
            "-------\n",
            "Train loss: 1.33433\n",
            "Epoch: 555\n",
            "-------\n",
            "Train loss: 1.33438\n",
            "Epoch: 556\n",
            "-------\n",
            "Train loss: 1.33468\n",
            "Epoch: 557\n",
            "-------\n",
            "Train loss: 1.33368\n",
            "Epoch: 558\n",
            "-------\n",
            "Train loss: 1.33406\n",
            "Epoch: 559\n",
            "-------\n",
            "Train loss: 1.33482\n",
            "Epoch: 560\n",
            "-------\n",
            "Train loss: 1.33420\n",
            "Epoch: 560 | MSE Train Loss: 1.3341972827911377 | MSE Test Loss: 1.3292784690856934 \n",
            "Epoch: 561\n",
            "-------\n",
            "Train loss: 1.33459\n",
            "Epoch: 562\n",
            "-------\n",
            "Train loss: 1.33497\n",
            "Epoch: 563\n",
            "-------\n",
            "Train loss: 1.33475\n",
            "Epoch: 564\n",
            "-------\n",
            "Train loss: 1.33512\n",
            "Epoch: 565\n",
            "-------\n",
            "Train loss: 1.33494\n",
            "Epoch: 566\n",
            "-------\n",
            "Train loss: 1.33556\n",
            "Epoch: 567\n",
            "-------\n",
            "Train loss: 1.33408\n",
            "Epoch: 568\n",
            "-------\n",
            "Train loss: 1.33427\n",
            "Epoch: 569\n",
            "-------\n",
            "Train loss: 1.33478\n",
            "Epoch: 570\n",
            "-------\n",
            "Train loss: 1.33441\n",
            "Epoch: 570 | MSE Train Loss: 1.3344085216522217 | MSE Test Loss: 1.329411268234253 \n",
            "Epoch: 571\n",
            "-------\n",
            "Train loss: 1.33461\n",
            "Epoch: 572\n",
            "-------\n",
            "Train loss: 1.33469\n",
            "Epoch: 573\n",
            "-------\n",
            "Train loss: 1.33437\n",
            "Epoch: 574\n",
            "-------\n",
            "Train loss: 1.33462\n",
            "Epoch: 575\n",
            "-------\n",
            "Train loss: 1.33532\n",
            "Epoch: 576\n",
            "-------\n",
            "Train loss: 1.33454\n",
            "Epoch: 577\n",
            "-------\n",
            "Train loss: 1.33386\n",
            "Epoch: 578\n",
            "-------\n",
            "Train loss: 1.33478\n",
            "Epoch: 579\n",
            "-------\n",
            "Train loss: 1.33470\n",
            "Epoch: 580\n",
            "-------\n",
            "Train loss: 1.33513\n",
            "Epoch: 580 | MSE Train Loss: 1.33512544631958 | MSE Test Loss: 1.3305882215499878 \n",
            "Epoch: 581\n",
            "-------\n",
            "Train loss: 1.33382\n",
            "Epoch: 582\n",
            "-------\n",
            "Train loss: 1.33512\n",
            "Epoch: 583\n",
            "-------\n",
            "Train loss: 1.33402\n",
            "Epoch: 584\n",
            "-------\n",
            "Train loss: 1.33471\n",
            "Epoch: 585\n",
            "-------\n",
            "Train loss: 1.33470\n",
            "Epoch: 586\n",
            "-------\n",
            "Train loss: 1.33469\n",
            "Epoch: 587\n",
            "-------\n",
            "Train loss: 1.33635\n",
            "Epoch: 588\n",
            "-------\n",
            "Train loss: 1.33442\n",
            "Epoch: 589\n",
            "-------\n",
            "Train loss: 1.33453\n",
            "Epoch: 590\n",
            "-------\n",
            "Train loss: 1.33454\n",
            "Epoch: 590 | MSE Train Loss: 1.3345428705215454 | MSE Test Loss: 1.3314672708511353 \n",
            "Epoch: 591\n",
            "-------\n",
            "Train loss: 1.33506\n",
            "Epoch: 592\n",
            "-------\n",
            "Train loss: 1.33418\n",
            "Epoch: 593\n",
            "-------\n",
            "Train loss: 1.33518\n",
            "Epoch: 594\n",
            "-------\n",
            "Train loss: 1.33459\n",
            "Epoch: 595\n",
            "-------\n",
            "Train loss: 1.33410\n",
            "Epoch: 596\n",
            "-------\n",
            "Train loss: 1.33460\n",
            "Epoch: 597\n",
            "-------\n",
            "Train loss: 1.33430\n",
            "Epoch: 598\n",
            "-------\n",
            "Train loss: 1.33469\n",
            "Epoch: 599\n",
            "-------\n",
            "Train loss: 1.33491\n",
            "Epoch: 600\n",
            "-------\n",
            "Train loss: 1.33439\n",
            "Epoch: 600 | MSE Train Loss: 1.3343932628631592 | MSE Test Loss: 1.329932451248169 \n",
            "Epoch: 601\n",
            "-------\n",
            "Train loss: 1.33451\n",
            "Epoch: 602\n",
            "-------\n",
            "Train loss: 1.33466\n",
            "Epoch: 603\n",
            "-------\n",
            "Train loss: 1.33498\n",
            "Epoch: 604\n",
            "-------\n",
            "Train loss: 1.33471\n",
            "Epoch: 605\n",
            "-------\n",
            "Train loss: 1.33465\n",
            "Epoch: 606\n",
            "-------\n",
            "Train loss: 1.33540\n",
            "Epoch: 607\n",
            "-------\n",
            "Train loss: 1.33466\n",
            "Epoch: 608\n",
            "-------\n",
            "Train loss: 1.33448\n",
            "Epoch: 609\n",
            "-------\n",
            "Train loss: 1.33450\n",
            "Epoch: 610\n",
            "-------\n",
            "Train loss: 1.33430\n",
            "Epoch: 610 | MSE Train Loss: 1.3343013525009155 | MSE Test Loss: 1.3300079107284546 \n",
            "Epoch: 611\n",
            "-------\n",
            "Train loss: 1.33410\n",
            "Epoch: 612\n",
            "-------\n",
            "Train loss: 1.33490\n",
            "Epoch: 613\n",
            "-------\n",
            "Train loss: 1.33422\n",
            "Epoch: 614\n",
            "-------\n",
            "Train loss: 1.33431\n",
            "Epoch: 615\n",
            "-------\n",
            "Train loss: 1.33379\n",
            "Epoch: 616\n",
            "-------\n",
            "Train loss: 1.33385\n",
            "Epoch: 617\n",
            "-------\n",
            "Train loss: 1.33478\n",
            "Epoch: 618\n",
            "-------\n",
            "Train loss: 1.33412\n",
            "Epoch: 619\n",
            "-------\n",
            "Train loss: 1.33423\n",
            "Epoch: 620\n",
            "-------\n",
            "Train loss: 1.33512\n",
            "Epoch: 620 | MSE Train Loss: 1.3351243734359741 | MSE Test Loss: 1.3293378353118896 \n",
            "Epoch: 621\n",
            "-------\n",
            "Train loss: 1.33499\n",
            "Epoch: 622\n",
            "-------\n",
            "Train loss: 1.33452\n",
            "Epoch: 623\n",
            "-------\n",
            "Train loss: 1.33522\n",
            "Epoch: 624\n",
            "-------\n",
            "Train loss: 1.33431\n",
            "Epoch: 625\n",
            "-------\n",
            "Train loss: 1.33482\n",
            "Epoch: 626\n",
            "-------\n",
            "Train loss: 1.33409\n",
            "Epoch: 627\n",
            "-------\n",
            "Train loss: 1.33478\n",
            "Epoch: 628\n",
            "-------\n",
            "Train loss: 1.33453\n",
            "Epoch: 629\n",
            "-------\n",
            "Train loss: 1.33456\n",
            "Epoch: 630\n",
            "-------\n",
            "Train loss: 1.33436\n",
            "Epoch: 630 | MSE Train Loss: 1.334357738494873 | MSE Test Loss: 1.3293919563293457 \n",
            "Epoch: 631\n",
            "-------\n",
            "Train loss: 1.33428\n",
            "Epoch: 632\n",
            "-------\n",
            "Train loss: 1.33444\n",
            "Epoch: 633\n",
            "-------\n",
            "Train loss: 1.33453\n",
            "Epoch: 634\n",
            "-------\n",
            "Train loss: 1.33444\n",
            "Epoch: 635\n",
            "-------\n",
            "Train loss: 1.33434\n",
            "Epoch: 636\n",
            "-------\n",
            "Train loss: 1.33450\n",
            "Epoch: 637\n",
            "-------\n",
            "Train loss: 1.33464\n",
            "Epoch: 638\n",
            "-------\n",
            "Train loss: 1.33472\n",
            "Epoch: 639\n",
            "-------\n",
            "Train loss: 1.33469\n",
            "Epoch: 640\n",
            "-------\n",
            "Train loss: 1.33492\n",
            "Epoch: 640 | MSE Train Loss: 1.3349181413650513 | MSE Test Loss: 1.3291441202163696 \n",
            "Epoch: 641\n",
            "-------\n",
            "Train loss: 1.33498\n",
            "Epoch: 642\n",
            "-------\n",
            "Train loss: 1.33423\n",
            "Epoch: 643\n",
            "-------\n",
            "Train loss: 1.33431\n",
            "Epoch: 644\n",
            "-------\n",
            "Train loss: 1.33452\n",
            "Epoch: 645\n",
            "-------\n",
            "Train loss: 1.33503\n",
            "Epoch: 646\n",
            "-------\n",
            "Train loss: 1.33459\n",
            "Epoch: 647\n",
            "-------\n",
            "Train loss: 1.33507\n",
            "Epoch: 648\n",
            "-------\n",
            "Train loss: 1.33397\n",
            "Epoch: 649\n",
            "-------\n",
            "Train loss: 1.33463\n",
            "Epoch: 650\n",
            "-------\n",
            "Train loss: 1.33525\n",
            "Epoch: 650 | MSE Train Loss: 1.3352503776550293 | MSE Test Loss: 1.3292982578277588 \n",
            "Epoch: 651\n",
            "-------\n",
            "Train loss: 1.33443\n",
            "Epoch: 652\n",
            "-------\n",
            "Train loss: 1.33491\n",
            "Epoch: 653\n",
            "-------\n",
            "Train loss: 1.33425\n",
            "Epoch: 654\n",
            "-------\n",
            "Train loss: 1.33449\n",
            "Epoch: 655\n",
            "-------\n",
            "Train loss: 1.33452\n",
            "Epoch: 656\n",
            "-------\n",
            "Train loss: 1.33439\n",
            "Epoch: 657\n",
            "-------\n",
            "Train loss: 1.33423\n",
            "Epoch: 658\n",
            "-------\n",
            "Train loss: 1.33531\n",
            "Epoch: 659\n",
            "-------\n",
            "Train loss: 1.33460\n",
            "Epoch: 660\n",
            "-------\n",
            "Train loss: 1.33471\n",
            "Epoch: 660 | MSE Train Loss: 1.3347100019454956 | MSE Test Loss: 1.3286612033843994 \n",
            "Epoch: 661\n",
            "-------\n",
            "Train loss: 1.33521\n",
            "Epoch: 662\n",
            "-------\n",
            "Train loss: 1.33538\n",
            "Epoch: 663\n",
            "-------\n",
            "Train loss: 1.33456\n",
            "Epoch: 664\n",
            "-------\n",
            "Train loss: 1.33437\n",
            "Epoch: 665\n",
            "-------\n",
            "Train loss: 1.33475\n",
            "Epoch: 666\n",
            "-------\n",
            "Train loss: 1.33430\n",
            "Epoch: 667\n",
            "-------\n",
            "Train loss: 1.33466\n",
            "Epoch: 668\n",
            "-------\n",
            "Train loss: 1.33480\n",
            "Epoch: 669\n",
            "-------\n",
            "Train loss: 1.33466\n",
            "Epoch: 670\n",
            "-------\n",
            "Train loss: 1.33489\n",
            "Epoch: 670 | MSE Train Loss: 1.3348913192749023 | MSE Test Loss: 1.3293763399124146 \n",
            "Epoch: 671\n",
            "-------\n",
            "Train loss: 1.33423\n",
            "Epoch: 672\n",
            "-------\n",
            "Train loss: 1.33449\n",
            "Epoch: 673\n",
            "-------\n",
            "Train loss: 1.33420\n",
            "Epoch: 674\n",
            "-------\n",
            "Train loss: 1.33421\n",
            "Epoch: 675\n",
            "-------\n",
            "Train loss: 1.33375\n",
            "Epoch: 676\n",
            "-------\n",
            "Train loss: 1.33458\n",
            "Epoch: 677\n",
            "-------\n",
            "Train loss: 1.33504\n",
            "Epoch: 678\n",
            "-------\n",
            "Train loss: 1.33433\n",
            "Epoch: 679\n",
            "-------\n",
            "Train loss: 1.33525\n",
            "Epoch: 680\n",
            "-------\n",
            "Train loss: 1.33443\n",
            "Epoch: 680 | MSE Train Loss: 1.3344305753707886 | MSE Test Loss: 1.329529047012329 \n",
            "Epoch: 681\n",
            "-------\n",
            "Train loss: 1.33532\n",
            "Epoch: 682\n",
            "-------\n",
            "Train loss: 1.33433\n",
            "Epoch: 683\n",
            "-------\n",
            "Train loss: 1.33428\n",
            "Epoch: 684\n",
            "-------\n",
            "Train loss: 1.33435\n",
            "Epoch: 685\n",
            "-------\n",
            "Train loss: 1.33477\n",
            "Epoch: 686\n",
            "-------\n",
            "Train loss: 1.33468\n",
            "Epoch: 687\n",
            "-------\n",
            "Train loss: 1.33439\n",
            "Epoch: 688\n",
            "-------\n",
            "Train loss: 1.33469\n",
            "Epoch: 689\n",
            "-------\n",
            "Train loss: 1.33482\n",
            "Epoch: 690\n",
            "-------\n",
            "Train loss: 1.33426\n",
            "Epoch: 690 | MSE Train Loss: 1.3342560529708862 | MSE Test Loss: 1.32981538772583 \n",
            "Epoch: 691\n",
            "-------\n",
            "Train loss: 1.33411\n",
            "Epoch: 692\n",
            "-------\n",
            "Train loss: 1.33471\n",
            "Epoch: 693\n",
            "-------\n",
            "Train loss: 1.33488\n",
            "Epoch: 694\n",
            "-------\n",
            "Train loss: 1.33483\n",
            "Epoch: 695\n",
            "-------\n",
            "Train loss: 1.33526\n",
            "Epoch: 696\n",
            "-------\n",
            "Train loss: 1.33441\n",
            "Epoch: 697\n",
            "-------\n",
            "Train loss: 1.33408\n",
            "Epoch: 698\n",
            "-------\n",
            "Train loss: 1.33554\n",
            "Epoch: 699\n",
            "-------\n",
            "Train loss: 1.33449\n",
            "Epoch: 700\n",
            "-------\n",
            "Train loss: 1.33420\n",
            "Epoch: 700 | MSE Train Loss: 1.334203839302063 | MSE Test Loss: 1.3287566900253296 \n",
            "Epoch: 701\n",
            "-------\n",
            "Train loss: 1.33414\n",
            "Epoch: 702\n",
            "-------\n",
            "Train loss: 1.33472\n",
            "Epoch: 703\n",
            "-------\n",
            "Train loss: 1.33480\n",
            "Epoch: 704\n",
            "-------\n",
            "Train loss: 1.33448\n",
            "Epoch: 705\n",
            "-------\n",
            "Train loss: 1.33418\n",
            "Epoch: 706\n",
            "-------\n",
            "Train loss: 1.33461\n",
            "Epoch: 707\n",
            "-------\n",
            "Train loss: 1.33427\n",
            "Epoch: 708\n",
            "-------\n",
            "Train loss: 1.33464\n",
            "Epoch: 709\n",
            "-------\n",
            "Train loss: 1.33453\n",
            "Epoch: 710\n",
            "-------\n",
            "Train loss: 1.33404\n",
            "Epoch: 710 | MSE Train Loss: 1.3340407609939575 | MSE Test Loss: 1.329307198524475 \n",
            "Epoch: 711\n",
            "-------\n",
            "Train loss: 1.33440\n",
            "Epoch: 712\n",
            "-------\n",
            "Train loss: 1.33463\n",
            "Epoch: 713\n",
            "-------\n",
            "Train loss: 1.33483\n",
            "Epoch: 714\n",
            "-------\n",
            "Train loss: 1.33421\n",
            "Epoch: 715\n",
            "-------\n",
            "Train loss: 1.33509\n",
            "Epoch: 716\n",
            "-------\n",
            "Train loss: 1.33416\n",
            "Epoch: 717\n",
            "-------\n",
            "Train loss: 1.33468\n",
            "Epoch: 718\n",
            "-------\n",
            "Train loss: 1.33435\n",
            "Epoch: 719\n",
            "-------\n",
            "Train loss: 1.33404\n",
            "Epoch: 720\n",
            "-------\n",
            "Train loss: 1.33435\n",
            "Epoch: 720 | MSE Train Loss: 1.334351897239685 | MSE Test Loss: 1.3297741413116455 \n",
            "Epoch: 721\n",
            "-------\n",
            "Train loss: 1.33511\n",
            "Epoch: 722\n",
            "-------\n",
            "Train loss: 1.33450\n",
            "Epoch: 723\n",
            "-------\n",
            "Train loss: 1.33470\n",
            "Epoch: 724\n",
            "-------\n",
            "Train loss: 1.33484\n",
            "Epoch: 725\n",
            "-------\n",
            "Train loss: 1.33409\n",
            "Epoch: 726\n",
            "-------\n",
            "Train loss: 1.33407\n",
            "Epoch: 727\n",
            "-------\n",
            "Train loss: 1.33466\n",
            "Epoch: 728\n",
            "-------\n",
            "Train loss: 1.33458\n",
            "Epoch: 729\n",
            "-------\n",
            "Train loss: 1.33438\n",
            "Epoch: 730\n",
            "-------\n",
            "Train loss: 1.33376\n",
            "Epoch: 730 | MSE Train Loss: 1.3337550163269043 | MSE Test Loss: 1.3287723064422607 \n",
            "Epoch: 731\n",
            "-------\n",
            "Train loss: 1.33546\n",
            "Epoch: 732\n",
            "-------\n",
            "Train loss: 1.33456\n",
            "Epoch: 733\n",
            "-------\n",
            "Train loss: 1.33452\n",
            "Epoch: 734\n",
            "-------\n",
            "Train loss: 1.33494\n",
            "Epoch: 735\n",
            "-------\n",
            "Train loss: 1.33440\n",
            "Epoch: 736\n",
            "-------\n",
            "Train loss: 1.33464\n",
            "Epoch: 737\n",
            "-------\n",
            "Train loss: 1.33433\n",
            "Epoch: 738\n",
            "-------\n",
            "Train loss: 1.33425\n",
            "Epoch: 739\n",
            "-------\n",
            "Train loss: 1.33521\n",
            "Epoch: 740\n",
            "-------\n",
            "Train loss: 1.33497\n",
            "Epoch: 740 | MSE Train Loss: 1.3349711894989014 | MSE Test Loss: 1.3291475772857666 \n",
            "Epoch: 741\n",
            "-------\n",
            "Train loss: 1.33398\n",
            "Epoch: 742\n",
            "-------\n",
            "Train loss: 1.33508\n",
            "Epoch: 743\n",
            "-------\n",
            "Train loss: 1.33436\n",
            "Epoch: 744\n",
            "-------\n",
            "Train loss: 1.33460\n",
            "Epoch: 745\n",
            "-------\n",
            "Train loss: 1.33500\n",
            "Epoch: 746\n",
            "-------\n",
            "Train loss: 1.33468\n",
            "Epoch: 747\n",
            "-------\n",
            "Train loss: 1.33416\n",
            "Epoch: 748\n",
            "-------\n",
            "Train loss: 1.33489\n",
            "Epoch: 749\n",
            "-------\n",
            "Train loss: 1.33422\n",
            "Epoch: 750\n",
            "-------\n",
            "Train loss: 1.33453\n",
            "Epoch: 750 | MSE Train Loss: 1.3345328569412231 | MSE Test Loss: 1.3302026987075806 \n",
            "Epoch: 751\n",
            "-------\n",
            "Train loss: 1.33416\n",
            "Epoch: 752\n",
            "-------\n",
            "Train loss: 1.33441\n",
            "Epoch: 753\n",
            "-------\n",
            "Train loss: 1.33420\n",
            "Epoch: 754\n",
            "-------\n",
            "Train loss: 1.33510\n",
            "Epoch: 755\n",
            "-------\n",
            "Train loss: 1.33474\n",
            "Epoch: 756\n",
            "-------\n",
            "Train loss: 1.33398\n",
            "Epoch: 757\n",
            "-------\n",
            "Train loss: 1.33480\n",
            "Epoch: 758\n",
            "-------\n",
            "Train loss: 1.33497\n",
            "Epoch: 759\n",
            "-------\n",
            "Train loss: 1.33386\n",
            "Epoch: 760\n",
            "-------\n",
            "Train loss: 1.33476\n",
            "Epoch: 760 | MSE Train Loss: 1.3347642421722412 | MSE Test Loss: 1.3286478519439697 \n",
            "Epoch: 761\n",
            "-------\n",
            "Train loss: 1.33517\n",
            "Epoch: 762\n",
            "-------\n",
            "Train loss: 1.33413\n",
            "Epoch: 763\n",
            "-------\n",
            "Train loss: 1.33458\n",
            "Epoch: 764\n",
            "-------\n",
            "Train loss: 1.33478\n",
            "Epoch: 765\n",
            "-------\n",
            "Train loss: 1.33424\n",
            "Epoch: 766\n",
            "-------\n",
            "Train loss: 1.33464\n",
            "Epoch: 767\n",
            "-------\n",
            "Train loss: 1.33461\n",
            "Epoch: 768\n",
            "-------\n",
            "Train loss: 1.33448\n",
            "Epoch: 769\n",
            "-------\n",
            "Train loss: 1.33430\n",
            "Epoch: 770\n",
            "-------\n",
            "Train loss: 1.33438\n",
            "Epoch: 770 | MSE Train Loss: 1.3343751430511475 | MSE Test Loss: 1.3309271335601807 \n",
            "Epoch: 771\n",
            "-------\n",
            "Train loss: 1.33474\n",
            "Epoch: 772\n",
            "-------\n",
            "Train loss: 1.33484\n",
            "Epoch: 773\n",
            "-------\n",
            "Train loss: 1.33398\n",
            "Epoch: 774\n",
            "-------\n",
            "Train loss: 1.33529\n",
            "Epoch: 775\n",
            "-------\n",
            "Train loss: 1.33457\n",
            "Epoch: 776\n",
            "-------\n",
            "Train loss: 1.33436\n",
            "Epoch: 777\n",
            "-------\n",
            "Train loss: 1.33495\n",
            "Epoch: 778\n",
            "-------\n",
            "Train loss: 1.33424\n",
            "Epoch: 779\n",
            "-------\n",
            "Train loss: 1.33462\n",
            "Epoch: 780\n",
            "-------\n",
            "Train loss: 1.33421\n",
            "Epoch: 780 | MSE Train Loss: 1.3342115879058838 | MSE Test Loss: 1.3287601470947266 \n",
            "Epoch: 781\n",
            "-------\n",
            "Train loss: 1.33438\n",
            "Epoch: 782\n",
            "-------\n",
            "Train loss: 1.33414\n",
            "Epoch: 783\n",
            "-------\n",
            "Train loss: 1.33399\n",
            "Epoch: 784\n",
            "-------\n",
            "Train loss: 1.33487\n",
            "Epoch: 785\n",
            "-------\n",
            "Train loss: 1.33397\n",
            "Epoch: 786\n",
            "-------\n",
            "Train loss: 1.33457\n",
            "Epoch: 787\n",
            "-------\n",
            "Train loss: 1.33506\n",
            "Epoch: 788\n",
            "-------\n",
            "Train loss: 1.33423\n",
            "Epoch: 789\n",
            "-------\n",
            "Train loss: 1.33427\n",
            "Epoch: 790\n",
            "-------\n",
            "Train loss: 1.33455\n",
            "Epoch: 790 | MSE Train Loss: 1.334547758102417 | MSE Test Loss: 1.3291293382644653 \n",
            "Epoch: 791\n",
            "-------\n",
            "Train loss: 1.33395\n",
            "Epoch: 792\n",
            "-------\n",
            "Train loss: 1.33386\n",
            "Epoch: 793\n",
            "-------\n",
            "Train loss: 1.33412\n",
            "Epoch: 794\n",
            "-------\n",
            "Train loss: 1.33444\n",
            "Epoch: 795\n",
            "-------\n",
            "Train loss: 1.33466\n",
            "Epoch: 796\n",
            "-------\n",
            "Train loss: 1.33414\n",
            "Epoch: 797\n",
            "-------\n",
            "Train loss: 1.33463\n",
            "Epoch: 798\n",
            "-------\n",
            "Train loss: 1.33477\n",
            "Epoch: 799\n",
            "-------\n",
            "Train loss: 1.33426\n",
            "Epoch: 800\n",
            "-------\n",
            "Train loss: 1.33495\n",
            "Epoch: 800 | MSE Train Loss: 1.3349450826644897 | MSE Test Loss: 1.3289519548416138 \n",
            "Epoch: 801\n",
            "-------\n",
            "Train loss: 1.33474\n",
            "Epoch: 802\n",
            "-------\n",
            "Train loss: 1.33453\n",
            "Epoch: 803\n",
            "-------\n",
            "Train loss: 1.33343\n",
            "Epoch: 804\n",
            "-------\n",
            "Train loss: 1.33411\n",
            "Epoch: 805\n",
            "-------\n",
            "Train loss: 1.33461\n",
            "Epoch: 806\n",
            "-------\n",
            "Train loss: 1.33418\n",
            "Epoch: 807\n",
            "-------\n",
            "Train loss: 1.33482\n",
            "Epoch: 808\n",
            "-------\n",
            "Train loss: 1.33469\n",
            "Epoch: 809\n",
            "-------\n",
            "Train loss: 1.33453\n",
            "Epoch: 810\n",
            "-------\n",
            "Train loss: 1.33392\n",
            "Epoch: 810 | MSE Train Loss: 1.3339163064956665 | MSE Test Loss: 1.32884681224823 \n",
            "Epoch: 811\n",
            "-------\n",
            "Train loss: 1.33492\n",
            "Epoch: 812\n",
            "-------\n",
            "Train loss: 1.33477\n",
            "Epoch: 813\n",
            "-------\n",
            "Train loss: 1.33434\n",
            "Epoch: 814\n",
            "-------\n",
            "Train loss: 1.33476\n",
            "Epoch: 815\n",
            "-------\n",
            "Train loss: 1.33427\n",
            "Epoch: 816\n",
            "-------\n",
            "Train loss: 1.33334\n",
            "Epoch: 817\n",
            "-------\n",
            "Train loss: 1.33427\n",
            "Epoch: 818\n",
            "-------\n",
            "Train loss: 1.33493\n",
            "Epoch: 819\n",
            "-------\n",
            "Train loss: 1.33412\n",
            "Epoch: 820\n",
            "-------\n",
            "Train loss: 1.33482\n",
            "Epoch: 820 | MSE Train Loss: 1.3348232507705688 | MSE Test Loss: 1.329532504081726 \n",
            "Epoch: 821\n",
            "-------\n",
            "Train loss: 1.33418\n",
            "Epoch: 822\n",
            "-------\n",
            "Train loss: 1.33462\n",
            "Epoch: 823\n",
            "-------\n",
            "Train loss: 1.33475\n",
            "Epoch: 824\n",
            "-------\n",
            "Train loss: 1.33521\n",
            "Epoch: 825\n",
            "-------\n",
            "Train loss: 1.33440\n",
            "Epoch: 826\n",
            "-------\n",
            "Train loss: 1.33427\n",
            "Epoch: 827\n",
            "-------\n",
            "Train loss: 1.33469\n",
            "Epoch: 828\n",
            "-------\n",
            "Train loss: 1.33446\n",
            "Epoch: 829\n",
            "-------\n",
            "Train loss: 1.33483\n",
            "Epoch: 830\n",
            "-------\n",
            "Train loss: 1.33415\n",
            "Epoch: 830 | MSE Train Loss: 1.334153413772583 | MSE Test Loss: 1.3292319774627686 \n",
            "Epoch: 831\n",
            "-------\n",
            "Train loss: 1.33419\n",
            "Epoch: 832\n",
            "-------\n",
            "Train loss: 1.33414\n",
            "Epoch: 833\n",
            "-------\n",
            "Train loss: 1.33422\n",
            "Epoch: 834\n",
            "-------\n",
            "Train loss: 1.33421\n",
            "Epoch: 835\n",
            "-------\n",
            "Train loss: 1.33407\n",
            "Epoch: 836\n",
            "-------\n",
            "Train loss: 1.33509\n",
            "Epoch: 837\n",
            "-------\n",
            "Train loss: 1.33450\n",
            "Epoch: 838\n",
            "-------\n",
            "Train loss: 1.33441\n",
            "Epoch: 839\n",
            "-------\n",
            "Train loss: 1.33551\n",
            "Epoch: 840\n",
            "-------\n",
            "Train loss: 1.33466\n",
            "Epoch: 840 | MSE Train Loss: 1.3346576690673828 | MSE Test Loss: 1.328808307647705 \n",
            "Epoch: 841\n",
            "-------\n",
            "Train loss: 1.33401\n",
            "Epoch: 842\n",
            "-------\n",
            "Train loss: 1.33427\n",
            "Epoch: 843\n",
            "-------\n",
            "Train loss: 1.33417\n",
            "Epoch: 844\n",
            "-------\n",
            "Train loss: 1.33449\n",
            "Epoch: 845\n",
            "-------\n",
            "Train loss: 1.33459\n",
            "Epoch: 846\n",
            "-------\n",
            "Train loss: 1.33478\n",
            "Epoch: 847\n",
            "-------\n",
            "Train loss: 1.33448\n",
            "Epoch: 848\n",
            "-------\n",
            "Train loss: 1.33500\n",
            "Epoch: 849\n",
            "-------\n",
            "Train loss: 1.33417\n",
            "Epoch: 850\n",
            "-------\n",
            "Train loss: 1.33481\n",
            "Epoch: 850 | MSE Train Loss: 1.3348069190979004 | MSE Test Loss: 1.3290302753448486 \n",
            "Epoch: 851\n",
            "-------\n",
            "Train loss: 1.33455\n",
            "Epoch: 852\n",
            "-------\n",
            "Train loss: 1.33416\n",
            "Epoch: 853\n",
            "-------\n",
            "Train loss: 1.33433\n",
            "Epoch: 854\n",
            "-------\n",
            "Train loss: 1.33387\n",
            "Epoch: 855\n",
            "-------\n",
            "Train loss: 1.33420\n",
            "Epoch: 856\n",
            "-------\n",
            "Train loss: 1.33410\n",
            "Epoch: 857\n",
            "-------\n",
            "Train loss: 1.33431\n",
            "Epoch: 858\n",
            "-------\n",
            "Train loss: 1.33411\n",
            "Epoch: 859\n",
            "-------\n",
            "Train loss: 1.33423\n",
            "Epoch: 860\n",
            "-------\n",
            "Train loss: 1.33458\n",
            "Epoch: 860 | MSE Train Loss: 1.3345792293548584 | MSE Test Loss: 1.330095648765564 \n",
            "Epoch: 861\n",
            "-------\n",
            "Train loss: 1.33381\n",
            "Epoch: 862\n",
            "-------\n",
            "Train loss: 1.33394\n",
            "Epoch: 863\n",
            "-------\n",
            "Train loss: 1.33438\n",
            "Epoch: 864\n",
            "-------\n",
            "Train loss: 1.33429\n",
            "Epoch: 865\n",
            "-------\n",
            "Train loss: 1.33476\n",
            "Epoch: 866\n",
            "-------\n",
            "Train loss: 1.33477\n",
            "Epoch: 867\n",
            "-------\n",
            "Train loss: 1.33498\n",
            "Epoch: 868\n",
            "-------\n",
            "Train loss: 1.33461\n",
            "Epoch: 869\n",
            "-------\n",
            "Train loss: 1.33400\n",
            "Epoch: 870\n",
            "-------\n",
            "Train loss: 1.33510\n",
            "Epoch: 870 | MSE Train Loss: 1.3350977897644043 | MSE Test Loss: 1.3291678428649902 \n",
            "Epoch: 871\n",
            "-------\n",
            "Train loss: 1.33412\n",
            "Epoch: 872\n",
            "-------\n",
            "Train loss: 1.33421\n",
            "Epoch: 873\n",
            "-------\n",
            "Train loss: 1.33510\n",
            "Epoch: 874\n",
            "-------\n",
            "Train loss: 1.33384\n",
            "Epoch: 875\n",
            "-------\n",
            "Train loss: 1.33456\n",
            "Epoch: 876\n",
            "-------\n",
            "Train loss: 1.33445\n",
            "Epoch: 877\n",
            "-------\n",
            "Train loss: 1.33438\n",
            "Epoch: 878\n",
            "-------\n",
            "Train loss: 1.33430\n",
            "Epoch: 879\n",
            "-------\n",
            "Train loss: 1.33429\n",
            "Epoch: 880\n",
            "-------\n",
            "Train loss: 1.33462\n",
            "Epoch: 880 | MSE Train Loss: 1.334618091583252 | MSE Test Loss: 1.3290133476257324 \n",
            "Epoch: 881\n",
            "-------\n",
            "Train loss: 1.33444\n",
            "Epoch: 882\n",
            "-------\n",
            "Train loss: 1.33390\n",
            "Epoch: 883\n",
            "-------\n",
            "Train loss: 1.33440\n",
            "Epoch: 884\n",
            "-------\n",
            "Train loss: 1.33470\n",
            "Epoch: 885\n",
            "-------\n",
            "Train loss: 1.33447\n",
            "Epoch: 886\n",
            "-------\n",
            "Train loss: 1.33435\n",
            "Epoch: 887\n",
            "-------\n",
            "Train loss: 1.33439\n",
            "Epoch: 888\n",
            "-------\n",
            "Train loss: 1.33436\n",
            "Epoch: 889\n",
            "-------\n",
            "Train loss: 1.33438\n",
            "Epoch: 890\n",
            "-------\n",
            "Train loss: 1.33471\n",
            "Epoch: 890 | MSE Train Loss: 1.3347114324569702 | MSE Test Loss: 1.3303651809692383 \n",
            "Epoch: 891\n",
            "-------\n",
            "Train loss: 1.33438\n",
            "Epoch: 892\n",
            "-------\n",
            "Train loss: 1.33452\n",
            "Epoch: 893\n",
            "-------\n",
            "Train loss: 1.33491\n",
            "Epoch: 894\n",
            "-------\n",
            "Train loss: 1.33449\n",
            "Epoch: 895\n",
            "-------\n",
            "Train loss: 1.33437\n",
            "Epoch: 896\n",
            "-------\n",
            "Train loss: 1.33396\n",
            "Epoch: 897\n",
            "-------\n",
            "Train loss: 1.33449\n",
            "Epoch: 898\n",
            "-------\n",
            "Train loss: 1.33495\n",
            "Epoch: 899\n",
            "-------\n",
            "Train loss: 1.33433\n",
            "Epoch: 900\n",
            "-------\n",
            "Train loss: 1.33415\n",
            "Epoch: 900 | MSE Train Loss: 1.334151029586792 | MSE Test Loss: 1.3291594982147217 \n",
            "Epoch: 901\n",
            "-------\n",
            "Train loss: 1.33394\n",
            "Epoch: 902\n",
            "-------\n",
            "Train loss: 1.33399\n",
            "Epoch: 903\n",
            "-------\n",
            "Train loss: 1.33473\n",
            "Epoch: 904\n",
            "-------\n",
            "Train loss: 1.33431\n",
            "Epoch: 905\n",
            "-------\n",
            "Train loss: 1.33461\n",
            "Epoch: 906\n",
            "-------\n",
            "Train loss: 1.33433\n",
            "Epoch: 907\n",
            "-------\n",
            "Train loss: 1.33463\n",
            "Epoch: 908\n",
            "-------\n",
            "Train loss: 1.33467\n",
            "Epoch: 909\n",
            "-------\n",
            "Train loss: 1.33424\n",
            "Epoch: 910\n",
            "-------\n",
            "Train loss: 1.33428\n",
            "Epoch: 910 | MSE Train Loss: 1.334283709526062 | MSE Test Loss: 1.3288161754608154 \n",
            "Epoch: 911\n",
            "-------\n",
            "Train loss: 1.33444\n",
            "Epoch: 912\n",
            "-------\n",
            "Train loss: 1.33475\n",
            "Epoch: 913\n",
            "-------\n",
            "Train loss: 1.33364\n",
            "Epoch: 914\n",
            "-------\n",
            "Train loss: 1.33399\n",
            "Epoch: 915\n",
            "-------\n",
            "Train loss: 1.33424\n",
            "Epoch: 916\n",
            "-------\n",
            "Train loss: 1.33475\n",
            "Epoch: 917\n",
            "-------\n",
            "Train loss: 1.33479\n",
            "Epoch: 918\n",
            "-------\n",
            "Train loss: 1.33443\n",
            "Epoch: 919\n",
            "-------\n",
            "Train loss: 1.33447\n",
            "Epoch: 920\n",
            "-------\n",
            "Train loss: 1.33457\n",
            "Epoch: 920 | MSE Train Loss: 1.3345693349838257 | MSE Test Loss: 1.3303154706954956 \n",
            "Epoch: 921\n",
            "-------\n",
            "Train loss: 1.33470\n",
            "Epoch: 922\n",
            "-------\n",
            "Train loss: 1.33371\n",
            "Epoch: 923\n",
            "-------\n",
            "Train loss: 1.33446\n",
            "Epoch: 924\n",
            "-------\n",
            "Train loss: 1.33456\n",
            "Epoch: 925\n",
            "-------\n",
            "Train loss: 1.33467\n",
            "Epoch: 926\n",
            "-------\n",
            "Train loss: 1.33403\n",
            "Epoch: 927\n",
            "-------\n",
            "Train loss: 1.33451\n",
            "Epoch: 928\n",
            "-------\n",
            "Train loss: 1.33381\n",
            "Epoch: 929\n",
            "-------\n",
            "Train loss: 1.33453\n",
            "Epoch: 930\n",
            "-------\n",
            "Train loss: 1.33424\n",
            "Epoch: 930 | MSE Train Loss: 1.334237813949585 | MSE Test Loss: 1.3293083906173706 \n",
            "Epoch: 931\n",
            "-------\n",
            "Train loss: 1.33404\n",
            "Epoch: 932\n",
            "-------\n",
            "Train loss: 1.33421\n",
            "Epoch: 933\n",
            "-------\n",
            "Train loss: 1.33435\n",
            "Epoch: 934\n",
            "-------\n",
            "Train loss: 1.33442\n",
            "Epoch: 935\n",
            "-------\n",
            "Train loss: 1.33436\n",
            "Epoch: 936\n",
            "-------\n",
            "Train loss: 1.33418\n",
            "Epoch: 937\n",
            "-------\n",
            "Train loss: 1.33420\n",
            "Epoch: 938\n",
            "-------\n",
            "Train loss: 1.33434\n",
            "Epoch: 939\n",
            "-------\n",
            "Train loss: 1.33417\n",
            "Epoch: 940\n",
            "-------\n",
            "Train loss: 1.33397\n",
            "Epoch: 940 | MSE Train Loss: 1.3339693546295166 | MSE Test Loss: 1.328979730606079 \n",
            "Epoch: 941\n",
            "-------\n",
            "Train loss: 1.33395\n",
            "Epoch: 942\n",
            "-------\n",
            "Train loss: 1.33411\n",
            "Epoch: 943\n",
            "-------\n",
            "Train loss: 1.33432\n",
            "Epoch: 944\n",
            "-------\n",
            "Train loss: 1.33451\n",
            "Epoch: 945\n",
            "-------\n",
            "Train loss: 1.33515\n",
            "Epoch: 946\n",
            "-------\n",
            "Train loss: 1.33449\n",
            "Epoch: 947\n",
            "-------\n",
            "Train loss: 1.33403\n",
            "Epoch: 948\n",
            "-------\n",
            "Train loss: 1.33485\n",
            "Epoch: 949\n",
            "-------\n",
            "Train loss: 1.33432\n",
            "Epoch: 950\n",
            "-------\n",
            "Train loss: 1.33470\n",
            "Epoch: 950 | MSE Train Loss: 1.3347041606903076 | MSE Test Loss: 1.328874945640564 \n",
            "Epoch: 951\n",
            "-------\n",
            "Train loss: 1.33412\n",
            "Epoch: 952\n",
            "-------\n",
            "Train loss: 1.33481\n",
            "Epoch: 953\n",
            "-------\n",
            "Train loss: 1.33475\n",
            "Epoch: 954\n",
            "-------\n",
            "Train loss: 1.33469\n",
            "Epoch: 955\n",
            "-------\n",
            "Train loss: 1.33500\n",
            "Epoch: 956\n",
            "-------\n",
            "Train loss: 1.33447\n",
            "Epoch: 957\n",
            "-------\n",
            "Train loss: 1.33445\n",
            "Epoch: 958\n",
            "-------\n",
            "Train loss: 1.33431\n",
            "Epoch: 959\n",
            "-------\n",
            "Train loss: 1.33507\n",
            "Epoch: 960\n",
            "-------\n",
            "Train loss: 1.33440\n",
            "Epoch: 960 | MSE Train Loss: 1.3344045877456665 | MSE Test Loss: 1.3291646242141724 \n",
            "Epoch: 961\n",
            "-------\n",
            "Train loss: 1.33440\n",
            "Epoch: 962\n",
            "-------\n",
            "Train loss: 1.33422\n",
            "Epoch: 963\n",
            "-------\n",
            "Train loss: 1.33473\n",
            "Epoch: 964\n",
            "-------\n",
            "Train loss: 1.33434\n",
            "Epoch: 965\n",
            "-------\n",
            "Train loss: 1.33486\n",
            "Epoch: 966\n",
            "-------\n",
            "Train loss: 1.33442\n",
            "Epoch: 967\n",
            "-------\n",
            "Train loss: 1.33416\n",
            "Epoch: 968\n",
            "-------\n",
            "Train loss: 1.33444\n",
            "Epoch: 969\n",
            "-------\n",
            "Train loss: 1.33455\n",
            "Epoch: 970\n",
            "-------\n",
            "Train loss: 1.33426\n",
            "Epoch: 970 | MSE Train Loss: 1.334259033203125 | MSE Test Loss: 1.3299840688705444 \n",
            "Epoch: 971\n",
            "-------\n",
            "Train loss: 1.33383\n",
            "Epoch: 972\n",
            "-------\n",
            "Train loss: 1.33495\n",
            "Epoch: 973\n",
            "-------\n",
            "Train loss: 1.33465\n",
            "Epoch: 974\n",
            "-------\n",
            "Train loss: 1.33427\n",
            "Epoch: 975\n",
            "-------\n",
            "Train loss: 1.33389\n",
            "Epoch: 976\n",
            "-------\n",
            "Train loss: 1.33414\n",
            "Epoch: 977\n",
            "-------\n",
            "Train loss: 1.33505\n",
            "Epoch: 978\n",
            "-------\n",
            "Train loss: 1.33406\n",
            "Epoch: 979\n",
            "-------\n",
            "Train loss: 1.33401\n",
            "Epoch: 980\n",
            "-------\n",
            "Train loss: 1.33466\n",
            "Epoch: 980 | MSE Train Loss: 1.3346587419509888 | MSE Test Loss: 1.329180121421814 \n",
            "Epoch: 981\n",
            "-------\n",
            "Train loss: 1.33468\n",
            "Epoch: 982\n",
            "-------\n",
            "Train loss: 1.33431\n",
            "Epoch: 983\n",
            "-------\n",
            "Train loss: 1.33465\n",
            "Epoch: 984\n",
            "-------\n",
            "Train loss: 1.33475\n",
            "Epoch: 985\n",
            "-------\n",
            "Train loss: 1.33442\n",
            "Epoch: 986\n",
            "-------\n",
            "Train loss: 1.33465\n",
            "Epoch: 987\n",
            "-------\n",
            "Train loss: 1.33410\n",
            "Epoch: 988\n",
            "-------\n",
            "Train loss: 1.33422\n",
            "Epoch: 989\n",
            "-------\n",
            "Train loss: 1.33452\n",
            "Epoch: 990\n",
            "-------\n",
            "Train loss: 1.33464\n",
            "Epoch: 990 | MSE Train Loss: 1.3346446752548218 | MSE Test Loss: 1.3291120529174805 \n",
            "Epoch: 991\n",
            "-------\n",
            "Train loss: 1.33451\n",
            "Epoch: 992\n",
            "-------\n",
            "Train loss: 1.33468\n",
            "Epoch: 993\n",
            "-------\n",
            "Train loss: 1.33473\n",
            "Epoch: 994\n",
            "-------\n",
            "Train loss: 1.33443\n",
            "Epoch: 995\n",
            "-------\n",
            "Train loss: 1.33378\n",
            "Epoch: 996\n",
            "-------\n",
            "Train loss: 1.33416\n",
            "Epoch: 997\n",
            "-------\n",
            "Train loss: 1.33432\n",
            "Epoch: 998\n",
            "-------\n",
            "Train loss: 1.33435\n",
            "Epoch: 999\n",
            "-------\n",
            "Train loss: 1.33489\n",
            "Train time on cpu: 750.949 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss curves\n",
        "plt.plot(epoch_count, train_loss_values, label=\"Train loss\")\n",
        "plt.plot(epoch_count, test_loss_values, label=\"Test loss\")\n",
        "plt.title(\"Training and test loss curves\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "j3Ti0BDlGP4E",
        "outputId": "6529e41c-58c4-4da2-907f-d98192d459d2"
      },
      "id": "j3Ti0BDlGP4E",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABWNElEQVR4nO3deXgT1cIG8HeStEm3pC1Q2kKBsrasIptYNgVZL1rcELjQunHRFkTEq4iyKVRRuV71UgEVXEC2j01kq0BBEGRH9kWWVqDs3WnaJOf7I8nQ0FJKSTJteX/Pk6dkMsuZk0nycuacGUkIIUBERERUSaiULgARERGRMzHcEBERUaXCcENERESVCsMNERERVSoMN0RERFSpMNwQERFRpcJwQ0RERJUKww0RERFVKgw3REREVKkw3BC5WGxsLOrUqVOmZSdMmABJkpxboHLmzJkzkCQJc+bMUbooZSJJEiZMmKB0MYioEIYbum9JklSqR3JystJFJQCHDx/GhAkTcObMGZduZ/r06RU2aBGRlUbpAhAp5YcffnB4/v333yMpKanI9MjIyHvazqxZs2CxWMq07Lvvvou33377nrZfWRw+fBgTJ05Ely5dytwSVhrTp09H1apVERsb67JtEJFrMdzQfeuf//ynw/Pt27cjKSmpyPRb5ebmwtvbu9Tb8fDwKFP5AECj0UCj4ceUyheLxYL8/HzodDqli0JULJ6WIipBly5d0LRpU+zevRudOnWCt7c33nnnHQDA8uXL0adPH4SGhkKr1aJevXp4//33YTabHdZxa58bex+TTz75BDNnzkS9evWg1WrRpk0b7Ny502HZ4vrcSJKE+Ph4LFu2DE2bNoVWq0WTJk2wZs2aIuVPTk5G69atodPpUK9ePcyYMaPU/Xh+++03PPPMM6hVqxa0Wi3CwsLw+uuv48aNG0X2z9fXF+fOnUN0dDR8fX1RrVo1jB49ukhdpKenIzY2FgaDAf7+/oiJiUF6evodyzJnzhw888wzAIBHHnmk2FOGq1evRseOHeHj4wM/Pz/06dMHhw4dclhPWloann/+edSsWRNarRYhISF44okn5FNdderUwaFDh7Bp0yZ5G126dLlj+W61d+9e9OrVC3q9Hr6+vujatSu2b9/uME9BQQEmTpyIBg0aQKfToUqVKujQoQOSkpJKXd6SHD16FM8++yyqVasGLy8vNGrUCGPHjpVfv11fsJKOublz56JJkybQarX4+eefERgYiOeff77IOjIzM6HT6TB69Gh5mtFoxPjx41G/fn35ePr3v/8No9HosGxSUhI6dOgAf39/+Pr6olGjRvJnjqi0+F9Coju4evUqevXqheeeew7//Oc/Ub16dQDWH1xfX1+MGjUKvr6+2LBhA8aNG4fMzEx8/PHHd1zvvHnzkJWVhX/961+QJAlTp07Fk08+iVOnTt2xtWfLli1YsmQJXn31Vfj5+eHzzz/HU089hZSUFFSpUgWA9Qe2Z8+eCAkJwcSJE2E2mzFp0iRUq1atVPu9aNEi5Obm4pVXXkGVKlWwY8cOfPHFF/j777+xaNEih3nNZjN69OiBdu3a4ZNPPsGvv/6KTz/9FPXq1cMrr7wCABBC4IknnsCWLVswbNgwREZGYunSpYiJibljWTp16oQRI0bg888/xzvvvCOfKrT//eGHHxATE4MePXrgo48+Qm5uLhITE9GhQwfs3btX/hF/6qmncOjQIQwfPhx16tTBpUuXkJSUhJSUFNSpUwefffYZhg8fDl9fXzkI2N/v0jp06BA6duwIvV6Pf//73/Dw8MCMGTPQpUsXbNq0Ce3atQNgDREJCQl46aWX0LZtW2RmZmLXrl3Ys2cPHnvssVKV93b+/PNPdOzYER4eHhg6dCjq1KmDv/76Cz///DMmT558V/tjt2HDBixcuBDx8fGoWrUqGjRogH79+mHJkiWYMWMGPD095XmXLVsGo9GI5557DoC1pefxxx/Hli1bMHToUERGRuLAgQP4z3/+g+PHj2PZsmVy3f3jH/9A8+bNMWnSJGi1Wpw8eRJbt24tU5npPiaISAghRFxcnLj1I9G5c2cBQHz11VdF5s/NzS0y7V//+pfw9vYWeXl58rSYmBhRu3Zt+fnp06cFAFGlShVx7do1efry5csFAPHzzz/L08aPH1+kTACEp6enOHnypDxt//79AoD44osv5Gl9+/YV3t7e4ty5c/K0EydOCI1GU2SdxSlu/xISEoQkSeLs2bMO+wdATJo0yWHeli1bilatWsnPly1bJgCIqVOnytNMJpPo2LGjACBmz55dYnkWLVokAIiNGzc6TM/KyhL+/v7i5ZdfdpielpYmDAaDPP369esCgPj4449L3E6TJk1E586dS5ynMABi/Pjx8vPo6Gjh6ekp/vrrL3na+fPnhZ+fn+jUqZM8rUWLFqJPnz63XW9py1ucTp06CT8/P4f3SQghLBaL/O9bj0u72x1zKpVKHDp0yGH62rVrixyzQgjRu3dvUbduXfn5Dz/8IFQqlfjtt98c5vvqq68EALF161YhhBD/+c9/BABx+fLl0u8sUTF4WoroDrRabbFN715eXvK/s7KycOXKFXTs2BG5ubk4evToHdfbv39/BAQEyM87duwIADh16tQdl+3WrRvq1asnP2/evDn0er28rNlsxq+//oro6GiEhobK89WvXx+9evW64/oBx/3LycnBlStX8PDDD0MIgb179xaZf9iwYQ7PO3bs6LAvq1atgkajkVtyAECtVmP48OGlKs/tJCUlIT09HQMGDMCVK1fkh1qtRrt27bBx40Z5fzw9PZGcnIzr16/f0zZvx2w2Y926dYiOjkbdunXl6SEhIRg4cCC2bNmCzMxMAIC/vz8OHTqEEydOFLuuspb38uXL2Lx5M1544QXUqlXL4bV7uaxA586d0bhxY4dpjz76KKpWrYoFCxbI065fv46kpCT0799fnrZo0SJERkYiIiLC4T169NFHAUB+j/z9/QFYT/mWtRM+EcA+N0R3VKNGDYcmd7tDhw6hX79+MBgM0Ov1qFatmtwZOSMj447rvfWHxx50SvNDduuy9uXty166dAk3btxA/fr1i8xX3LTipKSkIDY2FoGBgXI/ms6dOwMoun86na7I6a7C5QGAs2fPIiQkBL6+vg7zNWrUqFTluR17OHj00UdRrVo1h8e6detw6dIlANaQ+tFHH2H16tWoXr06OnXqhKlTpyItLe2etl/Y5cuXkZubW+w+RUZGwmKxIDU1FQAwadIkpKeno2HDhmjWrBnefPNN/Pnnn/L8ZS2vPVA2bdrUafsFAOHh4UWmaTQaPPXUU1i+fLncd2bJkiUoKChwCDcnTpzAoUOHirw/DRs2BAD5Perfvz+ioqLw0ksvoXr16njuueewcOFCBh26a+xzQ3QHhVsw7NLT09G5c2fo9XpMmjQJ9erVg06nw549e/DWW2+V6stYrVYXO10I4dJlS8NsNuOxxx7DtWvX8NZbbyEiIgI+Pj44d+4cYmNji+zf7crjDvay/PDDDwgODi7yeuHRZiNHjkTfvn2xbNkyrF27Fu+99x4SEhKwYcMGtGzZ0m1lBqz9iP766y8sX74c69atw9dff43//Oc/+Oqrr/DSSy+5vLy3a8W5tRO4XXGfAwB47rnnMGPGDKxevRrR0dFYuHAhIiIi0KJFC3kei8WCZs2aYdq0acWuIywsTN7G5s2bsXHjRvzyyy9Ys2YNFixYgEcffRTr1q1T9DijioXhhqgMkpOTcfXqVSxZsgSdOnWSp58+fVrBUt0UFBQEnU6HkydPFnmtuGm3OnDgAI4fP47vvvsOQ4YMkacXHslzt2rXro3169cjOzvbofXm2LFjpVr+dj/G9tNzQUFB6Nat2x3XU69ePbzxxht44403cOLECTzwwAP49NNP8eOPP5a4ndKoVq0avL29i92no0ePQqVSyT/kAOTRRs8//zyys7PRqVMnTJgwQQ43pSnvreynww4ePFhiWQMCAoodqXb27NnS7KqsU6dOCAkJwYIFC9ChQwds2LDBYVSWfR/279+Prl273rF+VSoVunbtiq5du2LatGmYMmUKxo4di40bN5bq/SUCeFqKqEzs/4Ms3FKSn5+P6dOnK1UkB2q1Gt26dcOyZctw/vx5efrJkyexevXqUi0POO6fEAL//e9/y1ym3r17w2QyITExUZ5mNpvxxRdflGp5Hx8fACjyg9yjRw/o9XpMmTIFBQUFRZa7fPkyAOv1ifLy8hxeq1evHvz8/ByGI/v4+JRqeHpx1Go1unfvjuXLlzsM17548SLmzZuHDh06QK/XA7COwivM19cX9evXl8tS2vLeqlq1aujUqRO+/fZbpKSkOLxW+P2sV68eMjIyHE6FXbhwAUuXLr2rfVapVHj66afx888/44cffoDJZHI4JQUAzz77LM6dO4dZs2YVWf7GjRvIyckBAFy7dq3I6w888AAAlLjPRLdiyw1RGTz88MMICAhATEwMRowYAUmS8MMPPzjttJAzTJgwAevWrUNUVBReeeUVmM1mfPnll2jatCn27dtX4rIRERGoV68eRo8ejXPnzkGv1+P//u//7qkjbt++fREVFYW3334bZ86cQePGjbFkyZJS9U8CrD9yarUaH330ETIyMqDVavHoo48iKCgIiYmJGDx4MB588EE899xzqFatGlJSUvDLL78gKioKX375JY4fP46uXbvi2WefRePGjaHRaLB06VJcvHhRHrIMAK1atUJiYiI++OAD1K9fH0FBQXLH19L44IMP5Gu1vPrqq9BoNJgxYwaMRiOmTp0qz9e4cWN06dIFrVq1QmBgIHbt2oXFixcjPj4eAEpd3uJ8/vnn6NChAx588EEMHToU4eHhOHPmDH755Rf5vX/uuefw1ltvoV+/fhgxYoQ8fL5hw4bYs2dPqfcXsPaV+eKLLzB+/Hg0a9asyFW9Bw8ejIULF2LYsGHYuHEjoqKiYDabcfToUSxcuBBr165F69atMWnSJGzevBl9+vRB7dq1cenSJUyfPh01a9ZEhw4d7qpMdJ9TbqAWUflyu6HgTZo0KXb+rVu3ioceekh4eXmJ0NBQ8e9//1seGlt4uPLthoIXN8QXtwwrvt2w3Li4uCLL1q5dW8TExDhMW79+vWjZsqXw9PQU9erVE19//bV44403hE6nu00t3HT48GHRrVs34evrK6pWrSpefvllech54WHbMTExwsfHp8jyxZX96tWrYvDgwUKv1wuDwSAGDx4s9u7dW6qh4EIIMWvWLFG3bl2hVquL1PPGjRtFjx49hMFgEDqdTtSrV0/ExsaKXbt2CSGEuHLlioiLixMRERHCx8dHGAwG0a5dO7Fw4UKHbaSlpYk+ffoIPz8/AeCOw8Jvfc+EEGLPnj2iR48ewtfXV3h7e4tHHnlE/P777w7zfPDBB6Jt27bC399feHl5iYiICDF58mSRn59/V+W9nYMHD4p+/foJf39/odPpRKNGjcR7773nMM+6detE06ZNhaenp2jUqJH48ccf7+qYs7NYLCIsLEwAEB988EGx8+Tn54uPPvpINGnSRGi1WhEQECBatWolJk6cKDIyMoQQ1uP1iSeeEKGhocLT01OEhoaKAQMGiOPHj5dqn4nsJCHK0X81icjloqOjSxyCTERU0bHPDVElduutEk6cOIFVq1aV6ZYCREQVBVtuiCqxkJAQxMbGom7dujh79iwSExNhNBqxd+9eNGjQQOniERG5BDsUE1ViPXv2xE8//YS0tDRotVq0b98eU6ZMYbAhokqNLTdERERUqbDPDREREVUqDDdERERUqdx3fW4sFgvOnz8PPz+/e7rMOhEREbmPEAJZWVkIDQ2FSlVy28x9F27Onz/vcG8XIiIiqjhSU1NRs2bNEue578KNn58fAGvl2O/xQkREROVbZmYmwsLC5N/xktx34cZ+Kkqv1zPcEBERVTCl6VLCDsVERERUqTDcEBERUaXCcENERESVyn3X54aIiCo3s9mMgoICpYtBZeDp6XnHYd6lwXBDRESVghACaWlpSE9PV7ooVEYqlQrh4eHw9PS8p/Uw3BARUaVgDzZBQUHw9vbmhVorGPtFdi9cuIBatWrd0/vHcENERBWe2WyWg02VKlWULg6VUbVq1XD+/HmYTCZ4eHiUeT3sUExERBWevY+Nt7e3wiWhe2E/HWU2m+9pPQw3RERUafBUVMXmrPeP4YaIiIgqFYYbIiKiSqROnTr47LPPFF+HkhhuiIiIFCBJUomPCRMmlGm9O3fuxNChQ51b2AqGo6WcxGS24GKWERaLQFggO7QREVHJLly4IP97wYIFGDduHI4dOyZP8/X1lf8thIDZbIZGc+ef7WrVqjm3oBUQW26c5Ep2PqI+3IAunyQrXRQiIqoAgoOD5YfBYIAkSfLzo0ePws/PD6tXr0arVq2g1WqxZcsW/PXXX3jiiSdQvXp1+Pr6ok2bNvj1118d1nvrKSVJkvD111+jX79+8Pb2RoMGDbBixYq7KmtKSgqeeOIJ+Pr6Qq/X49lnn8XFixfl1/fv349HHnkEfn5+0Ov1aNWqFXbt2gUAOHv2LPr27YuAgAD4+PigSZMmWLVqVdkrrhTYcuMkapW1h7fZIiCEYI99IiKFCSFwo+DehhSXhZeH2mm/AW+//TY++eQT1K1bFwEBAUhNTUXv3r0xefJkaLVafP/99+jbty+OHTuGWrVq3XY9EydOxNSpU/Hxxx/jiy++wKBBg3D27FkEBgbesQwWi0UONps2bYLJZEJcXBz69++P5ORkAMCgQYPQsmVLJCYmQq1WY9++ffJ1auLi4pCfn4/NmzfDx8cHhw8fdmiVcgWGGyexhxsAsAhAzWxDRKSoGwVmNB631u3bPTypB7w9nfPzOmnSJDz22GPy88DAQLRo0UJ+/v7772Pp0qVYsWIF4uPjb7ue2NhYDBgwAAAwZcoUfP7559ixYwd69ux5xzKsX78eBw4cwOnTpxEWFgYA+P7779GkSRPs3LkTbdq0QUpKCt58801EREQAABo0aCAvn5KSgqeeegrNmjUDANStW/cuaqBseFrKSQqHG7NFKFgSIiKqLFq3bu3wPDs7G6NHj0ZkZCT8/f3h6+uLI0eOICUlpcT1NG/eXP63j48P9Ho9Ll26VKoyHDlyBGFhYXKwAYDGjRvD398fR44cAQCMGjUKL730Erp164YPP/wQf/31lzzviBEj8MEHHyAqKgrjx4/Hn3/+Wart3gu23DgJww0RUfni5aHG4Uk9FNmus/j4+Dg8Hz16NJKSkvDJJ5+gfv368PLywtNPP438/PwS13PrrQwkSYLFYnFaOSdMmICBAwfil19+werVqzF+/HjMnz8f/fr1w0svvYQePXrgl19+wbp165CQkIBPP/0Uw4cPd9r2b8Vw4ySawuFGMNwQESlNkiSnnR4qL7Zu3YrY2Fj069cPgLUl58yZMy7dZmRkJFJTU5Gamiq33hw+fBjp6elo3LixPF/Dhg3RsGFDvP766xgwYABmz54tlzMsLAzDhg3DsGHDMGbMGMyaNcul4YanpZxEVajzmNnMcENERM7XoEEDLFmyBPv27cP+/fsxcOBAp7bAFKdbt25o1qwZBg0ahD179mDHjh0YMmQIOnfujNatW+PGjRuIj49HcnIyzp49i61bt2Lnzp2IjIwEAIwcORJr167F6dOnsWfPHmzcuFF+zVUYbpyELTdERORq06ZNQ0BAAB5++GH07dsXPXr0wIMPPujSbUqShOXLlyMgIACdOnVCt27dULduXSxYsAAAoFarcfXqVQwZMgQNGzbEs88+i169emHixIkArDfBjIuLQ2RkJHr27ImGDRti+vTpri2zEPfXL3FmZiYMBgMyMjKg1+uduu46b/8CANgxtiuC/HROXTcREd1eXl4eTp8+jfDwcOh0/P6tqEp6H+/m95stN05kb71xcQshERERlYDhxolUtnBjYrohIiJSDMONE7HlhoiISHmKhpuEhAS0adMGfn5+CAoKQnR0tMNNw27ns88+Q6NGjeDl5YWwsDC8/vrryMvLc0OJS6aW2HJDRESkNEXDzaZNmxAXF4ft27cjKSkJBQUF6N69O3Jycm67zLx58/D2229j/PjxOHLkCL755hssWLAA77zzjhtLXjy17Z4LlvurjzYREVG5oujVjdasWePwfM6cOQgKCsLu3bvRqVOnYpf5/fffERUVhYEDBwKw3v10wIAB+OOPP1xe3ju52XLDcENERKSUctXnJiMjAwBKvEvpww8/jN27d2PHjh0AgFOnTmHVqlXo3bt3sfMbjUZkZmY6PFyl8J3BiYiISBnl5rrUFosFI0eORFRUFJo2bXrb+QYOHIgrV66gQ4cOEELAZDJh2LBhtz0tlZCQIF9IyNUYboiIiJRXblpu4uLicPDgQcyfP7/E+ZKTkzFlyhRMnz4de/bswZIlS/DLL7/g/fffL3b+MWPGICMjQ36kpqa6ovgAGG6IiIjKg3LRchMfH4+VK1di8+bNqFmzZonzvvfeexg8eDBeeuklAECzZs2Qk5ODoUOHYuzYsVCpHPOaVquFVqt1WdkLY7ghIqKK4syZMwgPD8fevXvxwAMPKF0cp1K05UYIgfj4eCxduhQbNmxAeHj4HZfJzc0tEmDUarW8PiUx3BARUWlJklTiY8KECfe07mXLljmtrBWNoi03cXFxmDdvHpYvXw4/Pz+kpaUBAAwGA7y8vAAAQ4YMQY0aNZCQkAAA6Nu3L6ZNm4aWLVuiXbt2OHnyJN577z307dtXDjlKsY+WYrghIqI7uXDhgvzvBQsWYNy4cQ7XevP19VWiWJWCoi03iYmJyMjIQJcuXRASEiI/7HcaBYCUlBSHA+Ddd9/FG2+8gXfffReNGzfGiy++iB49emDGjBlK7IIDueWG17khIqI7CA4Olh8GgwGSJDlMmz9/PiIjI6HT6RAREeFwJ+38/HzEx8cjJCQEOp0OtWvXlhsB6tSpAwDo168fJEmSn5fGpk2b0LZtW2i1WoSEhODtt9+GyWSSX1+8eDGaNWsGLy8vVKlSBd26dZOvTZecnIy2bdvCx8cH/v7+iIqKwtmzZ++9ospA0Zab0pxGSk5Odniu0Wgwfvx4jB8/3kWlKju1ite5ISIqN4QACnLdv10Pb8DWkl9Wc+fOxbhx4/Dll1+iZcuW2Lt3L15++WX4+PggJiYGn3/+OVasWIGFCxeiVq1aSE1NlQfM7Ny5E0FBQZg9ezZ69uxZ6rMa586dQ+/evREbG4vvv/8eR48excsvvwydTocJEybgwoULGDBgAKZOnYp+/fohKysLv/32mzxyOTo6Gi+//DJ++ukn5OfnY8eOHZDusR7Kqlx0KK4sbt5biuGGiEhxBbnAlFD3b/ed84Cnzz2tYvz48fj000/x5JNPAgDCw8Nx+PBhzJgxAzExMUhJSUGDBg3QoUMHSJKE2rVry8tWq1YNAODv74/g4OBSb3P69OkICwvDl19+CUmSEBERgfPnz+Ott97CuHHjcOHCBZhMJjz55JPy9po1awYAuHbtGjIyMvCPf/wD9erVAwBERkbeUx3ci3IzFLwyULHlhoiI7lFOTg7++usvvPjii/D19ZUfH3zwAf766y8AQGxsLPbt24dGjRphxIgRWLdu3T1v98iRI2jfvr1Da0tUVBSys7Px999/o0WLFujatSuaNWuGZ555BrNmzcL169cBWC++Gxsbix49eqBv377473//69ClxN3YcuNE9g7FbLkhIioHPLytrShKbPceZGdnAwBmzZqFdu3aObxmP8X04IMP4vTp01i9ejV+/fVXPPvss+jWrRsWL158T9suiVqtRlJSEn7//XesW7cOX3zxBcaOHYs//vgD4eHhmD17NkaMGIE1a9ZgwYIFePfdd5GUlISHHnrIZWW6HYYbJ2KfGyKickSS7vn0kBKqV6+O0NBQnDp1CoMGDbrtfHq9Hv3790f//v3x9NNPo2fPnrh27RoCAwPh4eEBs9l8V9uNjIzE//3f/0EIIbfebN26FX5+fvI16CRJQlRUFKKiojBu3DjUrl0bS5cuxahRowAALVu2RMuWLTFmzBi0b98e8+bNY7ip6OzhhncFJyKiezFx4kSMGDECBoMBPXv2hNFoxK5du3D9+nWMGjUK06ZNQ0hICFq2bAmVSoVFixYhODgY/v7+AKwjptavX4+oqChotVoEBATccZuvvvoqPvvsMwwfPhzx8fE4duwYxo8fj1GjRkGlUuGPP/7A+vXr0b17dwQFBeGPP/7A5cuXERkZidOnT2PmzJl4/PHHERoaimPHjuHEiRMYMmSIi2uqeAw3TiS33JgZboiIqOxeeukleHt74+OPP8abb74JHx8fNGvWDCNHjgQA+Pn5YerUqThx4gTUajXatGmDVatWyRe5/fTTTzFq1CjMmjULNWrUwJkzZ+64zRo1amDVqlV488030aJFCwQGBuLFF1/Eu+++C8DaUrR582Z89tlnyMzMRO3atfHpp5+iV69euHjxIo4ePYrvvvsOV69eRUhICOLi4vCvf/3LVVVUIkkofVlfN8vMzITBYEBGRgb0er1T1x07eweSj13G1Keb49nWYU5dNxER3V5eXh5Onz6N8PBw6HQ6pYtDZVTS+3g3v98cLeVEHApORESkPIYbJ1JJ7FBMRESkNIYbJ9Ko2aGYiIhIaQw3TiS33LBDMRERkWIYbpxIw6HgRESKus/GyFQ6znr/GG6ciLdfICJShoeHBwAgN1eBG2WS0+Tn5wNAqW/2eTu8zo0T2VtuzAw3RERupVar4e/vj0uXLgEAvL29FbsjNZWNxWLB5cuX4e3tDY3m3uIJw40TqRluiIgUY78Dtj3gUMWjUqlQq1atew6mDDdOxHBDRKQcSZIQEhKCoKAgFBQUKF0cKgNPT0/5Ksv3guHGiex3BWe4ISJSjlqtvuc+G1SxsUOxE6ltadPM3vpERESKYbhxIrWtNtlyQ0REpByGGyeSW24YboiIiBTDcONEbLkhIiJSHsONE7HlhoiISHkMN06k5l3BiYiIFMdw40TyXcEZboiIiBTDcONEKrbcEBERKY7hxol4V3AiIiLlMdw4Ee8KTkREpDyGGyeSW24YboiIiBTDcONEN1tuLAqXhIiI6P7FcONEGvmu4AoXhIiI6D7GcONEN+8KznRDRESkFIYbJ1LbW27Y5YaIiEgxioabhIQEtGnTBn5+fggKCkJ0dDSOHTt2x+XS09MRFxeHkJAQaLVaNGzYEKtWrXJDiUsmhxu23BARESlGo+TGN23ahLi4OLRp0wYmkwnvvPMOunfvjsOHD8PHx6fYZfLz8/HYY48hKCgIixcvRo0aNXD27Fn4+/u7t/DFuBlu2HRDRESkFEXDzZo1axyez5kzB0FBQdi9ezc6depU7DLffvstrl27ht9//x0eHh4AgDp16ri6qKXCcENERKS8ctXnJiMjAwAQGBh423lWrFiB9u3bIy4uDtWrV0fTpk0xZcoUmM1mdxXzthhuiIiIlKdoy01hFosFI0eORFRUFJo2bXrb+U6dOoUNGzZg0KBBWLVqFU6ePIlXX30VBQUFGD9+fJH5jUYjjEaj/DwzM9Ml5QcKj5ZiuCEiIlJKuQk3cXFxOHjwILZs2VLifBaLBUFBQZg5cybUajVatWqFc+fO4eOPPy423CQkJGDixImuKraDm6OlGG6IiIiUUi5OS8XHx2PlypXYuHEjatasWeK8ISEhaNiwIdRqtTwtMjISaWlpyM/PLzL/mDFjkJGRIT9SU1OdXn47NS/iR0REpDhFw40QAvHx8Vi6dCk2bNiA8PDwOy4TFRWFkydPwlJouPXx48cREhICT0/PIvNrtVro9XqHh6twKDgREZHyFA03cXFx+PHHHzFv3jz4+fkhLS0NaWlpuHHjhjzPkCFDMGbMGPn5K6+8gmvXruG1117D8ePH8csvv2DKlCmIi4tTYhccsEMxERGR8hTtc5OYmAgA6NKli8P02bNnIzY2FgCQkpIClepmBgsLC8PatWvx+uuvo3nz5qhRowZee+01vPXWW+4q9m0x3BARESlP0XAjStHxNjk5uci09u3bY/v27S4o0b1hh2IiIiLllYsOxZWFPBScN5ciIiJSDMONE7HlhoiISHkMN07EPjdERETKY7hxIg3DDRERkeIYbpxIZQs3JoYbIiIixTDcOJG95cbCcENERKQYhhsnUklsuSEiIlIaw40TadS2lhuOliIiIlIMw40TqdlyQ0REpDiGGyeyDwUXgv1uiIiIlMJw40T2cAPwQn5ERERKYbhxIodww5YbIiIiRTDcOBHDDRERkfIYbpyIp6WIiIiUx3DjRPbRUgDvDE5ERKQUhhsnYssNERGR8hhunEiSJNjzDfvcEBERKYPhxsk0KmuVMtwQEREpg+HGyWzZhuGGiIhIIQw3TsaWGyIiImUx3DiZvc8N7y9FRESkDIYbJ9OorVXKO4MTEREpg+HGyVT2O4PzOjdERESKYLhxMo3tvBRbboiIiJTBcONk9gv5sc8NERGRMhhunMwebjhaioiISBkMN07GcENERKQshhsnY7ghIiJSFsONk9nvDM4OxURERMpguHEyFTsUExERKYrhxsnkoeAMN0RERIpguHEyttwQEREpi+HGyTTsUExERKQoRcNNQkIC2rRpAz8/PwQFBSE6OhrHjh0r9fLz58+HJEmIjo52XSHvkr1DMcMNERGRMhQNN5s2bUJcXBy2b9+OpKQkFBQUoHv37sjJybnjsmfOnMHo0aPRsWNHN5S09OSh4BwtRUREpAiNkhtfs2aNw/M5c+YgKCgIu3fvRqdOnW67nNlsxqBBgzBx4kT89ttvSE9Pd3FJS+/mdW4sCpeEiIjo/lSu+txkZGQAAAIDA0ucb9KkSQgKCsKLL754x3UajUZkZmY6PFzpZrhx6WaIiIjoNspNuLFYLBg5ciSioqLQtGnT2863ZcsWfPPNN5g1a1ap1puQkACDwSA/wsLCnFXkYrHlhoiISFnlJtzExcXh4MGDmD9//m3nycrKwuDBgzFr1ixUrVq1VOsdM2YMMjIy5EdqaqqzilwsttwQEREpS9E+N3bx8fFYuXIlNm/ejJo1a952vr/++gtnzpxB37595WkWWwuJRqPBsWPHUK9ePYdltFottFqtawpejJujpZhuiIiIlKBouBFCYPjw4Vi6dCmSk5MRHh5e4vwRERE4cOCAw7R3330XWVlZ+O9//+vyU06loVZzKDgREZGSFA03cXFxmDdvHpYvXw4/Pz+kpaUBAAwGA7y8vAAAQ4YMQY0aNZCQkACdTlekP46/vz8AlNhPx53sLTe8QjEREZEyFA03iYmJAIAuXbo4TJ89ezZiY2MBACkpKVCpyk3XoDuS7y3F69wQEREpQvHTUneSnJxc4utz5sxxTmGchPeWIiIiUlbFaRKpIHhXcCIiImUx3DgZW26IiIiUxXDjZGy5ISIiUhbDjZOpOFqKiIhIUQw3TqbhXcGJiIgUxXDjZPLtF8wMN0REREpguHEyNVtuiIiIFMVw42Q3b5zJcENERKQEhhsnY7ghIiJSFsONk928KzjDDRERkRIYbpyMdwUnIiJSFsONk7HlhoiISFkMN07G0VJERETKYrhxMjXvLUVERKQohhsn472liIiIlMVw42QqDgUnIiJSFMONk2kYboiIiBTFcONk9ruCs0MxERGRMhhunEzD69wQEREpiuHGyVS8zg0REZGiGG6cjEPBiYiIlMVw42QcCk5ERKQshhsns5+WYssNERGRMhhunMzeodjC0VJERESKYLhxMrnlxsxwQ0REpASGGyfTqKxVypYbIiIiZTDcOJkt27DPDRERkUIYbpxMbrlhuCEiIlIEw42TqdlyQ0REpCiGGydT21pueIViIiIiZTDcOJmat18gIiJSFMONk9lvv8C7ghMRESmD4cbJ5HDDlhsiIiJFKBpuEhIS0KZNG/j5+SEoKAjR0dE4duxYicvMmjULHTt2REBAAAICAtCtWzfs2LHDTSW+M4YbIiIiZSkabjZt2oS4uDhs374dSUlJKCgoQPfu3ZGTk3PbZZKTkzFgwABs3LgR27ZtQ1hYGLp3745z5865seS3x3BDRESkLEmI8tM55PLlywgKCsKmTZvQqVOnUi1jNpsREBCAL7/8EkOGDLnj/JmZmTAYDMjIyIBer7/XIheRei0XHaduhJeHGkfe7+n09RMREd2P7ub3W+OmMpVKRkYGACAwMLDUy+Tm5qKgoOC2yxiNRhiNRvl5ZmbmvRXyDlRsuSEiIlJUuelQbLFYMHLkSERFRaFp06alXu6tt95CaGgounXrVuzrCQkJMBgM8iMsLMxZRS6WhqOliIiIFFVuwk1cXBwOHjyI+fPnl3qZDz/8EPPnz8fSpUuh0+mKnWfMmDHIyMiQH6mpqc4qcrFUha5zU47O+BEREd03ysVpqfj4eKxcuRKbN29GzZo1S7XMJ598gg8//BC//vormjdvftv5tFottFqts4p6R/aWGwCwCEAtlTAzEREROZ2i4UYIgeHDh2Pp0qVITk5GeHh4qZabOnUqJk+ejLVr16J169YuLuXdURUKNyaLBWqVWsHSEBER3X8UDTdxcXGYN28eli9fDj8/P6SlpQEADAYDvLy8AABDhgxBjRo1kJCQAAD46KOPMG7cOMybNw916tSRl/H19YWvr68yO1KIQ8uNRcGCEBER3acU7XOTmJiIjIwMdOnSBSEhIfJjwYIF8jwpKSm4cOGCwzL5+fl4+umnHZb55JNPlNiFItS3tNwQERGReyl+WupOkpOTHZ6fOXPGNYVxEjVbboiIiBRVbkZLVRb2u4IDbLkhIiJSAsONk6lUEuz5hte6ISIicj+GGxewt96w4YaIiMj9yhRuUlNT8ffff8vPd+zYgZEjR2LmzJlOK1hFZu93w9NSRERE7lemcDNw4EBs3LgRAJCWlobHHnsMO3bswNixYzFp0iSnFrAisocbZhsiIiL3K1O4OXjwINq2bQsAWLhwIZo2bYrff/8dc+fOxZw5c5xZvgqJLTdERETKKVO4KSgokG9p8Ouvv+Lxxx8HAERERDhck+Z+JbfcsEMxERGR25Up3DRp0gRfffUVfvvtNyQlJaFnz54AgPPnz6NKlSpOLWBFpJFbbhhuiIiI3K1M4eajjz7CjBkz0KVLFwwYMAAtWrQAAKxYsUI+XXU/K3xncCIiInKvMl2huEuXLrhy5QoyMzMREBAgTx86dCi8vb2dVriKyt5yw3BDRETkfmVqublx4waMRqMcbM6ePYvPPvsMx44dQ1BQkFMLWBGpGG6IiIgUU6Zw88QTT+D7778HAKSnp6Ndu3b49NNPER0djcTERKcWsCJSM9wQEREppkzhZs+ePejYsSMAYPHixahevTrOnj2L77//Hp9//rlTC1gRMdwQEREpp0zhJjc3F35+fgCAdevW4cknn4RKpcJDDz2Es2fPOrWAFZGaHYqJiIgUU6ZwU79+fSxbtgypqalYu3YtunfvDgC4dOkS9Hq9UwtYEcktN7zODRERkduVKdyMGzcOo0ePRp06ddC2bVu0b98egLUVp2XLlk4tYEWk5nVuiIiIFFOmoeBPP/00OnTogAsXLsjXuAGArl27ol+/fk4rXEWlke8txXBDRETkbmUKNwAQHByM4OBg+e7gNWvW5AX8bFRsuSEiIlJMmU5LWSwWTJo0CQaDAbVr10bt2rXh7++P999/HxbeLJItN0RERAoqU8vN2LFj8c033+DDDz9EVFQUAGDLli2YMGEC8vLyMHnyZKcWsqKx336BLTdERETuV6Zw89133+Hrr7+W7wYOAM2bN0eNGjXw6quv3vfhRqPmXcGJiIiUUqbTUteuXUNERESR6REREbh27do9F6qik1tuzAw3RERE7lamcNOiRQt8+eWXRaZ/+eWXaN68+T0XqqLT8Do3REREiinTaampU6eiT58++PXXX+Vr3Gzbtg2pqalYtWqVUwtYEfH2C0RERMopU8tN586dcfz4cfTr1w/p6elIT0/Hk08+iUOHDuGHH35wdhkrHIYbIiIi5ZT5OjehoaFFOg7v378f33zzDWbOnHnPBavIGG6IiIiUU6aWGyqZWmWtVoYbIiIi92O4cQHbSHCGGyIiIgUw3LiA3HLD0VJERERud1d9bp588skSX09PT7+XslQaaltkZMsNERGR+91VuDEYDHd8fciQIfdUoMqAfW6IiIiUc1fhZvbs2a4qR6Vib7nhvaWIiIjcT9E+NwkJCWjTpg38/PwQFBSE6OhoHDt27I7LLVq0CBEREdDpdGjWrFm5u3CgxtZyw7uCExERuZ+i4WbTpk2Ii4vD9u3bkZSUhIKCAnTv3h05OTm3Xeb333/HgAED8OKLL2Lv3r2Ijo5GdHQ0Dh486MaSl8x+byl2KCYiInI/SYjy8wt8+fJlBAUFYdOmTejUqVOx8/Tv3x85OTlYuXKlPO2hhx7CAw88gK+++uqO28jMzITBYEBGRgb0er3Tyl7YlFVHMHPzKQztVBfv9I50yTaIiIjuJ3fz+12uhoJnZGQAAAIDA287z7Zt29CtWzeHaT169MC2bdtcWra7Ibfc8LQUERGR25X59gvOZrFYMHLkSERFRaFp06a3nS8tLQ3Vq1d3mFa9enWkpaUVO7/RaITRaJSfZ2ZmOqfAJdDw9gtERESKKTctN3FxcTh48CDmz5/v1PUmJCTAYDDIj7CwMKeuvzgqhhsiIiLFlItwEx8fj5UrV2Ljxo2oWbNmifMGBwfj4sWLDtMuXryI4ODgYucfM2YMMjIy5EdqaqrTyn079pYbDgUnIiJyP0XDjRAC8fHxWLp0KTZs2IDw8PA7LtO+fXusX7/eYVpSUhLat29f7PxarRZ6vd7h4Wr2u4JzKDgREZH7KdrnJi4uDvPmzcPy5cvh5+cn95sxGAzw8vICAAwZMgQ1atRAQkICAOC1115D586d8emnn6JPnz6YP38+du3ahZkzZyq2H7dSs+WGiIhIMYq23CQmJiIjIwNdunRBSEiI/FiwYIE8T0pKCi5cuCA/f/jhhzFv3jzMnDkTLVq0wOLFi7Fs2bISOyG7m9o2WspSfkbZExER3TcUbbkpzSV2kpOTi0x75pln8Mwzz7igRM7BlhsiIiLllIsOxZUN+9wQEREph+HGBW623FgULgkREdH9h+HGBdTydW4ULggREdF9iOHGBdTy7ReYboiIiNyN4cYF5JYbdrkhIiJyO4YbF7h5WootN0RERO7GcOMCat5bioiISDEMNy7AcENERKQchhsXYLghIiJSDsONC9wcLcVwQ0RE5G4MNy6gVttHSzHcEBERuRvDjQvYW25MHAtORETkdgw3LqBR8a7gRERESmG4cQEV7wpORESkGIYbF9DwruBERESKYbhxAbbcEBERKYfhxgXYckNERKQchhsXUElsuSEiIlIKw40LaNQcLUVERKQUhhsXULPlhoiISDEMNy7Ae0sREREph+HGBRhuiIiIlMNw4wIMN0RERMphuHEBNW+/QEREpBiGGxdQ8yJ+REREimG4cQH7aCkheCE/IiIid2O4cQGN6ma1mnlqioiIyK0YblygULZhp2IiIiI3Y7hxAYeWG4YbIiIit2K4cQGHlhueliIiInIrhhsXcGi5MTPcEBERuRPDjQvYRoIDYMsNERGRuzHcuIAkSbxKMRERkUIUDTebN29G3759ERoaCkmSsGzZsjsuM3fuXLRo0QLe3t4ICQnBCy+8gKtXr7q+sHfJfq0bhhsiIiL3UjTc5OTkoEWLFvjf//5Xqvm3bt2KIUOG4MUXX8ShQ4ewaNEi7NixAy+//LKLS3r32HJDRESkDI2SG+/Vqxd69epV6vm3bduGOnXqYMSIEQCA8PBw/Otf/8JHH33kqiKWGcMNERGRMipUn5v27dsjNTUVq1atghACFy9exOLFi9G7d+/bLmM0GpGZmenwcAfeX4qIiEgZFSrcREVFYe7cuejfvz88PT0RHBwMg8FQ4mmthIQEGAwG+REWFuaWsvLO4ERERMqoUOHm8OHDeO211zBu3Djs3r0ba9aswZkzZzBs2LDbLjNmzBhkZGTIj9TUVLeUVWXrUGzidW6IiIjcStE+N3crISEBUVFRePPNNwEAzZs3h4+PDzp27IgPPvgAISEhRZbRarXQarXuLio0bLkhIiJSRIVqucnNzYVK5VhktVoNABDlLESwzw0REZEyFA032dnZ2LdvH/bt2wcAOH36NPbt24eUlBQA1lNKQ4YMkefv27cvlixZgsTERJw6dQpbt27FiBEj0LZtW4SGhiqxC7fF0VJERETKUPS01K5du/DII4/Iz0eNGgUAiImJwZw5c3DhwgU56ABAbGwssrKy8OWXX+KNN96Av78/Hn30UQ4FJyIiIpkkytv5HBfLzMyEwWBARkYG9Hq9y7bTbdomnLyUjZ9efgjt61Vx2XaIiIjuB3fz+12h+txUJLz9AhERkTIYblxEPi11fzWMERERKY7hxkVu9rmxKFwSIiKi+wvDjYvcDDcKF4SIiOg+w3DjImy5ISIiUgbDjYuw5YaIiEgZDDcuYh8tZWLLDRERkVsx3LiIRs17SxERESmB4cZFeFdwIiIiZTDcuAjvCk5ERKQMhhsXUbFDMRERkSIYblxEw6HgREREimC4cREV7wpORESkCIYbF7G33JgYboiIiNyK4cZF7Ne5YYdiIiIi92K4cRE1W26IiIgUwXDjIvZwY2G4ISIiciuGGxdhyw0REZEyGG5chC03REREymC4cRG23BARESmD4cZF7KOlzBwtRURE5FYMNy6itt0V3MwbZxIREbkVw42LsOWGiIhIGQw3LqLh7ReIiIgUwXDjIry3FBERkTIYblyELTdERETKYLhxEbbcEBERKYPhxkXYckNERKQMhhsXUXG0FBERkSIYblyEVygmIiJSBsONi2h4bykiIiJFMNy4iIotN0RERIpQNNxs3rwZffv2RWhoKCRJwrJly+64jNFoxNixY1G7dm1otVrUqVMH3377resLe5fYckNERKQMjZIbz8nJQYsWLfDCCy/gySefLNUyzz77LC5evIhvvvkG9evXx4ULF2CxWFxc0rtn71DMlhsiIiL3UjTc9OrVC7169Sr1/GvWrMGmTZtw6tQpBAYGAgDq1KnjotLdG43txpkWjpYiIiJyqwrV52bFihVo3bo1pk6diho1aqBhw4YYPXo0bty4oXTRipBbbnhXcCIiIrdStOXmbp06dQpbtmyBTqfD0qVLceXKFbz66qu4evUqZs+eXewyRqMRRqNRfp6ZmemWsmpU1tzI69wQERG5V4VqubFYLJAkCXPnzkXbtm3Ru3dvTJs2Dd99991tW28SEhJgMBjkR1hYmFvKqrbVLK9QTERE5F4VKtyEhISgRo0aMBgM8rTIyEgIIfD3338Xu8yYMWOQkZEhP1JTU91SVrW95YbhhoiIyK0qVLiJiorC+fPnkZ2dLU87fvw4VCoVatasWewyWq0Wer3e4eEObLkhIiJShqLhJjs7G/v27cO+ffsAAKdPn8a+ffuQkpICwNrqMmTIEHn+gQMHokqVKnj++edx+PBhbN68GW+++SZeeOEFeHl5KbELt8WWGyIiImUoGm527dqFli1bomXLlgCAUaNGoWXLlhg3bhwA4MKFC3LQAQBfX18kJSUhPT0drVu3xqBBg9C3b198/vnnipS/JGqJQ8GJiIiUoOhoqS5dukCU8OM/Z86cItMiIiKQlJTkwlI5B2+cSUREpIwK1eemIlHz9gtERESKYLhxEbbcEBERKYPhxkXs4YYdiomIiNyL4cZFNAw3REREimC4cRH7vaV4+wUiIiL3YrhxEftdwdlyQ0RE5F4MNy4it9ww3BAREbkVw42LsM8NERGRMhhuXISjpYiIiJTBcOMiDDdERETKYLhxETnccLQUERGRWzHcuEjhlpuS7p9FREREzsVw4yL2u4IDAM9MERERuQ/DjYuo1TfDjcliUbAkRERE9xeGGxdxaLlhtiEiInIbhhsXsfe5AdhyQ0RE5E4MNy5SONww2xAREbkPw42LFD4txZYbIiIi92G4cRGVSoI93/BaN0RERO7DcONCat48k4iIyO0YblyIt2AgIiJyP4YbF2K4ISIicj+GGxdiuCEiInI/hhsXYrghIiJyP4YbF9LwzuBERERux3DjQirbaCmTmeGGiIjIXRhuXMjecmNhyw0REZHbMNy4kMoWbkzsc0NEROQ2DDcuJLfcMNwQERG5DcONC6k4WoqIiMjtGG5cSMNwQ0RE5HYMNy5kHy3FoeBERETuw3DjQho1OxQTERG5m6LhZvPmzejbty9CQ0MhSRKWLVtW6mW3bt0KjUaDBx54wGXlu1f2u4KzQzEREZH7KBpucnJy0KJFC/zvf/+7q+XS09MxZMgQdO3a1UUlcw41h4ITERG5nUbJjffq1Qu9evW66+WGDRuGgQMHQq1W31Vrj7upORSciIjI7Spcn5vZs2fj1KlTGD9+fKnmNxqNyMzMdHi4C1tuiIiI3K9ChZsTJ07g7bffxo8//giNpnSNTgkJCTAYDPIjLCzMxaW8Sc3bLxAREbldhQk3ZrMZAwcOxMSJE9GwYcNSLzdmzBhkZGTIj9TUVBeW0pFaZa1e3jiTiIjIfRTtc3M3srKysGvXLuzduxfx8fEAAIvFAiEENBoN1q1bh0cffbTIclqtFlqt1t3FBQDYRoLzOjdERERuVGHCjV6vx4EDBxymTZ8+HRs2bMDixYsRHh6uUMluz95ywysUExERuY+i4SY7OxsnT56Un58+fRr79u1DYGAgatWqhTFjxuDcuXP4/vvvoVKp0LRpU4flg4KCoNPpikwvL9S2k34MN0RERO6jaLjZtWsXHnnkEfn5qFGjAAAxMTGYM2cOLly4gJSUFKWKd880bLkhIiJyO0mI+6tDSGZmJgwGAzIyMqDX6126reE/7cXP+89j3D8a44UO5e+0GRERUUVxN7/fFWa0VIVgsQB5N6+jw7uCExERuR/DjbNk/A18/ziwKBawNYbxruBERETux3DjLPk5wN87gb/WAzu/BsCWGyIiIiUw3DhLtUbAY5Os/173HnDlBFQMN0RERG7HcONMbV4G6j4CmG4AS16Gp2QGwHtLERERuRPDjTOpVED0dEDnD5zfi0cufgeAdwUnIiJypwpzheIKQx8K/GMasPgFdEr7Di2l2th5JhAzN/8FD7UKGtuV/YwFZhhNFvkvJGsfHbUkyVc2zjebkW+yWB9mCwBAkiSoJGtnZZUkwVOjgodagodaBQ+1CnkFZmTcKEB6bgEybhTgRr4Zei8PBPp4IMDbEwE+ntBqVLAIQAgBixAwWQSMBRYYTRbk2cojhICnRgVPtcr6V6OCSpIgAZAkezkkaFQSNGoJGrUKGpWEArMFxoKb6zFZrOvRFnqYLAK5+WbcyDcjN9+MfLMZnmq1vB2tbbtq+7pVKnk7apX1uVolQQiBPJMZebbt5eabcSXbiIuZebiYaf1rMgvUCPBCWIAXagZ4o2aAFzzUKpgt1v02WyywCMj7qvWw/rUIoMBsgcliQYFJIN9sgUUIWCwCZmENrBYhYBYCQn5e/CFh61cO2x+o1RI8VCq53lSS9dSl/WGyCEiwvsf2ugZg27Z1HotFQKWy1r9aZX3/JQnIN1kcjiuLELb13DxuJOlmWeTCFSaEfCzkFViQZzLDLL+Paof3Uuuhhs5DDZ2HCh4qFQosFhSYLCgwW+vMXjZP2zGqUklyGa1/ra2b1nlsx5ltnrwCM4wFZuSZrOsxeHnID73OA5JkvSmt2WJ7D2zvBQAI3Dy+C8zWeQrMFmsdC+FQ3/m2Y9ZoK4/ZIuDtqYGvVg0frQY+Wg1UkgST2YICi4DJth4PW3ntf+3voxCQt2N/L+x1abF/ruzHm21/Vbb3R5I/Y/bngARrfdwoMFsf+WYUmC3Qaqz1bq9/4Ob7n2+2Hrdmi8V2nN88ruzbt9e5gPXYLVx/hb9TPNSS/H2jUt0cKGEvq/0wKrwP8mAKez0La1lUkmT9fGtU8FSroVZJyDGakGUsQFaeCVl5JpjMAh4ax8+IuOWzB8Dhe8f+ObAf34W3b/+cF5it753JNs1ktkAIwFurgY+n9b321WpgtgjkFphxI9+EHKMZ+WYLPNQq6DxuHv+SZP1+sH83FJgtDt+p9u8CtSTZ3l9rmUwWgXyTdf6CIseR9btNkqxjUuzHsCRJ8FRLhY4btTyPRQh5PpPZ8diWJNh+T+zfnSr5ufVhG+Bb6DvUaLJAJUGeR6O6+d5Lhb4/zHIdCpgslmK7Xnh7qtG/Ta3ivxTdgOHGFZo+BRxbA9WBhZjmMR2fnL2KEyl58EEevJEHAMiCN7KEN7LhhRzooEUBfJAHH+kGfJEHFSzIhpc8T5bwgpdkhAE58Jdy4Icc+Eg3YIEEARUsQoIREnRSPgJhhC9uwEfKgxeMEJAgINnmlZANL1wRBlwWBlwW/rgOP6hhhhYF0CEfWqkA3jDCG3nwkfJs5bL9ta3XB3mwQMLfohr+FtVwRlTD36IqjPCE9WNp/QHVogBVpQxUk9IRgHRUkzJhhAaXhT9yhD8y4Y/rwhcSADUsUMMMDSwwQ4V8aGCEB4zCEwVQwwNmeEoF8EQBPGGCDvnyPvpIedChAF7CC34wIF/oIYQfsuCN7CwLjqUInIAFKlvZLNafEAgAPshDfdU5NJDOob50DnVVF5ArtDgtQvC3CMEpEYK/RTXAVkYNzFDZatNer7DV7a0fcS8YUUXKQiAyUUXKhF7KQY7wwjX44arQ45rQIwM+MAoPGOGBfHggHxqoYYEnCqBFATwlk+29McIbRnhJ+fBCHrQwQQMzNJIJHrCGhHThi+vwtf4VfsiDZ5GSFT4eUOgV+YcKFuiQD2/JCC/bNj0kC24ILdLhiRvQwig84CmZ5Ne9pTxoUVDko1AADW4ILXKhxQ14wig8rO+1ZK0/NSwQgO2dkWAR1nLJzwuV1QIVLFDBbHsXPWGrFynfWk+248IDJnhK1r8mqOW6tdevGSqYhRpm2zpNUFun2f4KADrkw8tWBzoY5X2RH0Jt++QJqCQh17HK+mm0hlNY4Isb8JVuwA+58JNuwBc35Lr1hhFa5CMXOlwXvrgGX1wXvsiBl+19uPm+maBGnvB02A+p0OcMtk+d2V5HwlpPKrmM1pq0QHVLfWhsn6UCaJEPnWStIfs7oLK9RypY5M+n9T2TkAsdsqFDjvBCNnTW4ATTzffB9h5YH2b5/ciBDrnCumwePKGCgAdMUMMCD5iglQqs31/Ig69kra9seOG68EW68MN1+EKCQLB0HcHSNYRIV1ENGciCNy4Kf6SJQFwUAciAj8O6NbbPyM1jSbId7Tdr2gKVtQyFjisPmGCBBDPU8rFiEtZ/F0ANEzQwQVXoO8y6BS0K4Cvd/M70hhEF0CDX9nnIhQ55wqPIse4j5UGPXOilHOiRAx0KkAOd/FuQCW8YhYetLNYySBDQIxcBUhYMUg78kQ0VBHKgQ47QIRc65EAHE9QwCZW8L/ZvxMLbt7/fGtt77QEzvCSj7TNh/f4psB2PeYW+D+zrsX/H+Pp4o3+bf5X0S+lSvIifq9xIh/l/7aHOPu+6bRAREZVDmZoq0L97yrnrvIvfb7bcuIqXP9T9vwOSxgGSGvD0Bjx9rA8BwJgBGLOsF/3LzwY0WsDTD9D6Ap6+gEoNGLOt89jn9fC29ufxCgC8/K3rAmxtmBbrQ6O1Lu/pA2j9AI3O2mYsLDfnM2YB2ReB7EtAziUg9yqg8gA8vKzLa3TWf3vayqK1re/W52YTkH4WSE+x/U0FLAUApJunO9SegG8Q4Fvd+vCpCpjygew06/az0oAb16x1pFIDKg0gqaxlNeUBZiNgMgLmfGsZNTpA4wmotYCHzloOD1u9anRAXgaQewXIuWx95OdY1yepbX8lxzqDsO5z1YZAtQjrqLeqDa3LXT0JXDlh/Zt5zrq8vXwqtXU/IWzXNRLy9Y0caHTWffauYv2r87e+3zm2MuZetR4D9v00Ga3/Vmms+2jfV42nbT+9rceBh5d1ulpjrRe1h3X7eenWdeZes/4121pTJAkO/8cvfDxAOL5nkGzvv31b3tb9LcgDCnKt70vBDet76+l9s1wa7c1t2NvNzfnWZfJzby7r8H7Yuv3Zj1/7A4WO6cLHt8XseKzbj1eN1lZPWmu51J7WOrGYrduUH/mAMFunW0y2f1us/7Y/F5abdWz/K6ms+2LKt/41598sv3xcSdZ6sv9bUlk/g1q99a9OX+h4tdWrRmc9Hm6kWz8Hudesz299zywmW/mNN/dDwi31bbHui33/hKVoGS3mQseabT1q+2ff9rlXaWz7obp5XKg0hY59jXUb+TnW76h82wPSLfXvaTt+bQ+Vxrofxizrsvk51mNCpbYewyqN9XjW6Ap9z/ha68mYZT2e7XUEAehrWLsB6EOt3y3GLCDzvPU7Jeu89btAXq/HzTqQ6+nWY832V+1xsy7s+2M/9ixm63ecxWT9bMl/CxyPa5XaupzW9+b3uoe3df78bOvnIT/HOvik8OdQCOt8Xv6AzmD9vtBorcvYfy+Mmdb3zn4Mmwusy9p/G7wDrX8l9c33xl7f9uPcYir0GSj0/SUshb6Hbd/J9uPDw8f218u6XEGu9Xug4Ia1PA7fgwJ6n2ql+KF0HbbcEBERUbnH2y8QERHRfYvhhoiIiCoVhhsiIiKqVBhuiIiIqFJhuCEiIqJKheGGiIiIKhWGGyIiIqpUGG6IiIioUmG4ISIiokqF4YaIiIgqFYYbIiIiqlQYboiIiKhSYbghIiKiSoXhhoiIiCoVjdIFcDchBADrrdOJiIioYrD/btt/x0ty34WbrKwsAEBYWJjCJSEiIqK7lZWVBYPBUOI8kihNBKpELBYLzp8/Dz8/P0iS5NR1Z2ZmIiwsDKmpqdDr9U5dNzliXbsP69p9WNfuw7p2H2fVtRACWVlZCA0NhUpVcq+a+67lRqVSoWbNmi7dhl6v54fFTVjX7sO6dh/Wtfuwrt3HGXV9pxYbO3YoJiIiokqF4YaIiIgqFYYbJ9JqtRg/fjy0Wq3SRan0WNfuw7p2H9a1+7Cu3UeJur7vOhQTERFR5caWGyIiIqpUGG6IiIioUmG4ISIiokqF4YaIiIgqFYYbJ/nf//6HOnXqQKfToV27dtixY4fSRapwEhIS0KZNG/j5+SEoKAjR0dE4duyYwzx5eXmIi4tDlSpV4Ovri6eeegoXL150mCclJQV9+vSBt7c3goKC8Oabb8JkMrlzVyqcDz/8EJIkYeTIkfI01rXznDt3Dv/85z9RpUoVeHl5oVmzZti1a5f8uhAC48aNQ0hICLy8vNCtWzecOHHCYR3Xrl3DoEGDoNfr4e/vjxdffBHZ2dnu3pVyzWw247333kN4eDi8vLxQr149vP/++w73ImJdl83mzZvRt29fhIaGQpIkLFu2zOF1Z9Xrn3/+iY4dO0Kn0yEsLAxTp04tW4EF3bP58+cLT09P8e2334pDhw6Jl19+Wfj7+4uLFy8qXbQKpUePHmL27Nni4MGDYt++faJ3796iVq1aIjs7W55n2LBhIiwsTKxfv17s2rVLPPTQQ+Lhhx+WXzeZTKJp06aiW7duYu/evWLVqlWiatWqYsyYMUrsUoWwY8cOUadOHdG8eXPx2muvydNZ185x7do1Ubt2bREbGyv++OMPcerUKbF27Vpx8uRJeZ4PP/xQGAwGsWzZMrF//37x+OOPi/DwcHHjxg15np49e4oWLVqI7du3i99++03Ur19fDBgwQIldKrcmT54sqlSpIlauXClOnz4tFi1aJHx9fcV///tfeR7WddmsWrVKjB07VixZskQAEEuXLnV43Rn1mpGRIapXry4GDRokDh48KH766Sfh5eUlZsyYcdflZbhxgrZt24q4uDj5udlsFqGhoSIhIUHBUlV8ly5dEgDEpk2bhBBCpKenCw8PD7Fo0SJ5niNHjggAYtu2bUII6wdQpVKJtLQ0eZ7ExESh1+uF0Wh07w5UAFlZWaJBgwYiKSlJdO7cWQ43rGvneeutt0SHDh1u+7rFYhHBwcHi448/lqelp6cLrVYrfvrpJyGEEIcPHxYAxM6dO+V5Vq9eLSRJEufOnXNd4SuYPn36iBdeeMFh2pNPPikGDRokhGBdO8ut4cZZ9Tp9+nQREBDg8P3x1ltviUaNGt11GXla6h7l5+dj9+7d6NatmzxNpVKhW7du2LZtm4Ilq/gyMjIAAIGBgQCA3bt3o6CgwKGuIyIiUKtWLbmut23bhmbNmqF69eryPD169EBmZiYOHTrkxtJXDHFxcejTp49DnQKsa2dasWIFWrdujWeeeQZBQUFo2bIlZs2aJb9++vRppKWlOdS1wWBAu3btHOra398frVu3lufp1q0bVCoV/vjjD/ftTDn38MMPY/369Th+/DgAYP/+/diyZQt69eoFgHXtKs6q123btqFTp07w9PSU5+nRoweOHTuG69ev31WZ7rsbZzrblStXYDabHb7gAaB69eo4evSoQqWq+CwWC0aOHImoqCg0bdoUAJCWlgZPT0/4+/s7zFu9enWkpaXJ8xT3Xthfo5vmz5+PPXv2YOfOnUVeY107z6lTp5CYmIhRo0bhnXfewc6dOzFixAh4enoiJiZGrqvi6rJwXQcFBTm8rtFoEBgYyLou5O2330ZmZiYiIiKgVqthNpsxefJkDBo0CABY1y7irHpNS0tDeHh4kXXYXwsICCh1mRhuqFyKi4vDwYMHsWXLFqWLUimlpqbitddeQ1JSEnQ6ndLFqdQsFgtat26NKVOmAABatmyJgwcP4quvvkJMTIzCpatcFi5ciLlz52LevHlo0qQJ9u3bh5EjRyI0NJR1fZ/haal7VLVqVajV6iKjSC5evIjg4GCFSlWxxcfHY+XKldi4cSNq1qwpTw8ODkZ+fj7S09Md5i9c18HBwcW+F/bXyGr37t24dOkSHnzwQWg0Gmg0GmzatAmff/45NBoNqlevzrp2kpCQEDRu3NhhWmRkJFJSUgDcrKuSvkOCg4Nx6dIlh9dNJhOuXbvGui7kzTffxNtvv43nnnsOzZo1w+DBg/H6668jISEBAOvaVZxVr878TmG4uUeenp5o1aoV1q9fL0+zWCxYv3492rdvr2DJKh4hBOLj47F06VJs2LChSPNkq1at4OHh4VDXx44dQ0pKilzX7du3x4EDBxw+RElJSdDr9UV+YO5nXbt2xYEDB7Bv3z750bp1awwaNEj+N+vaOaKioopc0uD48eOoXbs2ACA8PBzBwcEOdZ2ZmYk//vjDoa7T09Oxe/dueZ4NGzbAYrGgXbt2btiLiiE3NxcqlePPmlqthsViAcC6dhVn1Wv79u2xefNmFBQUyPMkJSWhUaNGd3VKCgCHgjvD/PnzhVarFXPmzBGHDx8WQ4cOFf7+/g6jSOjOXnnlFWEwGERycrK4cOGC/MjNzZXnGTZsmKhVq5bYsGGD2LVrl2jfvr1o3769/Lp9eHL37t3Fvn37xJo1a0S1atU4PLkUCo+WEoJ17Sw7duwQGo1GTJ48WZw4cULMnTtXeHt7ix9//FGe58MPPxT+/v5i+fLl4s8//xRPPPFEscNoW7ZsKf744w+xZcsW0aBBg/t+ePKtYmJiRI0aNeSh4EuWLBFVq1YV//73v+V5WNdlk5WVJfbu3Sv27t0rAIhp06aJvXv3irNnzwohnFOv6enponr16mLw4MHi4MGDYv78+cLb25tDwZX0xRdfiFq1aglPT0/Rtm1bsX37dqWLVOEAKPYxe/ZseZ4bN26IV199VQQEBAhvb2/Rr18/ceHCBYf1nDlzRvTq1Ut4eXmJqlWrijfeeEMUFBS4eW8qnlvDDevaeX7++WfRtGlTodVqRUREhJg5c6bD6xaLRbz33nuievXqQqvViq5du4pjx445zHP16lUxYMAA4evrK/R6vXj++edFVlaWO3ej3MvMzBSvvfaaqFWrltDpdKJu3bpi7NixDkOLWddls3HjxmK/n2NiYoQQzqvX/fv3iw4dOgitVitq1KghPvzwwzKVVxKi0KUbiYiIiCo49rkhIiKiSoXhhoiIiCoVhhsiIiKqVBhuiIiIqFJhuCEiIqJKheGGiIiIKhWGGyIiIqpUGG6I6L4kSRKWLVumdDGIyAUYbojI7WJjYyFJUpFHz549lS4aEVUCGqULQET3p549e2L27NkO07RarUKlIaLKhC03RKQIrVaL4OBgh4f9zr+SJCExMRG9evWCl5cX6tati8WLFzssf+DAATz66KPw8vJClSpVMHToUGRnZzvM8+2336JJkybQarUICQlBfHy8w+tXrlxBv3794O3tjQYNGmDFihXya9evX8egQYNQrVo1eHl5oUGDBkXCGBGVTww3RFQuvffee3jqqaewf/9+DBo0CM899xyOHDkCAMjJyUGPHj0QEBCAnTt3YtGiRfj1118dwktiYiLi4uIwdOhQHDhwACtWrED9+vUdtjFx4kQ8++yz+PPPP9G7d28MGjQI165dk7d/+PBhrF69GkeOHEFiYiKqVq3qvgogorIr0+02iYjuQUxMjFCr1cLHx8fhMXnyZCGE9Q7xw4YNc1imXbt24pVXXhFCCDFz5kwREBAgsrOz5dd/+eUXoVKpRFpamhBCiNDQUDF27NjblgGAePfdd+Xn2dnZAoBYvXq1EEKIvn37iueff945O0xEbsU+N0SkiEceeQSJiYkO0wIDA+V/t2/f3uG19u3bY9++fQCAI0eOoEWLFvDx8ZFfj4qKgsViwbFjxyBJEs6fP4+uXbuWWIbmzZvL//bx8YFer8elS5cAAK+88gqeeuop7NmzB927d0d0dDQefvjhMu0rEbkXww0RKcLHx6fIaSJn8fLyKtV8Hh4eDs8lSYLFYgEA9OrVC2fPnsWqVauQlJSErl27Ii4uDp988onTy0tEzsU+N0RULm3fvr3I88jISABAZGQk9u/fj5ycHPn1rVu3QqVSoVGjRvDz80OdOnWwfv36eypDtWrVEBMTgx9//BGfffYZZs6ceU/rIyL3YMsNESnCaDQiLS3NYZpGo5E77S5atAitW7dGhw4dMHfuXOzYsQPffPMNAGDQoEEYP348YmJiMGHCBFy+fBnDhw/H4MGDUb16dQDAhAkTMGzYMAQFBaFXr17IysrC1q1bMXz48FKVb9y4cWjVqhWaNGkCo9GIlStXyuGKiMo3hhsiUsSaNWsQEhLiMK1Ro0Y4evQoAOtIpvnz5+PVV19FSEgIfvrpJzRu3BgA4O3tjbVr1+K1115DmzZt4O3tjaeeegrTpk2T1xUTE4O8vDz85z//wejRo1G1alU8/fTTpS6fp6cnxowZgzNnzsDLywsdO3bE/PnznbDnRORqkhBCKF0IIqLCJEnC0qVLER0drXRRiKgCYp8bIiIiqlQYboiIiKhSYZ8bIip3eLaciO4FW26IiIioUmG4ISIiokqF4YaIiIgqFYYbIiIiqlQYboiIiKhSYbghIiKiSoXhhoiIiCoVhhsiIiKqVBhuiIiIqFL5f3rgek67qHXvAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred = californiaHousingRegressionModel(X_train_tensor)"
      ],
      "metadata": {
        "id": "QsYpwIQlQODs"
      },
      "id": "QsYpwIQlQODs",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDkOYhQceN_-",
        "outputId": "d040b6db-da06-4e73-9234-0b2bcca7079c"
      },
      "id": "LDkOYhQceN_-",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2.1034],\n",
              "        [2.1297],\n",
              "        [2.0956],\n",
              "        [2.1296],\n",
              "        [2.0396],\n",
              "        [2.0854],\n",
              "        [2.0902],\n",
              "        [2.0458],\n",
              "        [2.1561],\n",
              "        [2.0605],\n",
              "        [2.0673],\n",
              "        [2.0393],\n",
              "        [2.0897],\n",
              "        [2.0668],\n",
              "        [2.1054],\n",
              "        [2.0971],\n",
              "        [2.0258],\n",
              "        [2.0896],\n",
              "        [2.1041],\n",
              "        [2.1442],\n",
              "        [2.0680],\n",
              "        [2.0590],\n",
              "        [2.0738],\n",
              "        [2.0803],\n",
              "        [2.0405],\n",
              "        [2.0571],\n",
              "        [2.1261],\n",
              "        [2.0536],\n",
              "        [2.0648],\n",
              "        [2.0817],\n",
              "        [2.0946],\n",
              "        [2.1350],\n",
              "        [2.0939],\n",
              "        [2.0647],\n",
              "        [2.0753],\n",
              "        [2.0878],\n",
              "        [2.0453],\n",
              "        [2.0940],\n",
              "        [2.0745],\n",
              "        [2.1054],\n",
              "        [2.0485],\n",
              "        [2.0473],\n",
              "        [2.0931],\n",
              "        [2.0766],\n",
              "        [2.0616],\n",
              "        [2.0697],\n",
              "        [2.0079],\n",
              "        [2.0097],\n",
              "        [2.1047],\n",
              "        [2.0361],\n",
              "        [2.0741],\n",
              "        [2.1271],\n",
              "        [2.1477],\n",
              "        [2.1014],\n",
              "        [2.1087],\n",
              "        [2.1322],\n",
              "        [2.0555],\n",
              "        [2.1065],\n",
              "        [2.0320],\n",
              "        [2.0541],\n",
              "        [2.0617],\n",
              "        [2.0667],\n",
              "        [2.0529],\n",
              "        [2.1283],\n",
              "        [2.0816],\n",
              "        [2.1244],\n",
              "        [2.0937],\n",
              "        [2.0850],\n",
              "        [2.0626],\n",
              "        [2.0967],\n",
              "        [2.0974],\n",
              "        [2.0771],\n",
              "        [2.0999],\n",
              "        [2.0632],\n",
              "        [2.0378],\n",
              "        [2.0804],\n",
              "        [2.0447],\n",
              "        [2.1430],\n",
              "        [2.0752],\n",
              "        [2.1598],\n",
              "        [2.1158],\n",
              "        [2.0533],\n",
              "        [2.0579],\n",
              "        [2.0702],\n",
              "        [2.0690],\n",
              "        [2.0634],\n",
              "        [2.0553],\n",
              "        [2.1144],\n",
              "        [2.0894],\n",
              "        [2.0798],\n",
              "        [1.9922],\n",
              "        [2.0428],\n",
              "        [2.1071],\n",
              "        [2.1256],\n",
              "        [2.0089],\n",
              "        [2.0683],\n",
              "        [2.1166],\n",
              "        [2.1015],\n",
              "        [2.0525],\n",
              "        [2.0576]], grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_tensor[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsVq8oTieWK5",
        "outputId": "5d9759af-edc2-42f8-e4ac-f96ddb586091"
      },
      "id": "KsVq8oTieWK5",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2.2850, 2.7990, 1.8300, 4.6580, 1.5000, 2.5090, 5.0000, 1.2080, 5.0000,\n",
              "        0.9930, 1.7340, 1.0950, 1.5240, 1.5750, 3.0310, 1.5150, 1.1750, 2.4880,\n",
              "        3.4670, 3.6750, 1.4210, 1.3750, 1.6320, 3.3680, 0.4750, 1.3680, 2.1660,\n",
              "        1.9480, 1.5180, 1.2060, 2.4200, 3.3020, 2.1700, 1.6620, 1.5310, 2.9050,\n",
              "        1.7390, 2.6890, 1.4460, 1.5190, 1.5630, 0.9170, 3.1500, 1.7410, 1.0720,\n",
              "        1.0900, 0.7330, 0.6230, 4.0000, 0.6290, 2.0430, 2.4950, 5.0000, 2.8610,\n",
              "        5.0000, 3.6070, 1.3230, 2.5370, 0.7980, 0.8750, 1.6130, 0.7000, 1.0140,\n",
              "        3.9550, 1.3320, 2.8530, 2.0820, 1.8900, 0.5320, 3.0210, 2.1380, 2.9750,\n",
              "        2.7930, 1.3420, 1.7500, 1.2230, 0.7320, 4.9900, 1.3480, 5.0000, 2.9900,\n",
              "        3.2500, 1.1460, 2.2300, 1.4880, 1.0150, 1.2250, 2.2420, 2.0930, 1.4130,\n",
              "        1.4300, 1.0690, 2.8830, 3.9820, 0.6180, 1.3750, 3.6530, 1.7410, 1.3750,\n",
              "        1.3200])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4h3e-ccweZu6"
      },
      "id": "4h3e-ccweZu6",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "3fbe1355223f7b2ffc113ba3ade6a2b520cadace5d5ec3e828c83ce02eb221bf"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1c919e376d864fb294279638fdeae304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_69d799775eda40f2b25c2e397071d660",
              "IPY_MODEL_77cb12b7edcc4937bc1edf0ea09759d5",
              "IPY_MODEL_a8339910e5fa4ed7911e3825e61ce44e"
            ],
            "layout": "IPY_MODEL_d0d8db2f15a6428da4c4aac405d73794"
          }
        },
        "69d799775eda40f2b25c2e397071d660": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6828dab460d142aabd6ced801148c095",
            "placeholder": "​",
            "style": "IPY_MODEL_28f50172dc284acb9d09bb7dacf0a51e",
            "value": "100%"
          }
        },
        "77cb12b7edcc4937bc1edf0ea09759d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf7c054cf7374d9c944e39410a8166bd",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3cee61ca41894450b0e769a12cb7e9f9",
            "value": 3
          }
        },
        "a8339910e5fa4ed7911e3825e61ce44e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1d78ba7fb334f3e856d4cad0bd2f63d",
            "placeholder": "​",
            "style": "IPY_MODEL_b6a295fd826d4c4fb63a0c8b5783094c",
            "value": " 3/3 [00:38&lt;00:00, 12.10s/it]"
          }
        },
        "d0d8db2f15a6428da4c4aac405d73794": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6828dab460d142aabd6ced801148c095": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28f50172dc284acb9d09bb7dacf0a51e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf7c054cf7374d9c944e39410a8166bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cee61ca41894450b0e769a12cb7e9f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b1d78ba7fb334f3e856d4cad0bd2f63d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6a295fd826d4c4fb63a0c8b5783094c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92e1437d16554378b7645b0618aaa433": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b8c1f0401bd34817819b8f6aca8dcea0",
              "IPY_MODEL_0b0683e477274b45a22d5b4cb99ba14d",
              "IPY_MODEL_e8ebc2dcfe6842529ac3a99f94252aae"
            ],
            "layout": "IPY_MODEL_2a2f72864d374f63955f86b092a8e127"
          }
        },
        "b8c1f0401bd34817819b8f6aca8dcea0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00bc1b8e560845d988b2f38242fdd088",
            "placeholder": "​",
            "style": "IPY_MODEL_bbf6a85cf3c043559cc92c07592d1b85",
            "value": "100%"
          }
        },
        "0b0683e477274b45a22d5b4cb99ba14d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38b820d701a0460db7faec9d54f7b1c8",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_891b0c76e4484fde99b228c7bb0c68e2",
            "value": 30
          }
        },
        "e8ebc2dcfe6842529ac3a99f94252aae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9a6f74d8dd14d0a91ca40eeb4d3a8fb",
            "placeholder": "​",
            "style": "IPY_MODEL_a53fffa625b2404fb1ccd06d3678325d",
            "value": " 30/30 [05:06&lt;00:00, 10.02s/it]"
          }
        },
        "2a2f72864d374f63955f86b092a8e127": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00bc1b8e560845d988b2f38242fdd088": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbf6a85cf3c043559cc92c07592d1b85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38b820d701a0460db7faec9d54f7b1c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "891b0c76e4484fde99b228c7bb0c68e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c9a6f74d8dd14d0a91ca40eeb4d3a8fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a53fffa625b2404fb1ccd06d3678325d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fea3b2ef577f46a69c69b84dbf7ea50e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff1ea5f67002439c9aa4aa5eafa389e9",
              "IPY_MODEL_405c4f89d7cf433a90a19b6e430775b3",
              "IPY_MODEL_187c3495931f4fbeac925e605afb5419"
            ],
            "layout": "IPY_MODEL_5fb1ff518a104a7298c844ce912affef"
          }
        },
        "ff1ea5f67002439c9aa4aa5eafa389e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4054a98bd5b94a9f989b170e17dedd6d",
            "placeholder": "​",
            "style": "IPY_MODEL_d19dfac458d4465faf9877c83e08263c",
            "value": "100%"
          }
        },
        "405c4f89d7cf433a90a19b6e430775b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_078b62523dd345eabc2c52dd146edab0",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_540a5629bff84cceb8bc42137fd7e96c",
            "value": 1000
          }
        },
        "187c3495931f4fbeac925e605afb5419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be474ede7489437e82d4f61f617e7a3d",
            "placeholder": "​",
            "style": "IPY_MODEL_74f6055ac4094a06afae151b4d5b27c9",
            "value": " 1000/1000 [12:30&lt;00:00,  1.16it/s]"
          }
        },
        "5fb1ff518a104a7298c844ce912affef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4054a98bd5b94a9f989b170e17dedd6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d19dfac458d4465faf9877c83e08263c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "078b62523dd345eabc2c52dd146edab0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "540a5629bff84cceb8bc42137fd7e96c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be474ede7489437e82d4f61f617e7a3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74f6055ac4094a06afae151b4d5b27c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}